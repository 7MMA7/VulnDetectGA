{"idx": 431633, "project": "ImageMagick", "commit_id": "ca3654ebf7a439dc736f56f083c9aa98e4464b7f", "project_url": "https://github.com/ImageMagick/ImageMagick", "commit_url": "https://github.com/ImageMagick/ImageMagick/commit/ca3654ebf7a439dc736f56f083c9aa98e4464b7f", "commit_message": "https://github.com/ImageMagick/ImageMagick/issues/4988", "target": 0, "func": "static Image *ReadCINImage(const ImageInfo *image_info,ExceptionInfo *exception)\n{\n#define MonoColorType  1\n#define RGBColorType  3\n\n  char\n    property[MagickPathExtent];\n\n  CINInfo\n    cin;\n\n  Image\n    *image;\n\n  MagickBooleanType\n    status;\n\n  MagickOffsetType\n    offset;\n\n  QuantumInfo\n    *quantum_info;\n\n  QuantumType\n    quantum_type;\n\n  ssize_t\n    i;\n\n  Quantum\n    *q;\n\n  size_t\n    length;\n\n  ssize_t\n    count,\n    y;\n\n  unsigned char\n    magick[4],\n    *pixels;\n\n\n  /*\n    Open image file.\n  */\n  assert(image_info != (const ImageInfo *) NULL);\n  assert(image_info->signature == MagickCoreSignature);\n  if (image_info->debug != MagickFalse)\n    (void) LogMagickEvent(TraceEvent,GetMagickModule(),\"%s\",\n      image_info->filename);\n  assert(exception != (ExceptionInfo *) NULL);\n  assert(exception->signature == MagickCoreSignature);\n  image=AcquireImage(image_info,exception);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    File information.\n  */\n  offset=0;\n  count=ReadBlob(image,4,magick);\n  offset+=count;\n  if ((count != 4) ||\n      ((LocaleNCompare((char *) magick,\"\\200\\052\\137\\327\",4) != 0)))\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  memset(&cin,0,sizeof(cin));\n  image->endian=(magick[0] == 0x80) && (magick[1] == 0x2a) &&\n    (magick[2] == 0x5f) && (magick[3] == 0xd7) ? MSBEndian : LSBEndian;\n  cin.file.image_offset=ReadBlobLong(image);\n  if (cin.file.image_offset < 712)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  offset+=4;\n  cin.file.generic_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.industry_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.user_length=ReadBlobLong(image);\n  offset+=4;\n  cin.file.file_size=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(cin.file.version),(unsigned char *)\n    cin.file.version);\n  (void) CopyMagickString(property,cin.file.version,sizeof(cin.file.version));\n  (void) SetImageProperty(image,\"dpx:file.version\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.filename),(unsigned char *)\n    cin.file.filename);\n  (void) CopyMagickString(property,cin.file.filename,sizeof(cin.file.filename));\n  (void) SetImageProperty(image,\"dpx:file.filename\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.create_date),(unsigned char *)\n    cin.file.create_date);\n  (void) CopyMagickString(property,cin.file.create_date,\n    sizeof(cin.file.create_date));\n  (void) SetImageProperty(image,\"dpx:file.create_date\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.create_time),(unsigned char *)\n    cin.file.create_time);\n  (void) CopyMagickString(property,cin.file.create_time,\n    sizeof(cin.file.create_time));\n  (void) SetImageProperty(image,\"dpx:file.create_time\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.file.reserve),(unsigned char *)\n    cin.file.reserve);\n  /*\n    Image information.\n  */\n  cin.image.orientation=(unsigned char) ReadBlobByte(image);\n  offset++;\n  if (cin.image.orientation != (unsigned char) (~0))\n    (void) FormatImageProperty(image,\"dpx:image.orientation\",\"%d\",\n      cin.image.orientation);\n  switch (cin.image.orientation)\n  {\n    default:\n    case 0: image->orientation=TopLeftOrientation; break;\n    case 1: image->orientation=TopRightOrientation; break;\n    case 2: image->orientation=BottomLeftOrientation; break;\n    case 3: image->orientation=BottomRightOrientation; break;\n    case 4: image->orientation=LeftTopOrientation; break;\n    case 5: image->orientation=RightTopOrientation; break;\n    case 6: image->orientation=LeftBottomOrientation; break;\n    case 7: image->orientation=RightBottomOrientation; break;\n  }\n  cin.image.number_channels=(unsigned char) ReadBlobByte(image);\n  offset++;\n  offset+=ReadBlob(image,sizeof(cin.image.reserve1),(unsigned char *)\n    cin.image.reserve1);\n  for (i=0; i < 8; i++)\n  {\n    cin.image.channel[i].designator[0]=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].designator[1]=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].bits_per_pixel=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].reserve=(unsigned char) ReadBlobByte(image);\n    offset++;\n    cin.image.channel[i].pixels_per_line=ReadBlobLong(image);\n    offset+=4;\n    cin.image.channel[i].lines_per_image=ReadBlobLong(image);\n    offset+=4;\n    cin.image.channel[i].min_data=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].min_quantity=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].max_data=ReadBlobFloat(image);\n    offset+=4;\n    cin.image.channel[i].max_quantity=ReadBlobFloat(image);\n    offset+=4;\n  }\n  cin.image.white_point[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.white_point[0]) != MagickFalse)\n    image->chromaticity.white_point.x=cin.image.white_point[0];\n  cin.image.white_point[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.white_point[1]) != MagickFalse)\n    image->chromaticity.white_point.y=cin.image.white_point[1];\n  cin.image.red_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.red_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.red_primary.x=cin.image.red_primary_chromaticity[0];\n  cin.image.red_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.red_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.red_primary.y=cin.image.red_primary_chromaticity[1];\n  cin.image.green_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.green_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.red_primary.x=cin.image.green_primary_chromaticity[0];\n  cin.image.green_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.green_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.green_primary.y=cin.image.green_primary_chromaticity[1];\n  cin.image.blue_primary_chromaticity[0]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.blue_primary_chromaticity[0]) != MagickFalse)\n    image->chromaticity.blue_primary.x=cin.image.blue_primary_chromaticity[0];\n  cin.image.blue_primary_chromaticity[1]=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.image.blue_primary_chromaticity[1]) != MagickFalse)\n    image->chromaticity.blue_primary.y=cin.image.blue_primary_chromaticity[1];\n  offset+=ReadBlob(image,sizeof(cin.image.label),(unsigned char *)\n    cin.image.label);\n  (void) CopyMagickString(property,cin.image.label,sizeof(cin.image.label));\n  (void) SetImageProperty(image,\"dpx:image.label\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.image.reserve),(unsigned char *)\n    cin.image.reserve);\n  /*\n    Image data format information.\n  */\n  cin.data_format.interleave=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.packing=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.sign=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.sense=(unsigned char) ReadBlobByte(image);\n  offset++;\n  cin.data_format.line_pad=ReadBlobLong(image);\n  offset+=4;\n  cin.data_format.channel_pad=ReadBlobLong(image);\n  offset+=4;\n  offset+=ReadBlob(image,sizeof(cin.data_format.reserve),(unsigned char *)\n    cin.data_format.reserve);\n  /*\n    Image origination information.\n  */\n  cin.origination.x_offset=ReadBlobSignedLong(image);\n  offset+=4;\n  if ((size_t) cin.origination.x_offset != ~0UL)\n    (void) FormatImageProperty(image,\"dpx:origination.x_offset\",\"%.20g\",\n      (double) cin.origination.x_offset);\n  cin.origination.y_offset=(ssize_t) ReadBlobLong(image);\n  offset+=4;\n  if ((size_t) cin.origination.y_offset != ~0UL)\n    (void) FormatImageProperty(image,\"dpx:origination.y_offset\",\"%.20g\",\n      (double) cin.origination.y_offset);\n  offset+=ReadBlob(image,sizeof(cin.origination.filename),(unsigned char *)\n    cin.origination.filename);\n  (void) CopyMagickString(property,cin.origination.filename,\n    sizeof(cin.origination.filename));\n  (void) SetImageProperty(image,\"dpx:origination.filename\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.create_date),(unsigned char *)\n    cin.origination.create_date);\n  (void) CopyMagickString(property,cin.origination.create_date,\n    sizeof(cin.origination.create_date));\n  (void) SetImageProperty(image,\"dpx:origination.create_date\",property,\n    exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.create_time),(unsigned char *)\n    cin.origination.create_time);\n  (void) CopyMagickString(property,cin.origination.create_time,\n    sizeof(cin.origination.create_time));\n  (void) SetImageProperty(image,\"dpx:origination.create_time\",property,\n    exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.device),(unsigned char *)\n    cin.origination.device);\n  (void) CopyMagickString(property,cin.origination.device,\n    sizeof(cin.origination.device));\n  (void) SetImageProperty(image,\"dpx:origination.device\",property,exception);\n  offset+=ReadBlob(image,sizeof(cin.origination.model),(unsigned char *)\n    cin.origination.model);\n  (void) CopyMagickString(property,cin.origination.model,\n    sizeof(cin.origination.model));\n  (void) SetImageProperty(image,\"dpx:origination.model\",property,exception);\n  (void) memset(cin.origination.serial,0, \n    sizeof(cin.origination.serial));\n  offset+=ReadBlob(image,sizeof(cin.origination.serial),(unsigned char *)\n    cin.origination.serial);\n  (void) CopyMagickString(property,cin.origination.serial,\n    sizeof(cin.origination.serial));\n  (void) SetImageProperty(image,\"dpx:origination.serial\",property,exception);\n  cin.origination.x_pitch=ReadBlobFloat(image);\n  offset+=4;\n  cin.origination.y_pitch=ReadBlobFloat(image);\n  offset+=4;\n  cin.origination.gamma=ReadBlobFloat(image);\n  offset+=4;\n  if (IsFloatDefined(cin.origination.gamma) != MagickFalse)\n    image->gamma=cin.origination.gamma;\n  offset+=ReadBlob(image,sizeof(cin.origination.reserve),(unsigned char *)\n    cin.origination.reserve);\n  if ((cin.file.image_offset > 2048) && (cin.file.user_length != 0))\n    {\n      int\n        c;\n\n      /*\n        Image film information.\n      */\n      cin.film.id=ReadBlobByte(image);\n      offset++;\n      c=cin.film.id;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.id\",\"%d\",cin.film.id);\n      cin.film.type=ReadBlobByte(image);\n      offset++;\n      c=cin.film.type;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.type\",\"%d\",cin.film.type);\n      cin.film.offset=ReadBlobByte(image);\n      offset++;\n      c=cin.film.offset;\n      if (c != ~0)\n        (void) FormatImageProperty(image,\"dpx:film.offset\",\"%d\",\n          cin.film.offset);\n      cin.film.reserve1=ReadBlobByte(image);\n      offset++;\n      cin.film.prefix=ReadBlobLong(image);\n      offset+=4;\n      if (cin.film.prefix != ~0UL)\n        (void) FormatImageProperty(image,\"dpx:film.prefix\",\"%.20g\",(double)\n          cin.film.prefix);\n      cin.film.count=ReadBlobLong(image);\n      offset+=4;\n      offset+=ReadBlob(image,sizeof(cin.film.format),(unsigned char *)\n        cin.film.format);\n      (void) CopyMagickString(property,cin.film.format,sizeof(cin.film.format));\n      (void) SetImageProperty(image,\"dpx:film.format\",property,exception);\n      cin.film.frame_position=ReadBlobLong(image);\n      offset+=4;\n      if (cin.film.frame_position != ~0UL)\n        (void) FormatImageProperty(image,\"dpx:film.frame_position\",\"%.20g\",\n          (double) cin.film.frame_position);\n      cin.film.frame_rate=ReadBlobFloat(image);\n      offset+=4;\n      if (IsFloatDefined(cin.film.frame_rate) != MagickFalse)\n        (void) FormatImageProperty(image,\"dpx:film.frame_rate\",\"%g\",\n          cin.film.frame_rate);\n      offset+=ReadBlob(image,sizeof(cin.film.frame_id),(unsigned char *)\n        cin.film.frame_id);\n      (void) CopyMagickString(property,cin.film.frame_id,\n        sizeof(cin.film.frame_id));\n      (void) SetImageProperty(image,\"dpx:film.frame_id\",property,exception);\n      offset+=ReadBlob(image,sizeof(cin.film.slate_info),(unsigned char *)\n        cin.film.slate_info);\n      (void) CopyMagickString(property,cin.film.slate_info,\n        sizeof(cin.film.slate_info));\n      (void) SetImageProperty(image,\"dpx:film.slate_info\",property,exception);\n      offset+=ReadBlob(image,sizeof(cin.film.reserve),(unsigned char *)\n        cin.film.reserve);\n    }\n  if ((cin.file.image_offset > 2048) && (cin.file.user_length != 0))\n    {\n      StringInfo\n        *profile;\n\n      /*\n        User defined data.\n      */\n      if (cin.file.user_length > GetBlobSize(image))\n        ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n      profile=BlobToStringInfo((const unsigned char *) NULL,\n        cin.file.user_length);\n      if (profile == (StringInfo *) NULL)\n        ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n      offset+=ReadBlob(image,GetStringInfoLength(profile),\n        GetStringInfoDatum(profile));\n      (void) SetImageProfile(image,\"dpx:user.data\",profile,exception);\n      profile=DestroyStringInfo(profile);\n    }\n  image->depth=cin.image.channel[0].bits_per_pixel;\n  image->columns=cin.image.channel[0].pixels_per_line;\n  image->rows=cin.image.channel[0].lines_per_image;\n  if (image_info->ping != MagickFalse)\n    {\n      (void) CloseBlob(image);\n      return(image);\n    }\n  if (((MagickSizeType) image->columns*image->rows/8) > GetBlobSize(image))\n    ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n  for ( ; offset < (MagickOffsetType) cin.file.image_offset; offset++)\n  {\n    int\n      c;\n\n    c=ReadBlobByte(image);\n    if (c == EOF)\n      break;\n  }\n  if (offset < (MagickOffsetType) cin.file.image_offset)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    return(DestroyImageList(image));\n  (void) SetImageBackgroundColor(image,exception);\n  /*\n    Convert CIN raster image to pixel packets.\n  */\n  quantum_info=AcquireQuantumInfo(image_info,image);\n  if (quantum_info == (QuantumInfo *) NULL)\n    ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n  SetQuantumQuantum(quantum_info,32);\n  SetQuantumPack(quantum_info,MagickFalse);\n  quantum_type=RGBQuantum;\n  length=GetBytesPerRow(image->columns,3,image->depth,MagickTrue);\n  if (cin.image.number_channels == 1)\n    {\n      quantum_type=GrayQuantum;\n      length=GetBytesPerRow(image->columns,1,image->depth,MagickTrue);\n    }\n  status=SetQuantumPad(image,quantum_info,0);\n  pixels=GetQuantumPixels(quantum_info);\n  for (y=0; y < (ssize_t) image->rows; y++)\n  {\n    const void\n      *stream;\n\n    q=QueueAuthenticPixels(image,0,y,image->columns,1,exception);\n    if (q == (Quantum *) NULL)\n      break;\n    stream=ReadBlobStream(image,length,pixels,&count);\n    if ((size_t) count != length)\n      break;\n    (void) ImportQuantumPixels(image,(CacheView *) NULL,quantum_info,\n      quantum_type,(unsigned char *) stream,exception);\n    if (SyncAuthenticPixels(image,exception) == MagickFalse)\n      break;\n    if (image->previous == (Image *) NULL)\n      {\n        status=SetImageProgress(image,LoadImageTag,(MagickOffsetType) y,\n          image->rows);\n        if (status == MagickFalse)\n          break;\n      }\n  }\n  SetQuantumImageType(image,quantum_type);\n  quantum_info=DestroyQuantumInfo(quantum_info);\n  if (EOFBlob(image) != MagickFalse)\n    ThrowFileException(exception,CorruptImageError,\"UnexpectedEndOfFile\",\n      image->filename);\n  SetImageColorspace(image,LogColorspace,exception);\n  (void) CloseBlob(image);\n  return(GetFirstImageInList(image));\n}", "func_hash": 125130615977204070333523361327116450874, "file_name": "cin.c", "file_hash": 296697563604173271604352885758570763809, "cwe": ["CWE-787"], "cve": "CVE-2022-28463", "cve_desc": "ImageMagick 7.1.0-27 is vulnerable to Buffer Overflow.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-28463", "file_path": "coders/cin.c"}
{"idx": 210271, "project": "vim", "commit_id": "6669de1b235843968e88844ca6d3c8dec4b01a9e", "project_url": "https://github.com/vim/vim", "commit_url": "https://github.com/vim/vim/commit/6669de1b235843968e88844ca6d3c8dec4b01a9e", "commit_message": "patch 9.0.0240: crash when using \":mkspell\" with an empty .dic file\n\nProblem:    Crash when using \":mkspell\" with an empty .dic file.\nSolution:   Check for an empty word tree.", "target": 1, "func": "sug_filltree(spellinfo_T *spin, slang_T *slang)\n{\n    char_u\t*byts;\n    idx_T\t*idxs;\n    int\t\tdepth;\n    idx_T\tarridx[MAXWLEN];\n    int\t\tcuri[MAXWLEN];\n    char_u\ttword[MAXWLEN];\n    char_u\ttsalword[MAXWLEN];\n    int\t\tc;\n    idx_T\tn;\n    unsigned\twords_done = 0;\n    int\t\twordcount[MAXWLEN];\n\n    // We use si_foldroot for the soundfolded trie.\n    spin->si_foldroot = wordtree_alloc(spin);\n    if (spin->si_foldroot == NULL)\n\treturn FAIL;\n\n    // let tree_add_word() know we're adding to the soundfolded tree\n    spin->si_sugtree = TRUE;\n\n    /*\n     * Go through the whole case-folded tree, soundfold each word and put it\n     * in the trie.\n     */\n    byts = slang->sl_fbyts;\n    idxs = slang->sl_fidxs;\n\n    arridx[0] = 0;\n    curi[0] = 1;\n    wordcount[0] = 0;\n\n    depth = 0;\n    while (depth >= 0 && !got_int)\n    {\n\tif (curi[depth] > byts[arridx[depth]])\n\t{\n\t    // Done all bytes at this node, go up one level.\n\t    idxs[arridx[depth]] = wordcount[depth];\n\t    if (depth > 0)\n\t\twordcount[depth - 1] += wordcount[depth];\n\n\t    --depth;\n\t    line_breakcheck();\n\t}\n\telse\n\t{\n\n\t    // Do one more byte at this node.\n\t    n = arridx[depth] + curi[depth];\n\t    ++curi[depth];\n\n\t    c = byts[n];\n\t    if (c == 0)\n\t    {\n\t\t// Sound-fold the word.\n\t\ttword[depth] = NUL;\n\t\tspell_soundfold(slang, tword, TRUE, tsalword);\n\n\t\t// We use the \"flags\" field for the MSB of the wordnr,\n\t\t// \"region\" for the LSB of the wordnr.\n\t\tif (tree_add_word(spin, tsalword, spin->si_foldroot,\n\t\t\t\twords_done >> 16, words_done & 0xffff,\n\t\t\t\t\t\t\t   0) == FAIL)\n\t\t    return FAIL;\n\n\t\t++words_done;\n\t\t++wordcount[depth];\n\n\t\t// Reset the block count each time to avoid compression\n\t\t// kicking in.\n\t\tspin->si_blocks_cnt = 0;\n\n\t\t// Skip over any other NUL bytes (same word with different\n\t\t// flags).  But don't go over the end.\n\t\twhile (n + 1 < slang->sl_fbyts_len && byts[n + 1] == 0)\n\t\t{\n\t\t    ++n;\n\t\t    ++curi[depth];\n\t\t}\n\t    }\n\t    else\n\t    {\n\t\t// Normal char, go one level deeper.\n\t\ttword[depth++] = c;\n\t\tarridx[depth] = idxs[n];\n\t\tcuri[depth] = 1;\n\t\twordcount[depth] = 0;\n\t    }\n\t}\n    }\n\n    smsg(_(\"Total number of words: %d\"), words_done);\n\n    return OK;\n}", "func_hash": 207388436213910008407743050781298909937, "file_name": "spellfile.c", "file_hash": 273224760850620955394187294241739462394, "cwe": ["CWE-787"], "cve": "CVE-2022-2923", "cve_desc": "NULL Pointer Dereference in GitHub repository vim/vim prior to 9.0.0240.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-2923", "file_path": "src/spellfile.c"}
{"idx": 432093, "project": "vim", "commit_id": "6669de1b235843968e88844ca6d3c8dec4b01a9e", "project_url": "https://github.com/vim/vim", "commit_url": "https://github.com/vim/vim/commit/6669de1b235843968e88844ca6d3c8dec4b01a9e", "commit_message": "patch 9.0.0240: crash when using \":mkspell\" with an empty .dic file\n\nProblem:    Crash when using \":mkspell\" with an empty .dic file.\nSolution:   Check for an empty word tree.", "target": 0, "func": "sug_filltree(spellinfo_T *spin, slang_T *slang)\n{\n    char_u\t*byts;\n    idx_T\t*idxs;\n    int\t\tdepth;\n    idx_T\tarridx[MAXWLEN];\n    int\t\tcuri[MAXWLEN];\n    char_u\ttword[MAXWLEN];\n    char_u\ttsalword[MAXWLEN];\n    int\t\tc;\n    idx_T\tn;\n    unsigned\twords_done = 0;\n    int\t\twordcount[MAXWLEN];\n\n    // We use si_foldroot for the soundfolded trie.\n    spin->si_foldroot = wordtree_alloc(spin);\n    if (spin->si_foldroot == NULL)\n\treturn FAIL;\n\n    // let tree_add_word() know we're adding to the soundfolded tree\n    spin->si_sugtree = TRUE;\n\n    /*\n     * Go through the whole case-folded tree, soundfold each word and put it\n     * in the trie.  Bail out if the tree is empty.\n     */\n    byts = slang->sl_fbyts;\n    idxs = slang->sl_fidxs;\n    if (byts == NULL || idxs == NULL)\n\treturn FAIL;\n\n    arridx[0] = 0;\n    curi[0] = 1;\n    wordcount[0] = 0;\n\n    depth = 0;\n    while (depth >= 0 && !got_int)\n    {\n\tif (curi[depth] > byts[arridx[depth]])\n\t{\n\t    // Done all bytes at this node, go up one level.\n\t    idxs[arridx[depth]] = wordcount[depth];\n\t    if (depth > 0)\n\t\twordcount[depth - 1] += wordcount[depth];\n\n\t    --depth;\n\t    line_breakcheck();\n\t}\n\telse\n\t{\n\n\t    // Do one more byte at this node.\n\t    n = arridx[depth] + curi[depth];\n\t    ++curi[depth];\n\n\t    c = byts[n];\n\t    if (c == 0)\n\t    {\n\t\t// Sound-fold the word.\n\t\ttword[depth] = NUL;\n\t\tspell_soundfold(slang, tword, TRUE, tsalword);\n\n\t\t// We use the \"flags\" field for the MSB of the wordnr,\n\t\t// \"region\" for the LSB of the wordnr.\n\t\tif (tree_add_word(spin, tsalword, spin->si_foldroot,\n\t\t\t\twords_done >> 16, words_done & 0xffff,\n\t\t\t\t\t\t\t   0) == FAIL)\n\t\t    return FAIL;\n\n\t\t++words_done;\n\t\t++wordcount[depth];\n\n\t\t// Reset the block count each time to avoid compression\n\t\t// kicking in.\n\t\tspin->si_blocks_cnt = 0;\n\n\t\t// Skip over any other NUL bytes (same word with different\n\t\t// flags).  But don't go over the end.\n\t\twhile (n + 1 < slang->sl_fbyts_len && byts[n + 1] == 0)\n\t\t{\n\t\t    ++n;\n\t\t    ++curi[depth];\n\t\t}\n\t    }\n\t    else\n\t    {\n\t\t// Normal char, go one level deeper.\n\t\ttword[depth++] = c;\n\t\tarridx[depth] = idxs[n];\n\t\tcuri[depth] = 1;\n\t\twordcount[depth] = 0;\n\t    }\n\t}\n    }\n\n    smsg(_(\"Total number of words: %d\"), words_done);\n\n    return OK;\n}", "func_hash": 208510972034693515698183125739179180871, "file_name": "spellfile.c", "file_hash": 159233725116993104212150761027929059833, "cwe": ["CWE-787"], "cve": "CVE-2022-2923", "cve_desc": "NULL Pointer Dereference in GitHub repository vim/vim prior to 9.0.0240.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-2923", "file_path": "src/spellfile.c"}
{"idx": 210273, "project": "mongo", "commit_id": "f3604b901d688c194de5e430c7fbab060c9dc8e0", "project_url": "https://github.com/mongodb/mongo", "commit_url": "https://github.com/mongodb/mongo/commit/f3604b901d688c194de5e430c7fbab060c9dc8e0", "commit_message": "SERVER-59071 Treat '$sample' as unsharded when connecting directly to shards", "target": 1, "func": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    // If the incoming operation is sharded, use the CSS to infer the filtering metadata for the\n    // collection, otherwise treat it as unsharded\n    auto collectionFilter =\n        CollectionShardingState::get(opCtx, coll->ns())\n            ->getOwnershipFilter(\n                opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (collectionFilter.isSharded() && !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // We can't take ARHASH optimization path for a direct $sample on the system.buckets\n        // collection because data is in compressed form. If we did have a direct $sample on the\n        // system.buckets collection, then the 'bucketUnpacker' would not be set up properly. We\n        // also should bail out early if a $sample is made against a time series collection that is\n        // empty. If we don't the 'minAdvancedToWorkRatio' can be nan/-nan depending on the\n        // architecture.\n        if (!(bucketUnpacker && numRecords)) {\n            return std::pair{nullptr, false};\n        }\n\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}", "func_hash": 239583334467362355681176105432769486304, "file_name": "pipeline_d.cpp", "file_hash": 58061396446339864036845237439152201402, "cwe": ["CWE-617"], "cve": "CVE-2021-32037", "cve_desc": "An authorized user may trigger an invariant which may result in denial of service or server exit if a relevant aggregation request is sent to a shard. Usually, the requests are sent via mongos and special privileges are required in order to know the address of the shards and to log in to the shards of an auth enabled environment.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-32037", "file_path": "src/mongo/db/pipeline/pipeline_d.cpp"}
{"idx": 432152, "project": "mongo", "commit_id": "f3604b901d688c194de5e430c7fbab060c9dc8e0", "project_url": "https://github.com/mongodb/mongo", "commit_url": "https://github.com/mongodb/mongo/commit/f3604b901d688c194de5e430c7fbab060c9dc8e0", "commit_message": "SERVER-59071 Treat '$sample' as unsharded when connecting directly to shards", "target": 0, "func": "createRandomCursorExecutor(const CollectionPtr& coll,\n                           const boost::intrusive_ptr<ExpressionContext>& expCtx,\n                           long long sampleSize,\n                           long long numRecords,\n                           boost::optional<BucketUnpacker> bucketUnpacker) {\n    OperationContext* opCtx = expCtx->opCtx;\n\n    // Verify that we are already under a collection lock. We avoid taking locks ourselves in this\n    // function because double-locking forces any PlanExecutor we create to adopt a NO_YIELD policy.\n    invariant(opCtx->lockState()->isCollectionLockedForMode(coll->ns(), MODE_IS));\n\n    static const double kMaxSampleRatioForRandCursor = 0.05;\n    if (!expCtx->ns.isTimeseriesBucketsCollection()) {\n        if (sampleSize > numRecords * kMaxSampleRatioForRandCursor || numRecords <= 100) {\n            return std::pair{nullptr, false};\n        }\n    } else {\n        // Suppose that a time-series bucket collection is observed to contain 200 buckets, and the\n        // 'gTimeseriesBucketMaxCount' parameter is set to 1000. If all buckets are full, then the\n        // maximum possible measurment count would be 200 * 1000 = 200,000. While the\n        // 'SampleFromTimeseriesBucket' plan is more efficient when the sample size is small\n        // relative to the total number of measurements in the time-series collection, for larger\n        // sample sizes the top-k sort based sample is faster. Experiments have approximated that\n        // the tipping point is roughly when the requested sample size is greater than 1% of the\n        // maximum possible number of measurements in the collection (i.e. numBuckets *\n        // maxMeasurementsPerBucket).\n        static const double kCoefficient = 0.01;\n        if (sampleSize > kCoefficient * numRecords * gTimeseriesBucketMaxCount) {\n            return std::pair{nullptr, false};\n        }\n    }\n\n    // Attempt to get a random cursor from the RecordStore.\n    auto rsRandCursor = coll->getRecordStore()->getRandomCursor(opCtx);\n    if (!rsRandCursor) {\n        // The storage engine has no random cursor support.\n        return std::pair{nullptr, false};\n    }\n\n    // Build a MultiIteratorStage and pass it the random-sampling RecordCursor.\n    auto ws = std::make_unique<WorkingSet>();\n    std::unique_ptr<PlanStage> root =\n        std::make_unique<MultiIteratorStage>(expCtx.get(), ws.get(), coll);\n    static_cast<MultiIteratorStage*>(root.get())->addIterator(std::move(rsRandCursor));\n\n    TrialStage* trialStage = nullptr;\n\n    // Because 'numRecords' includes orphan documents, our initial decision to optimize the $sample\n    // cursor may have been mistaken. For sharded collections, build a TRIAL plan that will switch\n    // to a collection scan if the ratio of orphaned to owned documents encountered over the first\n    // 100 works() is such that we would have chosen not to optimize.\n    static const size_t kMaxPresampleSize = 100;\n    if (auto css = CollectionShardingState::get(opCtx, coll->ns());\n        css->getCollectionDescription(opCtx).isSharded() &&\n        !expCtx->ns.isTimeseriesBucketsCollection()) {\n        // The ratio of owned to orphaned documents must be at least equal to the ratio between the\n        // requested sampleSize and the maximum permitted sampleSize for the original constraints to\n        // be satisfied. For instance, if there are 200 documents and the sampleSize is 5, then at\n        // least (5 / (200*0.05)) = (5/10) = 50% of those documents must be owned. If less than 5%\n        // of the documents in the collection are owned, we default to the backup plan.\n        const auto minAdvancedToWorkRatio = std::max(\n            sampleSize / (numRecords * kMaxSampleRatioForRandCursor), kMaxSampleRatioForRandCursor);\n        // Since the incoming operation is sharded, use the CSS to infer the filtering metadata for\n        // the collection. We get the shard ownership filter after checking to see if the collection\n        // is sharded to avoid an invariant from being fired in this call.\n        auto collectionFilter = css->getOwnershipFilter(\n            opCtx, CollectionShardingState::OrphanCleanupPolicy::kDisallowOrphanCleanup);\n        // The trial plan is SHARDING_FILTER-MULTI_ITERATOR.\n        auto randomCursorPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(root));\n        // The backup plan is SHARDING_FILTER-COLLSCAN.\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n        collScanPlan = std::make_unique<ShardFilterStage>(\n            expCtx.get(), collectionFilter, ws.get(), std::move(collScanPlan));\n        // Place a TRIAL stage at the root of the plan tree, and pass it the trial and backup plans.\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(randomCursorPlan),\n                                            std::move(collScanPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    } else if (expCtx->ns.isTimeseriesBucketsCollection()) {\n        // We can't take ARHASH optimization path for a direct $sample on the system.buckets\n        // collection because data is in compressed form. If we did have a direct $sample on the\n        // system.buckets collection, then the 'bucketUnpacker' would not be set up properly. We\n        // also should bail out early if a $sample is made against a time series collection that is\n        // empty. If we don't the 'minAdvancedToWorkRatio' can be nan/-nan depending on the\n        // architecture.\n        if (!(bucketUnpacker && numRecords)) {\n            return std::pair{nullptr, false};\n        }\n\n        // Use a 'TrialStage' to run a trial between 'SampleFromTimeseriesBucket' and\n        // 'UnpackTimeseriesBucket' with $sample left in the pipeline in-place. If the buckets are\n        // not sufficiently full, or the 'SampleFromTimeseriesBucket' plan draws too many\n        // duplicates, then we will fall back to the 'TrialStage' backup plan. This backup plan uses\n        // the top-k sort sampling approach.\n        //\n        // Suppose the 'gTimeseriesBucketMaxCount' is 1000, but each bucket only contains 500\n        // documents on average. The observed trial advanced/work ratio approximates the average\n        // bucket fullness, noted here as \"abf\". In this example, abf = 500 / 1000 = 0.5.\n        // Experiments have shown that the optimized 'SampleFromTimeseriesBucket' algorithm performs\n        // better than backup plan when\n        //\n        //     sampleSize < 0.02 * abf * numRecords * gTimeseriesBucketMaxCount\n        //\n        //  This inequality can be rewritten as\n        //\n        //     abf > sampleSize / (0.02 * numRecords * gTimeseriesBucketMaxCount)\n        //\n        // Therefore, if the advanced/work ratio exceeds this threshold, we will use the\n        // 'SampleFromTimeseriesBucket' plan. Note that as the sample size requested by the user\n        // becomes larger with respect to the number of buckets, we require a higher advanced/work\n        // ratio in order to justify using 'SampleFromTimeseriesBucket'.\n        //\n        // Additionally, we require the 'TrialStage' to approximate the abf as at least 0.25. When\n        // buckets are mostly empty, the 'SampleFromTimeseriesBucket' will be inefficient due to a\n        // lot of sampling \"misses\".\n        static const auto kCoefficient = 0.02;\n        static const auto kMinBucketFullness = 0.25;\n        const auto minAdvancedToWorkRatio = std::max(\n            std::min(sampleSize / (kCoefficient * numRecords * gTimeseriesBucketMaxCount), 1.0),\n            kMinBucketFullness);\n\n        auto arhashPlan = std::make_unique<SampleFromTimeseriesBucket>(\n            expCtx.get(),\n            ws.get(),\n            std::move(root),\n            *bucketUnpacker,\n            // By using a quantity slightly higher than 'kMaxPresampleSize', we ensure that the\n            // 'SampleFromTimeseriesBucket' stage won't fail due to too many consecutive sampling\n            // attempts during the 'TrialStage's trial period.\n            kMaxPresampleSize + 5,\n            sampleSize,\n            gTimeseriesBucketMaxCount);\n\n        std::unique_ptr<PlanStage> collScanPlan = std::make_unique<CollectionScan>(\n            expCtx.get(), coll, CollectionScanParams{}, ws.get(), nullptr);\n\n        auto topkSortPlan = std::make_unique<UnpackTimeseriesBucket>(\n            expCtx.get(), ws.get(), std::move(collScanPlan), *bucketUnpacker);\n\n        root = std::make_unique<TrialStage>(expCtx.get(),\n                                            ws.get(),\n                                            std::move(arhashPlan),\n                                            std::move(topkSortPlan),\n                                            kMaxPresampleSize,\n                                            minAdvancedToWorkRatio);\n        trialStage = static_cast<TrialStage*>(root.get());\n    }\n\n    auto execStatus = plan_executor_factory::make(expCtx,\n                                                  std::move(ws),\n                                                  std::move(root),\n                                                  &coll,\n                                                  opCtx->inMultiDocumentTransaction()\n                                                      ? PlanYieldPolicy::YieldPolicy::INTERRUPT_ONLY\n                                                      : PlanYieldPolicy::YieldPolicy::YIELD_AUTO,\n                                                  QueryPlannerParams::RETURN_OWNED_DATA);\n    if (!execStatus.isOK()) {\n        return execStatus.getStatus();\n    }\n\n    // For sharded collections, the root of the plan tree is a TrialStage that may have chosen\n    // either a random-sampling cursor trial plan or a COLLSCAN backup plan. We can only optimize\n    // the $sample aggregation stage if the trial plan was chosen.\n    return std::pair{std::move(execStatus.getValue()),\n                     !trialStage || !trialStage->pickedBackupPlan()};\n}", "func_hash": 248936200771856096728172359306392383142, "file_name": "pipeline_d.cpp", "file_hash": 250256559163646850106921369320994189676, "cwe": ["CWE-617"], "cve": "CVE-2021-32037", "cve_desc": "An authorized user may trigger an invariant which may result in denial of service or server exit if a relevant aggregation request is sent to a shard. Usually, the requests are sent via mongos and special privileges are required in order to know the address of the shards and to log in to the shards of an auth enabled environment.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-32037", "file_path": "src/mongo/db/pipeline/pipeline_d.cpp"}
