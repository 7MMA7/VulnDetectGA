{"idx": 197760, "project": "tensorflow", "commit_id": "bb6a0383ed553c286f87ca88c207f6774d5c4a8f", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f", "commit_message": "Prevent heap OOB read in TFLite's `gather_nd.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387208551\nChange-Id: I6b7a8a62d3e7c13a16d81619e5bc23ae2cdbc7fd", "target": 1, "func": "TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\n                          const TfLiteTensor* indices, TfLiteTensor* output) {\n  switch (params->type) {\n    case kTfLiteFloat32:\n      return GatherNd<float, IndicesT>(params, indices, output);\n    case kTfLiteUInt8:\n      return GatherNd<uint8_t, IndicesT>(params, indices, output);\n    case kTfLiteInt8:\n      return GatherNd<int8_t, IndicesT>(params, indices, output);\n    case kTfLiteInt16:\n      return GatherNd<int16_t, IndicesT>(params, indices, output);\n    case kTfLiteInt32:\n      return GatherNd<int32_t, IndicesT>(params, indices, output);\n    case kTfLiteInt64:\n      return GatherNd<int64_t, IndicesT>(params, indices, output);\n    case kTfLiteString:\n      return GatherNdString<IndicesT>(params, indices, output);\n    default:\n      context->ReportError(context,\n                           \"Params type '%s' are not supported by gather_nd.\",\n                           TfLiteTypeGetName(params->type));\n      return kTfLiteError;\n  }\n}", "func_hash": 33385980683805390111226215596644506391, "file_name": "gather_nd.cc", "file_hash": 316439389085787575038399494263818469682, "cwe": ["CWE-125"], "cve": "CVE-2021-37687", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions TFLite's [`GatherNd` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather_nd.cc#L124) does not support negative indices but there are no checks for this situation. Hence, an attacker can read arbitrary data from the heap by carefully crafting a model with negative values in `indices`. Similar issue exists in [`Gather` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather.cc). We have patched the issue in GitHub commits bb6a0383ed553c286f87ca88c207f6774d5c4a8f and eb921122119a6b6e470ee98b89e65d721663179d. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37687", "file_path": "tensorflow/lite/kernels/gather_nd.cc"}
{"idx": 263521, "project": "tensorflow", "commit_id": "bb6a0383ed553c286f87ca88c207f6774d5c4a8f", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f", "commit_message": "Prevent heap OOB read in TFLite's `gather_nd.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387208551\nChange-Id: I6b7a8a62d3e7c13a16d81619e5bc23ae2cdbc7fd", "target": 0, "func": "TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\n                          const TfLiteTensor* indices, TfLiteTensor* output) {\n  bool indices_has_only_positive_elements = true;\n  const auto* indices_values = GetTensorData<IndicesT>(indices);\n  const size_t num_indices = indices->bytes / sizeof(IndicesT);\n  for (size_t i = 0; i < num_indices; i++) {\n    if (indices_values[i] < 0) {\n      indices_has_only_positive_elements = false;\n      break;\n    }\n  }\n  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n\n  switch (params->type) {\n    case kTfLiteFloat32:\n      return GatherNd<float, IndicesT>(params, indices, output);\n    case kTfLiteUInt8:\n      return GatherNd<uint8_t, IndicesT>(params, indices, output);\n    case kTfLiteInt8:\n      return GatherNd<int8_t, IndicesT>(params, indices, output);\n    case kTfLiteInt16:\n      return GatherNd<int16_t, IndicesT>(params, indices, output);\n    case kTfLiteInt32:\n      return GatherNd<int32_t, IndicesT>(params, indices, output);\n    case kTfLiteInt64:\n      return GatherNd<int64_t, IndicesT>(params, indices, output);\n    case kTfLiteString:\n      return GatherNdString<IndicesT>(params, indices, output);\n    default:\n      context->ReportError(context,\n                           \"Params type '%s' are not supported by gather_nd.\",\n                           TfLiteTypeGetName(params->type));\n      return kTfLiteError;\n  }\n}", "func_hash": 279577390165387156722866411822496673151, "file_name": "gather_nd.cc", "file_hash": 142063643007794638174357713974685200472, "cwe": ["CWE-125"], "cve": "CVE-2021-37687", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions TFLite's [`GatherNd` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather_nd.cc#L124) does not support negative indices but there are no checks for this situation. Hence, an attacker can read arbitrary data from the heap by carefully crafting a model with negative values in `indices`. Similar issue exists in [`Gather` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather.cc). We have patched the issue in GitHub commits bb6a0383ed553c286f87ca88c207f6774d5c4a8f and eb921122119a6b6e470ee98b89e65d721663179d. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37687", "file_path": "tensorflow/lite/kernels/gather_nd.cc"}
{"idx": 197801, "project": "tensorflow", "commit_id": "368af875869a204b4ac552b9ddda59f6a46a56ec", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/368af875869a204b4ac552b9ddda59f6a46a56ec", "commit_message": "Avoid buffer overflow when loading tensors with insufficient data from checkpoints.\n\n`CopyDataFromTensorSliceToTensorSlice` does not (and cannot conveniently)\nprovide any bounds checking on its own, so the size is instead checked prior\nto passing unvalidated data to that function.\n\nPiperOrigin-RevId: 392971286\nChange-Id: If2073b36d4d5eedd386329f56729395fd7effee1", "target": 1, "func": "bool TensorSliceReader::CopySliceData(const string& name,\n                                      const TensorSlice& slice, T* data) const {\n  std::vector<std::pair<TensorSlice, string>> details;\n  const TensorSliceSet* tss;\n  {\n    mutex_lock l(mu_);\n    tss = FindTensorSlice(name, slice, &details);\n    if (!tss && !all_shards_loaded_) {\n      VLOG(1) << \"Did not find slice in preferred shard, loading all shards.\"\n              << name << \": \" << slice.DebugString();\n      LoadAllShards();\n      tss = FindTensorSlice(name, slice, &details);\n    }\n    if (!tss) {\n      // No such tensor\n      return false;\n    }\n  }\n  // We have the data -- copy it over.\n  string value;\n  for (const auto& x : details) {\n    const TensorSlice& slice_s = x.first;\n    const string& fname = x.second;\n    int idx = gtl::FindWithDefault(fname_to_index_, fname, -1);\n    CHECK_GE(idx, 0) << \"Failed to find the index for filename \" << fname;\n    // We read a record in the corresponding sstable\n    const string key = EncodeTensorNameSlice(name, slice_s);\n    if (!sss_[idx]->Get(key, &value)) {\n      VLOG(1) << \"Failed to seek to the record for tensor \" << name\n              << \", slice \" << slice_s.DebugString()\n              << \": computed key = \" << key;\n      return false;\n    }\n    SavedTensorSlices sts;\n    if (!ParseProtoUnlimited(&sts, value)) {\n      VLOG(1) << \"Failed to parse the record for tensor \" << name << \", slice \"\n              << slice_s.DebugString() << \": computed key = \" << key;\n      return false;\n    }\n    CopyDataFromTensorSliceToTensorSlice(\n        tss->shape(), slice_s, slice,\n        checkpoint::TensorProtoData<T>(sts.data().data()), data);\n  }\n  return true;\n}", "func_hash": 253938811692403617918500480067513726895, "file_name": "tensor_slice_reader.h", "file_hash": 240433137636754905402215095789731366712, "cwe": ["CWE-345"], "cve": "CVE-2021-41203", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions an attacker can trigger undefined behavior, integer overflows, segfaults and `CHECK`-fail crashes if they can change saved checkpoints from outside of TensorFlow. This is because the checkpoints loading infrastructure is missing validation for invalid file formats. The fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41203", "file_path": "tensorflow/core/util/tensor_slice_reader.h"}
{"idx": 264367, "project": "tensorflow", "commit_id": "368af875869a204b4ac552b9ddda59f6a46a56ec", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/368af875869a204b4ac552b9ddda59f6a46a56ec", "commit_message": "Avoid buffer overflow when loading tensors with insufficient data from checkpoints.\n\n`CopyDataFromTensorSliceToTensorSlice` does not (and cannot conveniently)\nprovide any bounds checking on its own, so the size is instead checked prior\nto passing unvalidated data to that function.\n\nPiperOrigin-RevId: 392971286\nChange-Id: If2073b36d4d5eedd386329f56729395fd7effee1", "target": 0, "func": "bool TensorSliceReader::CopySliceData(const string& name,\n                                      const TensorSlice& slice, T* data) const {\n  std::vector<std::pair<TensorSlice, string>> details;\n  const TensorSliceSet* tss;\n  {\n    mutex_lock l(mu_);\n    tss = FindTensorSlice(name, slice, &details);\n    if (!tss && !all_shards_loaded_) {\n      VLOG(1) << \"Did not find slice in preferred shard, loading all shards.\"\n              << name << \": \" << slice.DebugString();\n      LoadAllShards();\n      tss = FindTensorSlice(name, slice, &details);\n    }\n    if (!tss) {\n      // No such tensor\n      return false;\n    }\n  }\n  // We have the data -- copy it over.\n  string value;\n  for (const auto& x : details) {\n    const TensorSlice& slice_s = x.first;\n    const string& fname = x.second;\n    int idx = gtl::FindWithDefault(fname_to_index_, fname, -1);\n    CHECK_GE(idx, 0) << \"Failed to find the index for filename \" << fname;\n    // We read a record in the corresponding sstable\n    const string key = EncodeTensorNameSlice(name, slice_s);\n    if (!sss_[idx]->Get(key, &value)) {\n      VLOG(1) << \"Failed to seek to the record for tensor \" << name\n              << \", slice \" << slice_s.DebugString()\n              << \": computed key = \" << key;\n      return false;\n    }\n    SavedTensorSlices sts;\n    if (!ParseProtoUnlimited(&sts, value)) {\n      VLOG(1) << \"Failed to parse the record for tensor \" << name << \", slice \"\n              << slice_s.DebugString() << \": computed key = \" << key;\n      return false;\n    }\n    // Ensure the TensorSlice contains the expected amount of data.\n    TensorShape shp_s;\n    Status s = slice_s.SliceTensorShape(tss->shape(), &shp_s);\n    if (!s.ok()) {\n      VLOG(1) << \"Failed to slice tensor \" << name << \", slice \"\n              << slice_s.DebugString() << \": \" << s;\n      return false;\n    }\n    if (checkpoint::TensorProtoDataSize<T>(sts.data().data()) !=\n        shp_s.num_elements()) {\n      VLOG(1) << \"Tensor \" << name << \", slice \" << slice_s.DebugString()\n              << \" had an unexpected amount of data: expected = \"\n              << shp_s.num_elements() << \", got = \"\n              << checkpoint::TensorProtoDataSize<T>(sts.data().data());\n      return false;\n    }\n    CopyDataFromTensorSliceToTensorSlice(\n        tss->shape(), slice_s, slice,\n        checkpoint::TensorProtoData<T>(sts.data().data()), data);\n  }\n  return true;\n}", "func_hash": 260958144262557057718377368328924715257, "file_name": "tensor_slice_reader.h", "file_hash": 20474431130261957581302899955079552996, "cwe": ["CWE-345"], "cve": "CVE-2021-41203", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions an attacker can trigger undefined behavior, integer overflows, segfaults and `CHECK`-fail crashes if they can change saved checkpoints from outside of TensorFlow. This is because the checkpoints loading infrastructure is missing validation for invalid file formats. The fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41203", "file_path": "tensorflow/core/util/tensor_slice_reader.h"}
{"idx": 197824, "project": "gpac", "commit_id": "c535bad50d5812d27ee5b22b54371bddec411514", "project_url": "https://github.com/gpac/gpac", "commit_url": "https://github.com/gpac/gpac/commit/c535bad50d5812d27ee5b22b54371bddec411514", "commit_message": "fixed #2194", "target": 1, "func": "static GF_Err BM_ParseGlobalQuantizer(GF_BifsDecoder *codec, GF_BitStream *bs, GF_List *com_list)\n{\n\tGF_Node *node;\n\tGF_Command *com;\n\tGF_CommandField *inf;\n\tnode = gf_bifs_dec_node(codec, bs, NDT_SFWorldNode);\n\tif (!node) return GF_NON_COMPLIANT_BITSTREAM;\n\n\t/*reset global QP*/\n\tif (codec->scenegraph->global_qp) {\n\t\tgf_node_unregister(codec->scenegraph->global_qp, NULL);\n\t}\n\tcodec->ActiveQP = NULL;\n\tcodec->scenegraph->global_qp = NULL;\n\n\tif (gf_node_get_tag(node) != TAG_MPEG4_QuantizationParameter) {\n\t\tgf_node_unregister(node, NULL);\n\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t}\n\n\t/*register global QP*/\n\tcodec->ActiveQP = (M_QuantizationParameter *) node;\n\tcodec->ActiveQP->isLocal = 0;\n\tcodec->scenegraph->global_qp = node;\n\n\t/*register TWICE: once for the command, and for the scenegraph globalQP*/\n\tnode->sgprivate->num_instances = 2;\n\n\tcom = gf_sg_command_new(codec->current_graph, GF_SG_GLOBAL_QUANTIZER);\n\tinf = gf_sg_command_field_new(com);\n\tinf->new_node = node;\n\tinf->field_ptr = &inf->new_node;\n\tinf->fieldType = GF_SG_VRML_SFNODE;\n\tgf_list_add(com_list, com);\n\treturn GF_OK;\n}", "func_hash": 227330749119032349040137878791232526501, "file_name": "memory_decoder.c", "file_hash": 144617483436873036933597599292760665537, "cwe": ["CWE-416"], "cve": "CVE-2022-1795", "cve_desc": "Use After Free in GitHub repository gpac/gpac prior to v2.1.0-DEV.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-1795", "file_path": "src/bifs/memory_decoder.c"}
{"idx": 264657, "project": "gpac", "commit_id": "c535bad50d5812d27ee5b22b54371bddec411514", "project_url": "https://github.com/gpac/gpac", "commit_url": "https://github.com/gpac/gpac/commit/c535bad50d5812d27ee5b22b54371bddec411514", "commit_message": "fixed #2194", "target": 0, "func": "static GF_Err BM_ParseGlobalQuantizer(GF_BifsDecoder *codec, GF_BitStream *bs, GF_List *com_list)\n{\n\tGF_Node *node;\n\tGF_Command *com;\n\tGF_CommandField *inf;\n\tnode = gf_bifs_dec_node(codec, bs, NDT_SFWorldNode);\n\tif (!node) return GF_NON_COMPLIANT_BITSTREAM;\n\n\t/*reset global QP*/\n\tif (codec->scenegraph->global_qp) {\n\t\tgf_node_unregister(codec->scenegraph->global_qp, NULL);\n\t}\n\tcodec->ActiveQP = NULL;\n\tcodec->scenegraph->global_qp = NULL;\n\n\tif (gf_node_get_tag(node) != TAG_MPEG4_QuantizationParameter) {\n\t\t//if node was just created (num_instances == 0), unregister\n\t\t//otherwise (USE node) don't do anything\n\t\tif (!node->sgprivate->num_instances) {\n\t\t\tnode->sgprivate->num_instances = 1;\n\t\t\tgf_node_unregister(node, NULL);\n\t\t}\n\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t}\n\n\t/*register global QP*/\n\tcodec->ActiveQP = (M_QuantizationParameter *) node;\n\tcodec->ActiveQP->isLocal = 0;\n\tcodec->scenegraph->global_qp = node;\n\n\t/*register TWICE: once for the command, and for the scenegraph globalQP*/\n\tgf_node_unregister(node, NULL);\n\tgf_node_unregister(node, NULL);\n\n\tcom = gf_sg_command_new(codec->current_graph, GF_SG_GLOBAL_QUANTIZER);\n\tinf = gf_sg_command_field_new(com);\n\tinf->new_node = node;\n\tinf->field_ptr = &inf->new_node;\n\tinf->fieldType = GF_SG_VRML_SFNODE;\n\tgf_list_add(com_list, com);\n\treturn GF_OK;\n}", "func_hash": 121103708905571280232350867601982124843, "file_name": "memory_decoder.c", "file_hash": 38461926007506254402005076671567829745, "cwe": ["CWE-416"], "cve": "CVE-2022-1795", "cve_desc": "Use After Free in GitHub repository gpac/gpac prior to v2.1.0-DEV.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-1795", "file_path": "src/bifs/memory_decoder.c"}
{"idx": 197826, "project": "tensorflow", "commit_id": "7731e8dfbe4a56773be5dc94d631611211156659", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659", "commit_message": "Don't constant-fold DT_RESOURCE constants.\n\nPiperOrigin-RevId: 391803952\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "target": 1, "func": "bool IsConstantFoldable(\n    const Node* n,\n    const std::unordered_map<string, std::vector<PartialTensorShape>>*\n        shape_map,\n    const std::function<bool(const Node*)>& consider,\n    int64_t max_constant_size_in_bytes,\n    std::unordered_map<const Node*, std::vector<Tensor>>*\n        shape_replacement_map) {\n  if (n->IsConstant()) {\n    return true;\n  }\n  if (MaybeReplaceShapeOp(n, shape_map, shape_replacement_map)) {\n    return true;\n  }\n  if (n->op_def().is_stateful()) {\n    return false;\n  }\n  if (consider && !consider(n)) {\n    return false;\n  }\n  if (shape_map != nullptr) {\n    // We can skip the node if an output is known to be oversized.\n    auto shape_it = shape_map->find(n->name());\n    if (shape_it != shape_map->end()) {\n      for (int64_t i = 0; i < shape_it->second.size(); ++i) {\n        const auto& out_shape = shape_it->second[i];\n        if (out_shape.IsFullyDefined() &&\n            out_shape.num_elements() * DataTypeSize(n->output_type(i)) >\n                max_constant_size_in_bytes) {\n          return false;\n        }\n      }\n    }\n  }\n  if (n->IsControlFlow() || n->IsSend() || n->IsRecv()) {\n    return false;\n  }\n  // TODO(yuanbyu): For now disable these session handle operations.\n  if (n->IsGetSessionHandle() || n->IsGetSessionTensor() ||\n      n->IsDeleteSessionTensor()) {\n    return false;\n  }\n  if (n->IsSource()) {\n    return false;\n  }\n  if (n->IsSink()) {\n    return false;\n  }\n  if (n->IsFakeParam()) {\n    return false;\n  }\n  // Since constant-folding runs on the CPU, do not attempt to constant-fold\n  // operators that have no CPU kernel. Also implies that we will not\n  // constant-fold functions.\n  // TODO(phawkins): allow constant-folding for functions; functions may\n  // be arbitrarily expensive to execute.\n  if (!KernelDefAvailable(DeviceType(DEVICE_CPU), n->def())) {\n    return false;\n  }\n  // Do not constant fold nodes which will be allocated by ScopedAllocator.\n  // This is because the constant-folding graph will not contain the\n  // `_ScopedAllocator` node, and that is necessary to be able to run a node\n  // that will use this allocator.\n  if (n->attrs().Find(kScopedAllocatorAttrName) != nullptr) {\n    VLOG(2) << \"Skip node [\" << n->DebugString()\n            << \"] for constant folding due to scoped allocator\";\n    return false;\n  }\n  return true;\n}", "func_hash": 116289485656616917830363077368123441202, "file_name": "constant_folding.cc", "file_hash": 46768745532828534791253050765124097339, "cwe": ["CWE-824"], "cve": "CVE-2021-41204", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions during TensorFlow's Grappler optimizer phase, constant folding might attempt to deep copy a resource tensor. This results in a segfault, as these tensors are supposed to not change. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41204", "file_path": "tensorflow/core/common_runtime/constant_folding.cc"}
{"idx": 264715, "project": "tensorflow", "commit_id": "7731e8dfbe4a56773be5dc94d631611211156659", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659", "commit_message": "Don't constant-fold DT_RESOURCE constants.\n\nPiperOrigin-RevId: 391803952\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "target": 0, "func": "bool IsConstantFoldable(\n    const Node* n,\n    const std::unordered_map<string, std::vector<PartialTensorShape>>*\n        shape_map,\n    const std::function<bool(const Node*)>& consider,\n    int64_t max_constant_size_in_bytes,\n    std::unordered_map<const Node*, std::vector<Tensor>>*\n        shape_replacement_map) {\n  if (n->IsConstant()) {\n    // Skip constant folding resources as they cannot be deep copied.\n    return n->output_type(0) != DT_RESOURCE;\n  }\n  if (MaybeReplaceShapeOp(n, shape_map, shape_replacement_map)) {\n    return true;\n  }\n  if (n->op_def().is_stateful()) {\n    return false;\n  }\n  if (consider && !consider(n)) {\n    return false;\n  }\n  if (shape_map != nullptr) {\n    // We can skip the node if an output is known to be oversized.\n    auto shape_it = shape_map->find(n->name());\n    if (shape_it != shape_map->end()) {\n      for (int64_t i = 0; i < shape_it->second.size(); ++i) {\n        const auto& out_shape = shape_it->second[i];\n        if (out_shape.IsFullyDefined() &&\n            out_shape.num_elements() * DataTypeSize(n->output_type(i)) >\n                max_constant_size_in_bytes) {\n          return false;\n        }\n      }\n    }\n  }\n  if (n->IsControlFlow() || n->IsSend() || n->IsRecv()) {\n    return false;\n  }\n  // TODO(yuanbyu): For now disable these session handle operations.\n  if (n->IsGetSessionHandle() || n->IsGetSessionTensor() ||\n      n->IsDeleteSessionTensor()) {\n    return false;\n  }\n  if (n->IsSource()) {\n    return false;\n  }\n  if (n->IsSink()) {\n    return false;\n  }\n  if (n->IsFakeParam()) {\n    return false;\n  }\n  // Since constant-folding runs on the CPU, do not attempt to constant-fold\n  // operators that have no CPU kernel. Also implies that we will not\n  // constant-fold functions.\n  // TODO(phawkins): allow constant-folding for functions; functions may\n  // be arbitrarily expensive to execute.\n  if (!KernelDefAvailable(DeviceType(DEVICE_CPU), n->def())) {\n    return false;\n  }\n  // Do not constant fold nodes which will be allocated by ScopedAllocator.\n  // This is because the constant-folding graph will not contain the\n  // `_ScopedAllocator` node, and that is necessary to be able to run a node\n  // that will use this allocator.\n  if (n->attrs().Find(kScopedAllocatorAttrName) != nullptr) {\n    VLOG(2) << \"Skip node [\" << n->DebugString()\n            << \"] for constant folding due to scoped allocator\";\n    return false;\n  }\n  return true;\n}", "func_hash": 319400414252695019518928506839820495961, "file_name": "constant_folding.cc", "file_hash": 129656680363802690204267718015734305272, "cwe": ["CWE-824"], "cve": "CVE-2021-41204", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions during TensorFlow's Grappler optimizer phase, constant folding might attempt to deep copy a resource tensor. This results in a segfault, as these tensors are supposed to not change. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41204", "file_path": "tensorflow/core/common_runtime/constant_folding.cc"}
{"idx": 197893, "project": "tensorflow", "commit_id": "eb921122119a6b6e470ee98b89e65d721663179d", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d", "commit_message": "Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8", "target": 1, "func": "TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\n                    const TfLiteTensor* positions, TfLiteTensor* output) {\n  tflite::GatherParams op_params;\n  op_params.axis = params.axis;\n  op_params.batch_dims = params.batch_dims;\n  optimized_ops::Gather(op_params, GetTensorShape(input),\n                        GetTensorData<InputT>(input), GetTensorShape(positions),\n                        GetTensorData<PositionsT>(positions),\n                        GetTensorShape(output), GetTensorData<InputT>(output));\n  return kTfLiteOk;\n}", "func_hash": 291153190876322119376469189416083053275, "file_name": "gather.cc", "file_hash": 109151045550446368977897795663338855851, "cwe": ["CWE-703"], "cve": "CVE-2021-37687", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions TFLite's [`GatherNd` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather_nd.cc#L124) does not support negative indices but there are no checks for this situation. Hence, an attacker can read arbitrary data from the heap by carefully crafting a model with negative values in `indices`. Similar issue exists in [`Gather` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather.cc). We have patched the issue in GitHub commits bb6a0383ed553c286f87ca88c207f6774d5c4a8f and eb921122119a6b6e470ee98b89e65d721663179d. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37687", "file_path": "tensorflow/lite/kernels/gather.cc"}
{"idx": 265432, "project": "tensorflow", "commit_id": "eb921122119a6b6e470ee98b89e65d721663179d", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d", "commit_message": "Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8", "target": 0, "func": "TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\n                    const TfLiteTensor* input, const TfLiteTensor* positions,\n                    TfLiteTensor* output) {\n  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\n  bool indices_has_only_positive_elements = true;\n  const size_t num_indices = positions->bytes / sizeof(PositionsT);\n  for (size_t i = 0; i < num_indices; i++) {\n    if (indexes[i] < 0) {\n      indices_has_only_positive_elements = false;\n      break;\n    }\n  }\n  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n\n  tflite::GatherParams op_params;\n  op_params.axis = params.axis;\n  op_params.batch_dims = params.batch_dims;\n  optimized_ops::Gather(op_params, GetTensorShape(input),\n                        GetTensorData<InputT>(input), GetTensorShape(positions),\n                        GetTensorData<PositionsT>(positions),\n                        GetTensorShape(output), GetTensorData<InputT>(output));\n  return kTfLiteOk;\n}", "func_hash": 50062335077271405437756025482165294383, "file_name": "gather.cc", "file_hash": 61671770353303657730185852955159982272, "cwe": ["CWE-703"], "cve": "CVE-2021-37687", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions TFLite's [`GatherNd` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather_nd.cc#L124) does not support negative indices but there are no checks for this situation. Hence, an attacker can read arbitrary data from the heap by carefully crafting a model with negative values in `indices`. Similar issue exists in [`Gather` implementation](https://github.com/tensorflow/tensorflow/blob/149562d49faa709ea80df1d99fc41d005b81082a/tensorflow/lite/kernels/gather.cc). We have patched the issue in GitHub commits bb6a0383ed553c286f87ca88c207f6774d5c4a8f and eb921122119a6b6e470ee98b89e65d721663179d. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37687", "file_path": "tensorflow/lite/kernels/gather.cc"}
{"idx": 197973, "project": "crun", "commit_id": "1aeeed2e4fdeffb4875c0d0b439915894594c8c6", "project_url": "https://github.com/containers/crun", "commit_url": "https://github.com/containers/crun/commit/1aeeed2e4fdeffb4875c0d0b439915894594c8c6", "commit_message": "exec: --cap do not set inheritable capabilities\n\nCloses: CVE-2022-27650\n\nSigned-off-by: Giuseppe Scrivano <gscrivan@redhat.com>", "target": 1, "func": "crun_command_exec (struct crun_global_arguments *global_args, int argc, char **argv, libcrun_error_t *err)\n{\n  int first_arg = 0, ret = 0;\n  libcrun_context_t crun_context = {\n    0,\n  };\n  cleanup_process_schema runtime_spec_schema_config_schema_process *process = NULL;\n  struct libcrun_container_exec_options_s exec_opts;\n\n  memset (&exec_opts, 0, sizeof (exec_opts));\n  exec_opts.struct_size = sizeof (exec_opts);\n\n  crun_context.preserve_fds = 0;\n  crun_context.listen_fds = 0;\n\n  argp_parse (&run_argp, argc, argv, ARGP_IN_ORDER, &first_arg, &exec_options);\n  crun_assert_n_args (argc - first_arg, exec_options.process ? 1 : 2, -1);\n\n  ret = init_libcrun_context (&crun_context, argv[first_arg], global_args, err);\n  if (UNLIKELY (ret < 0))\n    return ret;\n\n  crun_context.detach = exec_options.detach;\n  crun_context.console_socket = exec_options.console_socket;\n  crun_context.pid_file = exec_options.pid_file;\n  crun_context.preserve_fds = exec_options.preserve_fds;\n\n  if (getenv (\"LISTEN_FDS\"))\n    {\n      crun_context.listen_fds = strtoll (getenv (\"LISTEN_FDS\"), NULL, 10);\n      crun_context.preserve_fds += crun_context.listen_fds;\n    }\n\n  if (exec_options.process)\n    exec_opts.path = exec_options.process;\n  else\n    {\n      process = xmalloc0 (sizeof (*process));\n      int i;\n\n      process->args_len = argc;\n      process->args = xmalloc0 ((argc + 1) * sizeof (*process->args));\n      for (i = 0; i < argc - first_arg; i++)\n        process->args[i] = xstrdup (argv[first_arg + i + 1]);\n      process->args[i] = NULL;\n      if (exec_options.cwd)\n        process->cwd = exec_options.cwd;\n      process->terminal = exec_options.tty;\n      process->env = exec_options.env;\n      process->env_len = exec_options.env_size;\n      process->user = make_oci_process_user (exec_options.user);\n\n      if (exec_options.process_label != NULL)\n        process->selinux_label = exec_options.process_label;\n\n      if (exec_options.apparmor != NULL)\n        process->apparmor_profile = exec_options.apparmor;\n\n      if (exec_options.cap_size > 0)\n        {\n          runtime_spec_schema_config_schema_process_capabilities *capabilities\n              = xmalloc (sizeof (runtime_spec_schema_config_schema_process_capabilities));\n\n          capabilities->effective = exec_options.cap;\n          capabilities->effective_len = exec_options.cap_size;\n\n          capabilities->inheritable = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->inheritable_len = exec_options.cap_size;\n\n          capabilities->bounding = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->bounding_len = exec_options.cap_size;\n\n          capabilities->ambient = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->ambient_len = exec_options.cap_size;\n\n          capabilities->permitted = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->permitted_len = exec_options.cap_size;\n\n          process->capabilities = capabilities;\n        }\n\n      // noNewPriviledges will remain `false` if basespec has `false` unless specified\n      // Default is always `true` in generated basespec config\n      if (exec_options.no_new_privs)\n        process->no_new_privileges = 1;\n\n      exec_opts.process = process;\n    }\n\n  exec_opts.cgroup = exec_options.cgroup;\n\n  return libcrun_container_exec_with_options (&crun_context, argv[first_arg], &exec_opts, err);\n}", "func_hash": 202081994104919800529457023385761628896, "file_name": "exec.c", "file_hash": 10128723194506737000766570571290960247, "cwe": ["CWE-276"], "cve": "CVE-2022-27650", "cve_desc": "A flaw was found in crun where containers were incorrectly started with non-empty default permissions. A vulnerability was found in Moby (Docker Engine) where containers were started incorrectly with non-empty inheritable Linux process capabilities. This flaw allows an attacker with access to programs with inheritable file capabilities to elevate those capabilities to the permitted set when execve(2) runs.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-27650", "file_path": "src/exec.c"}
{"idx": 267357, "project": "crun", "commit_id": "1aeeed2e4fdeffb4875c0d0b439915894594c8c6", "project_url": "https://github.com/containers/crun", "commit_url": "https://github.com/containers/crun/commit/1aeeed2e4fdeffb4875c0d0b439915894594c8c6", "commit_message": "exec: --cap do not set inheritable capabilities\n\nCloses: CVE-2022-27650\n\nSigned-off-by: Giuseppe Scrivano <gscrivan@redhat.com>", "target": 0, "func": "crun_command_exec (struct crun_global_arguments *global_args, int argc, char **argv, libcrun_error_t *err)\n{\n  int first_arg = 0, ret = 0;\n  libcrun_context_t crun_context = {\n    0,\n  };\n  cleanup_process_schema runtime_spec_schema_config_schema_process *process = NULL;\n  struct libcrun_container_exec_options_s exec_opts;\n\n  memset (&exec_opts, 0, sizeof (exec_opts));\n  exec_opts.struct_size = sizeof (exec_opts);\n\n  crun_context.preserve_fds = 0;\n  crun_context.listen_fds = 0;\n\n  argp_parse (&run_argp, argc, argv, ARGP_IN_ORDER, &first_arg, &exec_options);\n  crun_assert_n_args (argc - first_arg, exec_options.process ? 1 : 2, -1);\n\n  ret = init_libcrun_context (&crun_context, argv[first_arg], global_args, err);\n  if (UNLIKELY (ret < 0))\n    return ret;\n\n  crun_context.detach = exec_options.detach;\n  crun_context.console_socket = exec_options.console_socket;\n  crun_context.pid_file = exec_options.pid_file;\n  crun_context.preserve_fds = exec_options.preserve_fds;\n\n  if (getenv (\"LISTEN_FDS\"))\n    {\n      crun_context.listen_fds = strtoll (getenv (\"LISTEN_FDS\"), NULL, 10);\n      crun_context.preserve_fds += crun_context.listen_fds;\n    }\n\n  if (exec_options.process)\n    exec_opts.path = exec_options.process;\n  else\n    {\n      process = xmalloc0 (sizeof (*process));\n      int i;\n\n      process->args_len = argc;\n      process->args = xmalloc0 ((argc + 1) * sizeof (*process->args));\n      for (i = 0; i < argc - first_arg; i++)\n        process->args[i] = xstrdup (argv[first_arg + i + 1]);\n      process->args[i] = NULL;\n      if (exec_options.cwd)\n        process->cwd = exec_options.cwd;\n      process->terminal = exec_options.tty;\n      process->env = exec_options.env;\n      process->env_len = exec_options.env_size;\n      process->user = make_oci_process_user (exec_options.user);\n\n      if (exec_options.process_label != NULL)\n        process->selinux_label = exec_options.process_label;\n\n      if (exec_options.apparmor != NULL)\n        process->apparmor_profile = exec_options.apparmor;\n\n      if (exec_options.cap_size > 0)\n        {\n          runtime_spec_schema_config_schema_process_capabilities *capabilities\n              = xmalloc (sizeof (runtime_spec_schema_config_schema_process_capabilities));\n\n          capabilities->effective = exec_options.cap;\n          capabilities->effective_len = exec_options.cap_size;\n\n          capabilities->inheritable = NULL;\n          capabilities->inheritable_len = 0;\n\n          capabilities->bounding = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->bounding_len = exec_options.cap_size;\n\n          capabilities->ambient = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->ambient_len = exec_options.cap_size;\n\n          capabilities->permitted = dup_array (exec_options.cap, exec_options.cap_size);\n          capabilities->permitted_len = exec_options.cap_size;\n\n          process->capabilities = capabilities;\n        }\n\n      // noNewPriviledges will remain `false` if basespec has `false` unless specified\n      // Default is always `true` in generated basespec config\n      if (exec_options.no_new_privs)\n        process->no_new_privileges = 1;\n\n      exec_opts.process = process;\n    }\n\n  exec_opts.cgroup = exec_options.cgroup;\n\n  return libcrun_container_exec_with_options (&crun_context, argv[first_arg], &exec_opts, err);\n}", "func_hash": 122856137879373398526993920566215406776, "file_name": "exec.c", "file_hash": 213004123111430302918491742718325619465, "cwe": ["CWE-276"], "cve": "CVE-2022-27650", "cve_desc": "A flaw was found in crun where containers were incorrectly started with non-empty default permissions. A vulnerability was found in Moby (Docker Engine) where containers were started incorrectly with non-empty inheritable Linux process capabilities. This flaw allows an attacker with access to programs with inheritable file capabilities to elevate those capabilities to the permitted set when execve(2) runs.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-27650", "file_path": "src/exec.c"}
{"idx": 197998, "project": "tensorflow", "commit_id": "704866eabe03a9aeda044ec91a8d0c83fc1ebdbe", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/704866eabe03a9aeda044ec91a8d0c83fc1ebdbe", "commit_message": "Fix overflow CHECK issue with `tf.raw_ops.UnsortedSegmentJoin`.\n\nPiperOrigin-RevId: 370766155\nChange-Id: I33e7c6626224e1060a8a4ab51ad5d861c6d4c63e", "target": 1, "func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const TensorShape& input_shape = input.shape();\n    const int32 input_dims = input_shape.dims();\n\n    const Tensor& segment_id = context->input(1);\n    const TensorShape& segment_id_shape = segment_id.shape();\n    const int32 segment_dims = segment_id_shape.dims();\n\n    const Tensor& num_segments_tensor = context->input(2);\n    auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n\n    OP_REQUIRES(context, segment_dims != 0,\n                errors::InvalidArgument(\"Segment_id cannot have rank 0\"));\n\n    OP_REQUIRES(\n        context, segment_dims <= input_dims,\n        errors::OutOfRange(\"Invalid segment_id rank \", segment_dims,\n                           \" for input with \", input_dims, \" dimension(s)\"));\n    for (auto i = 0; i < segment_dims; i++) {\n      OP_REQUIRES(\n          context, segment_id_shape.dim_size(i) == input_shape.dim_size(i),\n          errors::InvalidArgument(\n              \"Segment dimension is \", segment_id_shape.dim_size(i),\n              \" while input dimension is \", input_dims, \" in rank \", i));\n    }\n\n    // Making output tensor.\n    Tensor* output_tensor = nullptr;\n    TensorShape output_shape =\n        GetOutputShape(input_shape, segment_id_shape, num_segments);\n    OP_REQUIRES_OK(context, context->allocate_output(\"output\", output_shape,\n                                                     &output_tensor));\n\n    // Preparating flat tensors.\n    auto output_flat = output_tensor->flat<tstring>();\n    auto flat_segment_id = segment_id.flat<INDICES_TYPE>();\n    auto flat_input = input.flat<tstring>();\n\n    for (int i = 0; i < flat_segment_id.size(); i++) {\n      OP_REQUIRES(\n          context,\n          ((flat_segment_id(i) < num_segments) && (flat_segment_id(i) >= 0)),\n          errors::InvalidArgument(\n              \"segment_ids are not allowed to exceed num_segments or\"\n              \" to have negative values.\"));\n    }\n\n    int64 big_stride;\n    int64 small_stride;\n    std::tie(big_stride, small_stride) =\n        GetStrides<INDICES_TYPE>(input_shape, segment_id_shape);\n    auto relative_offset_set =\n        GetFlattenedRelativeOffsets<INDICES_TYPE>(small_stride, big_stride);\n    for (auto start_offset = 0; start_offset < big_stride; start_offset++) {\n      for (auto i = 0; i < relative_offset_set.size(); i++) {\n        auto output_index = start_offset + flat_segment_id(i) * big_stride;\n        auto offset = start_offset + relative_offset_set[i];\n        if (output_flat(output_index).length() != 0)\n          output_flat(output_index).append(separator_.c_str());\n        output_flat(output_index).append(flat_input(offset));\n      }\n    }\n  }", "func_hash": 25791049116451232925309470165962173994, "file_name": "unsorted_segment_join_op.cc", "file_hash": 172265001795447375510451213673230640731, "cwe": ["CWE-703"], "cve": "CVE-2021-29552", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. An attacker can cause a denial of service by controlling the values of `num_segments` tensor argument for `UnsortedSegmentJoin`. This is because the implementation(https://github.com/tensorflow/tensorflow/blob/a2a607db15c7cd01d754d37e5448d72a13491bdb/tensorflow/core/kernels/unsorted_segment_join_op.cc#L92-L93) assumes that the `num_segments` tensor is a valid scalar. Since the tensor is empty the `CHECK` involved in `.scalar<T>()()` that checks that the number of elements is exactly 1 will be invalidated and this would result in process termination. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-29552", "file_path": "tensorflow/core/kernels/unsorted_segment_join_op.cc"}
{"idx": 267917, "project": "tensorflow", "commit_id": "704866eabe03a9aeda044ec91a8d0c83fc1ebdbe", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/704866eabe03a9aeda044ec91a8d0c83fc1ebdbe", "commit_message": "Fix overflow CHECK issue with `tf.raw_ops.UnsortedSegmentJoin`.\n\nPiperOrigin-RevId: 370766155\nChange-Id: I33e7c6626224e1060a8a4ab51ad5d861c6d4c63e", "target": 0, "func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const TensorShape& input_shape = input.shape();\n    const int32 input_dims = input_shape.dims();\n\n    const Tensor& segment_id = context->input(1);\n    const TensorShape& segment_id_shape = segment_id.shape();\n    const int32 segment_dims = segment_id_shape.dims();\n\n    const Tensor& num_segments_tensor = context->input(2);\n    OP_REQUIRES(context, num_segments_tensor.NumElements() != 0,\n                errors::InvalidArgument(\"Number of segments cannot be empty.\"));\n    auto num_segments = num_segments_tensor.scalar<NUM_SEGMENTS_TYPE>()();\n\n    OP_REQUIRES(context, segment_dims != 0,\n                errors::InvalidArgument(\"Segment_id cannot have rank 0\"));\n\n    OP_REQUIRES(\n        context, segment_dims <= input_dims,\n        errors::OutOfRange(\"Invalid segment_id rank \", segment_dims,\n                           \" for input with \", input_dims, \" dimension(s)\"));\n    for (auto i = 0; i < segment_dims; i++) {\n      OP_REQUIRES(\n          context, segment_id_shape.dim_size(i) == input_shape.dim_size(i),\n          errors::InvalidArgument(\n              \"Segment dimension is \", segment_id_shape.dim_size(i),\n              \" while input dimension is \", input_dims, \" in rank \", i));\n    }\n\n    // Making output tensor.\n    Tensor* output_tensor = nullptr;\n    TensorShape output_shape =\n        GetOutputShape(input_shape, segment_id_shape, num_segments);\n    OP_REQUIRES_OK(context, context->allocate_output(\"output\", output_shape,\n                                                     &output_tensor));\n\n    // Preparating flat tensors.\n    auto output_flat = output_tensor->flat<tstring>();\n    auto flat_segment_id = segment_id.flat<INDICES_TYPE>();\n    auto flat_input = input.flat<tstring>();\n\n    for (int i = 0; i < flat_segment_id.size(); i++) {\n      OP_REQUIRES(\n          context,\n          ((flat_segment_id(i) < num_segments) && (flat_segment_id(i) >= 0)),\n          errors::InvalidArgument(\n              \"segment_ids are not allowed to exceed num_segments or\"\n              \" to have negative values.\"));\n    }\n\n    int64 big_stride;\n    int64 small_stride;\n    std::tie(big_stride, small_stride) =\n        GetStrides<INDICES_TYPE>(input_shape, segment_id_shape);\n    auto relative_offset_set =\n        GetFlattenedRelativeOffsets<INDICES_TYPE>(small_stride, big_stride);\n    for (auto start_offset = 0; start_offset < big_stride; start_offset++) {\n      for (auto i = 0; i < relative_offset_set.size(); i++) {\n        auto output_index = start_offset + flat_segment_id(i) * big_stride;\n        auto offset = start_offset + relative_offset_set[i];\n        if (output_flat(output_index).length() != 0)\n          output_flat(output_index).append(separator_.c_str());\n        output_flat(output_index).append(flat_input(offset));\n      }\n    }\n  }", "func_hash": 121381278306100727199108934923052625999, "file_name": "unsorted_segment_join_op.cc", "file_hash": 154318946168889826410916454566893664101, "cwe": ["CWE-703"], "cve": "CVE-2021-29552", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. An attacker can cause a denial of service by controlling the values of `num_segments` tensor argument for `UnsortedSegmentJoin`. This is because the implementation(https://github.com/tensorflow/tensorflow/blob/a2a607db15c7cd01d754d37e5448d72a13491bdb/tensorflow/core/kernels/unsorted_segment_join_op.cc#L92-L93) assumes that the `num_segments` tensor is a valid scalar. Since the tensor is empty the `CHECK` involved in `.scalar<T>()()` that checks that the number of elements is exactly 1 will be invalidated and this would result in process termination. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-29552", "file_path": "tensorflow/core/kernels/unsorted_segment_join_op.cc"}
{"idx": 198010, "project": "radare2", "commit_id": "193f4fe01d7f626e2ea937450f2e0c4604420e9d", "project_url": "https://github.com/radare/radare2", "commit_url": "https://github.com/radareorg/radare2/commit/193f4fe01d7f626e2ea937450f2e0c4604420e9d", "commit_message": "Fix integer overflow in string search causing oobread ##crash\n\n* Reported by @greatergoodest via huntrdev\n* BountyID: 8a3dc5cb-08b3-4807-82b2-77f08c137a04\n* Reproducer bfileovf", "target": 1, "func": "static int string_scan_range(RList *list, RBinFile *bf, int min,\n\t\t\t      const ut64 from, const ut64 to, int type, int raw, RBinSection *section) {\n\tRBin *bin = bf->rbin;\n\tut8 tmp[R_STRING_SCAN_BUFFER_SIZE];\n\tut64 str_start, needle = from;\n\tint count = 0, i, rc, runes;\n\tint str_type = R_STRING_TYPE_DETECT;\n\n\t// if list is null it means its gonna dump\n\tr_return_val_if_fail (bf, -1);\n\n\tif (type == -1) {\n\t\ttype = R_STRING_TYPE_DETECT;\n\t}\n\tif (from == to) {\n\t\treturn 0;\n\t}\n\tif (from > to) {\n\t\teprintf (\"Invalid range to find strings 0x%\"PFMT64x\" .. 0x%\"PFMT64x\"\\n\", from, to);\n\t\treturn -1;\n\t}\n\tst64 len = (st64)(to - from);\n\tif (len < 1 || len > ST32_MAX) {\n\t\teprintf (\"String scan range is invalid (%\"PFMT64d\" bytes)\\n\", len);\n\t\treturn -1;\n\t}\n\tut8 *buf = calloc (len, 1);\n\tif (!buf || !min) {\n\t\tfree (buf);\n\t\treturn -1;\n\t}\n\tst64 vdelta = 0, pdelta = 0;\n\tRBinSection *s = NULL;\n\tbool ascii_only = false;\n\tPJ *pj = NULL;\n\tif (bf->strmode == R_MODE_JSON && !list) {\n\t\tpj = pj_new ();\n\t\tif (pj) {\n\t\t\tpj_a (pj);\n\t\t}\n\t}\n\tr_buf_read_at (bf->buf, from, buf, len);\n\tchar *charset = r_sys_getenv (\"RABIN2_CHARSET\");\n\tif (!R_STR_ISEMPTY (charset)) {\n\t\tRCharset *ch = r_charset_new ();\n\t\tif (r_charset_use (ch, charset)) {\n\t\t\tint outlen = len * 4;\n\t\t\tut8 *out = calloc (len, 4);\n\t\t\tif (out) {\n\t\t\t\tint res = r_charset_encode_str (ch, out, outlen, buf, len);\n\t\t\t\tint i;\n\t\t\t\t// TODO unknown chars should be translated to null bytes\n\t\t\t\tfor (i = 0; i < res; i++) {\n\t\t\t\t\tif (out[i] == '?') {\n\t\t\t\t\t\tout[i] = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tlen = res;\n\t\t\t\tfree (buf);\n\t\t\t\tbuf = out;\n\t\t\t} else {\n\t\t\t\teprintf (\"Cannot allocate\\n\");\n\t\t\t}\n\t\t} else {\n\t\t\teprintf (\"Invalid value for RABIN2_CHARSET.\\n\");\n\t\t}\n\t\tr_charset_free (ch);\n\t}\n\tfree (charset);\n\tRConsIsBreaked is_breaked = (bin && bin->consb.is_breaked)? bin->consb.is_breaked: NULL;\n\t// may oobread\n\twhile (needle < to) {\n\t\tif (is_breaked && is_breaked ()) {\n\t\t\tbreak;\n\t\t}\n\t\t// smol optimization\n\t\tif (needle + 4 < to) {\n\t\t\tut32 n1 = r_read_le32 (buf + needle - from);\n\t\t\tif (!n1) {\n\t\t\t\tneedle += 4;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\trc = r_utf8_decode (buf + needle - from, to - needle, NULL);\n\t\tif (!rc) {\n\t\t\tneedle++;\n\t\t\tcontinue;\n\t\t}\n\t\tbool addr_aligned = !(needle % 4);\n\n\t\tif (type == R_STRING_TYPE_DETECT) {\n\t\t\tchar *w = (char *)buf + needle + rc - from;\n\t\t\tif (((to - needle) > 8 + rc)) {\n\t\t\t\t// TODO: support le and be\n\t\t\t\tbool is_wide32le = (needle + rc + 2 < to) && (!w[0] && !w[1] && !w[2] && w[3] && !w[4]);\n\t\t\t\t// reduce false positives\n\t\t\t\tif (is_wide32le) {\n\t\t\t\t\tif (!w[5] && !w[6] && w[7] && w[8]) {\n\t\t\t\t\t\tis_wide32le = false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!addr_aligned) {\n\t\t\t\t\tis_wide32le = false;\n\t\t\t\t}\n\t\t\t\t///is_wide32be &= (n1 < 0xff && n11 < 0xff); // false; // n11 < 0xff;\n\t\t\t\tif (is_wide32le  && addr_aligned) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_WIDE32; // asume big endian,is there little endian w32?\n\t\t\t\t} else {\n\t\t\t\t\t// bool is_wide = (n1 && n2 && n1 < 0xff && (!n2 || n2 < 0xff));\n\t\t\t\t\tbool is_wide = needle + rc + 4 < to && !w[0] && w[1] && !w[2] && w[3] && !w[4];\n\t\t\t\t\tstr_type = is_wide? R_STRING_TYPE_WIDE: R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8; // could be charset if set :?\n\t\t\t\t} else {\n\t\t\t\t\tstr_type = R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (type == R_STRING_TYPE_UTF8) {\n\t\t\tstr_type = R_STRING_TYPE_ASCII; // initial assumption\n\t\t} else {\n\t\t\tstr_type = type;\n\t\t}\n\t\trunes = 0;\n\t\tstr_start = needle;\n\n\t\t/* Eat a whole C string */\n\t\tfor (i = 0; i < sizeof (tmp) - 4 && needle < to; i += rc) {\n\t\t\tRRune r = {0};\n\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\trc = r_utf32le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc) {\n\t\t\t\t\trc = 4;\n\t\t\t\t}\n\t\t\t} else if (str_type == R_STRING_TYPE_WIDE) {\n\t\t\t\trc = r_utf16le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc == 1) {\n\t\t\t\t\trc = 2;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trc = r_utf8_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Invalid sequence detected */\n\t\t\tif (!rc || (ascii_only && r > 0x7f)) {\n\t\t\t\tneedle++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tneedle += rc;\n\n\t\t\tif (r_isprint (r) && r != '\\\\') {\n\t\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\t\tif (r == 0xff) {\n\t\t\t\t\t\tr = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trc = r_utf8_encode (tmp + i, r);\n\t\t\t\trunes++;\n\t\t\t\t/* Print the escape code */\n\t\t\t} else if (r && r < 0x100 && strchr (\"\\b\\v\\f\\n\\r\\t\\a\\033\\\\\", (char)r)) {\n\t\t\t\tif ((i + 32) < sizeof (tmp) && r < 93) {\n\t\t\t\t\ttmp[i + 0] = '\\\\';\n\t\t\t\t\ttmp[i + 1] = \"       abtnvfr             e  \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"  \\\\\"[r];\n\t\t\t\t} else {\n\t\t\t\t\t// string too long\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trc = 2;\n\t\t\t\trunes++;\n\t\t\t} else {\n\t\t\t\t/* \\0 marks the end of C-strings */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttmp[i++] = '\\0';\n\n\t\tif (runes < min && runes >= 2 && str_type == R_STRING_TYPE_ASCII && needle < to) {\n\t\t\t// back up past the \\0 to the last char just in case it starts a wide string\n\t\t\tneedle -= 2;\n\t\t}\n\t\tif (runes >= min) {\n\t\t\t// reduce false positives\n\t\t\tint j, num_blocks, *block_list;\n\t\t\tint *freq_list = NULL, expected_ascii, actual_ascii, num_chars;\n\t\t\tif (str_type == R_STRING_TYPE_ASCII) {\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tchar ch = tmp[j];\n\t\t\t\t\tif (ch != '\\n' && ch != '\\r' && ch != '\\t') {\n\t\t\t\t\t\tif (!IS_PRINTABLE (tmp[j])) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch (str_type) {\n\t\t\tcase R_STRING_TYPE_UTF8:\n\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\tnum_blocks = 0;\n\t\t\t\tblock_list = r_utf_block_list ((const ut8*)tmp, i - 1,\n\t\t\t\t\t\tstr_type == R_STRING_TYPE_WIDE? &freq_list: NULL);\n\t\t\t\tif (block_list) {\n\t\t\t\t\tfor (j = 0; block_list[j] != -1; j++) {\n\t\t\t\t\t\tnum_blocks++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (freq_list) {\n\t\t\t\t\tnum_chars = 0;\n\t\t\t\t\tactual_ascii = 0;\n\t\t\t\t\tfor (j = 0; freq_list[j] != -1; j++) {\n\t\t\t\t\t\tnum_chars += freq_list[j];\n\t\t\t\t\t\tif (!block_list[j]) { // ASCII\n\t\t\t\t\t\t\tactual_ascii = freq_list[j];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfree (freq_list);\n\t\t\t\t\texpected_ascii = num_blocks ? num_chars / num_blocks : 0;\n\t\t\t\t\tif (actual_ascii > expected_ascii) {\n\t\t\t\t\t\tascii_only = true;\n\t\t\t\t\t\tneedle = str_start;\n\t\t\t\t\t\tfree (block_list);\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfree (block_list);\n\t\t\t\tif (num_blocks > R_STRING_MAX_UNI_BLOCKS) {\n\t\t\t\t\tneedle++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tRBinString *bs = R_NEW0 (RBinString);\n\t\t\tif (!bs) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbs->type = str_type;\n\t\t\tbs->length = runes;\n\t\t\tbs->size = needle - str_start;\n\t\t\tbs->ordinal = count++;\n\t\t\t// TODO: move into adjust_offset\n\t\t\tswitch (str_type) {\n\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\t\tif (str_start - from > 1) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 2 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 2; // \\xff\\xfe\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\tif (str_start - from > 3) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 4 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 4; // \\xff\\xfe\\x00\\x00\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!s) {\n\t\t\t\tif (section) {\n\t\t\t\t\ts = section;\n\t\t\t\t} else if (bf->o) {\n\t\t\t\t\ts = r_bin_get_section_at (bf->o, str_start, false);\n\t\t\t\t}\n\t\t\t\tif (s) {\n\t\t\t\t\tvdelta = s->vaddr;\n\t\t\t\t\tpdelta = s->paddr;\n\t\t\t\t}\n\t\t\t}\n\t\t\tut64 baddr = bf->loadaddr && bf->o? bf->o->baddr: bf->loadaddr;\n\t\t\tbs->paddr = str_start + baddr;\n\t\t\tbs->vaddr = str_start - pdelta + vdelta + baddr;\n\t\t\tbs->string = r_str_ndup ((const char *)tmp, i);\n\t\t\tif (list) {\n\t\t\t\tr_list_append (list, bs);\n\t\t\t\tif (bf->o) {\n\t\t\t\t\tht_up_insert (bf->o->strings_db, bs->vaddr, bs);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tprint_string (bf, bs, raw, pj);\n\t\t\t\tr_bin_string_free (bs);\n\t\t\t}\n\t\t\tif (from == 0 && to == bf->size) {\n\t\t\t\t/* force lookup section at the next one */\n\t\t\t\ts = NULL;\n\t\t\t}\n\t\t}\n\t\tascii_only = false;\n\t}\n\tfree (buf);\n\tif (pj) {\n\t\tpj_end (pj);\n\t\tif (bin) {\n\t\t\tRIO *io = bin->iob.io;\n\t\t\tif (io) {\n\t\t\t\tio->cb_printf (\"%s\", pj_string (pj));\n\t\t\t}\n\t\t}\n\t\tpj_free (pj);\n\t}\n\treturn count;\n}", "func_hash": 327958810043229005935201535284879314576, "file_name": "bfile.c", "file_hash": 285505404053025817812791776020239027950, "cwe": ["CWE-125"], "cve": "CVE-2022-1899", "cve_desc": "Out-of-bounds Read in GitHub repository radareorg/radare2 prior to 5.7.0.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-1899", "file_path": "libr/bin/bfile.c"}
{"idx": 267985, "project": "radare2", "commit_id": "193f4fe01d7f626e2ea937450f2e0c4604420e9d", "project_url": "https://github.com/radare/radare2", "commit_url": "https://github.com/radareorg/radare2/commit/193f4fe01d7f626e2ea937450f2e0c4604420e9d", "commit_message": "Fix integer overflow in string search causing oobread ##crash\n\n* Reported by @greatergoodest via huntrdev\n* BountyID: 8a3dc5cb-08b3-4807-82b2-77f08c137a04\n* Reproducer bfileovf", "target": 0, "func": "static int string_scan_range(RList *list, RBinFile *bf, int min,\n\t\t\t      const ut64 from, const ut64 to, int type, int raw, RBinSection *section) {\n\tRBin *bin = bf->rbin;\n\tut8 tmp[R_STRING_SCAN_BUFFER_SIZE];\n\tut64 str_start, needle = from;\n\tint count = 0, i, rc, runes;\n\tint str_type = R_STRING_TYPE_DETECT;\n\n\t// if list is null it means its gonna dump\n\tr_return_val_if_fail (bf, -1);\n\n\tif (type == -1) {\n\t\ttype = R_STRING_TYPE_DETECT;\n\t}\n\tif (from == to) {\n\t\treturn 0;\n\t}\n\tif (from > to) {\n\t\teprintf (\"Invalid range to find strings 0x%\"PFMT64x\" .. 0x%\"PFMT64x\"\\n\", from, to);\n\t\treturn -1;\n\t}\n\tst64 len = (st64)(to - from);\n\tif (len < 1 || len > ST32_MAX) {\n\t\teprintf (\"String scan range is invalid (%\"PFMT64d\" bytes)\\n\", len);\n\t\treturn -1;\n\t}\n\tut8 *buf = calloc (len, 1);\n\tif (!buf || !min) {\n\t\tfree (buf);\n\t\treturn -1;\n\t}\n\tst64 vdelta = 0, pdelta = 0;\n\tRBinSection *s = NULL;\n\tbool ascii_only = false;\n\tPJ *pj = NULL;\n\tif (bf->strmode == R_MODE_JSON && !list) {\n\t\tpj = pj_new ();\n\t\tif (pj) {\n\t\t\tpj_a (pj);\n\t\t}\n\t}\n\tr_buf_read_at (bf->buf, from, buf, len);\n\tchar *charset = r_sys_getenv (\"RABIN2_CHARSET\");\n\tif (!R_STR_ISEMPTY (charset)) {\n\t\tRCharset *ch = r_charset_new ();\n\t\tif (r_charset_use (ch, charset)) {\n\t\t\tint outlen = len * 4;\n\t\t\tut8 *out = calloc (len, 4);\n\t\t\tif (out) {\n\t\t\t\tint res = r_charset_encode_str (ch, out, outlen, buf, len);\n\t\t\t\tint i;\n\t\t\t\t// TODO unknown chars should be translated to null bytes\n\t\t\t\tfor (i = 0; i < res; i++) {\n\t\t\t\t\tif (out[i] == '?') {\n\t\t\t\t\t\tout[i] = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tlen = res;\n\t\t\t\tfree (buf);\n\t\t\t\tbuf = out;\n\t\t\t} else {\n\t\t\t\teprintf (\"Cannot allocate\\n\");\n\t\t\t}\n\t\t} else {\n\t\t\teprintf (\"Invalid value for RABIN2_CHARSET.\\n\");\n\t\t}\n\t\tr_charset_free (ch);\n\t}\n\tfree (charset);\n\tRConsIsBreaked is_breaked = (bin && bin->consb.is_breaked)? bin->consb.is_breaked: NULL;\n\t// may oobread\n\twhile (needle < to && needle < UT64_MAX - 4) {\n\t\tif (is_breaked && is_breaked ()) {\n\t\t\tbreak;\n\t\t}\n\t\t// smol optimization\n\t\tif (needle < to - 4) {\n\t\t\tut32 n1 = r_read_le32 (buf + (needle - from));\n\t\t\tif (!n1) {\n\t\t\t\tneedle += 4;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\trc = r_utf8_decode (buf + (needle - from), to - needle, NULL);\n\t\tif (!rc) {\n\t\t\tneedle++;\n\t\t\tcontinue;\n\t\t}\n\t\tbool addr_aligned = !(needle % 4);\n\n\t\tif (type == R_STRING_TYPE_DETECT) {\n\t\t\tchar *w = (char *)buf + (needle + rc - from);\n\t\t\tif (((to - needle) > 8 + rc)) {\n\t\t\t\t// TODO: support le and be\n\t\t\t\tbool is_wide32le = (needle + rc + 2 < to) && (!w[0] && !w[1] && !w[2] && w[3] && !w[4]);\n\t\t\t\t// reduce false positives\n\t\t\t\tif (is_wide32le) {\n\t\t\t\t\tif (!w[5] && !w[6] && w[7] && w[8]) {\n\t\t\t\t\t\tis_wide32le = false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!addr_aligned) {\n\t\t\t\t\tis_wide32le = false;\n\t\t\t\t}\n\t\t\t\t///is_wide32be &= (n1 < 0xff && n11 < 0xff); // false; // n11 < 0xff;\n\t\t\t\tif (is_wide32le  && addr_aligned) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_WIDE32; // asume big endian,is there little endian w32?\n\t\t\t\t} else {\n\t\t\t\t\t// bool is_wide = (n1 && n2 && n1 < 0xff && (!n2 || n2 < 0xff));\n\t\t\t\t\tbool is_wide = needle + rc + 4 < to && !w[0] && w[1] && !w[2] && w[3] && !w[4];\n\t\t\t\t\tstr_type = is_wide? R_STRING_TYPE_WIDE: R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8; // could be charset if set :?\n\t\t\t\t} else {\n\t\t\t\t\tstr_type = R_STRING_TYPE_ASCII;\n\t\t\t\t}\n\t\t\t}\n\t\t} else if (type == R_STRING_TYPE_UTF8) {\n\t\t\tstr_type = R_STRING_TYPE_ASCII; // initial assumption\n\t\t} else {\n\t\t\tstr_type = type;\n\t\t}\n\t\trunes = 0;\n\t\tstr_start = needle;\n\n\t\t/* Eat a whole C string */\n\t\tfor (i = 0; i < sizeof (tmp) - 4 && needle < to; i += rc) {\n\t\t\tRRune r = {0};\n\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\trc = r_utf32le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc) {\n\t\t\t\t\trc = 4;\n\t\t\t\t}\n\t\t\t} else if (str_type == R_STRING_TYPE_WIDE) {\n\t\t\t\trc = r_utf16le_decode (buf + needle - from, to - needle, &r);\n\t\t\t\tif (rc == 1) {\n\t\t\t\t\trc = 2;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trc = r_utf8_decode (buf + (needle - from), to - needle, &r);\n\t\t\t\tif (rc > 1) {\n\t\t\t\t\tstr_type = R_STRING_TYPE_UTF8;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Invalid sequence detected */\n\t\t\tif (!rc || (ascii_only && r > 0x7f)) {\n\t\t\t\tneedle++;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tneedle += rc;\n\n\t\t\tif (r_isprint (r) && r != '\\\\') {\n\t\t\t\tif (str_type == R_STRING_TYPE_WIDE32) {\n\t\t\t\t\tif (r == 0xff) {\n\t\t\t\t\t\tr = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\trc = r_utf8_encode (tmp + i, r);\n\t\t\t\trunes++;\n\t\t\t\t/* Print the escape code */\n\t\t\t} else if (r && r < 0x100 && strchr (\"\\b\\v\\f\\n\\r\\t\\a\\033\\\\\", (char)r)) {\n\t\t\t\tif ((i + 32) < sizeof (tmp) && r < 93) {\n\t\t\t\t\ttmp[i + 0] = '\\\\';\n\t\t\t\t\ttmp[i + 1] = \"       abtnvfr             e  \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"                              \"\n\t\t\t\t\t             \"  \\\\\"[r];\n\t\t\t\t} else {\n\t\t\t\t\t// string too long\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trc = 2;\n\t\t\t\trunes++;\n\t\t\t} else {\n\t\t\t\t/* \\0 marks the end of C-strings */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttmp[i++] = '\\0';\n\n\t\tif (runes < min && runes >= 2 && str_type == R_STRING_TYPE_ASCII && needle < to) {\n\t\t\t// back up past the \\0 to the last char just in case it starts a wide string\n\t\t\tneedle -= 2;\n\t\t}\n\t\tif (runes >= min) {\n\t\t\t// reduce false positives\n\t\t\tint j, num_blocks, *block_list;\n\t\t\tint *freq_list = NULL, expected_ascii, actual_ascii, num_chars;\n\t\t\tif (str_type == R_STRING_TYPE_ASCII) {\n\t\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\t\tchar ch = tmp[j];\n\t\t\t\t\tif (ch != '\\n' && ch != '\\r' && ch != '\\t') {\n\t\t\t\t\t\tif (!IS_PRINTABLE (tmp[j])) {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tswitch (str_type) {\n\t\t\tcase R_STRING_TYPE_UTF8:\n\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\tnum_blocks = 0;\n\t\t\t\tblock_list = r_utf_block_list ((const ut8*)tmp, i - 1,\n\t\t\t\t\t\tstr_type == R_STRING_TYPE_WIDE? &freq_list: NULL);\n\t\t\t\tif (block_list) {\n\t\t\t\t\tfor (j = 0; block_list[j] != -1; j++) {\n\t\t\t\t\t\tnum_blocks++;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (freq_list) {\n\t\t\t\t\tnum_chars = 0;\n\t\t\t\t\tactual_ascii = 0;\n\t\t\t\t\tfor (j = 0; freq_list[j] != -1; j++) {\n\t\t\t\t\t\tnum_chars += freq_list[j];\n\t\t\t\t\t\tif (!block_list[j]) { // ASCII\n\t\t\t\t\t\t\tactual_ascii = freq_list[j];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tfree (freq_list);\n\t\t\t\t\texpected_ascii = num_blocks ? num_chars / num_blocks : 0;\n\t\t\t\t\tif (actual_ascii > expected_ascii) {\n\t\t\t\t\t\tascii_only = true;\n\t\t\t\t\t\tneedle = str_start;\n\t\t\t\t\t\tfree (block_list);\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tfree (block_list);\n\t\t\t\tif (num_blocks > R_STRING_MAX_UNI_BLOCKS) {\n\t\t\t\t\tneedle++;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tRBinString *bs = R_NEW0 (RBinString);\n\t\t\tif (!bs) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbs->type = str_type;\n\t\t\tbs->length = runes;\n\t\t\tbs->size = needle - str_start;\n\t\t\tbs->ordinal = count++;\n\t\t\t// TODO: move into adjust_offset\n\t\t\tswitch (str_type) {\n\t\t\tcase R_STRING_TYPE_WIDE:\n\t\t\t\tif (str_start - from > 1) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 2 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 2; // \\xff\\xfe\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase R_STRING_TYPE_WIDE32:\n\t\t\t\tif (str_start - from > 3) {\n\t\t\t\t\tconst ut8 *p = buf + str_start - 4 - from;\n\t\t\t\t\tif (p[0] == 0xff && p[1] == 0xfe) {\n\t\t\t\t\t\tstr_start -= 4; // \\xff\\xfe\\x00\\x00\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!s) {\n\t\t\t\tif (section) {\n\t\t\t\t\ts = section;\n\t\t\t\t} else if (bf->o) {\n\t\t\t\t\ts = r_bin_get_section_at (bf->o, str_start, false);\n\t\t\t\t}\n\t\t\t\tif (s) {\n\t\t\t\t\tvdelta = s->vaddr;\n\t\t\t\t\tpdelta = s->paddr;\n\t\t\t\t}\n\t\t\t}\n\t\t\tut64 baddr = bf->loadaddr && bf->o? bf->o->baddr: bf->loadaddr;\n\t\t\tbs->paddr = str_start + baddr;\n\t\t\tbs->vaddr = str_start - pdelta + vdelta + baddr;\n\t\t\tbs->string = r_str_ndup ((const char *)tmp, i);\n\t\t\tif (list) {\n\t\t\t\tr_list_append (list, bs);\n\t\t\t\tif (bf->o) {\n\t\t\t\t\tht_up_insert (bf->o->strings_db, bs->vaddr, bs);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tprint_string (bf, bs, raw, pj);\n\t\t\t\tr_bin_string_free (bs);\n\t\t\t}\n\t\t\tif (from == 0 && to == bf->size) {\n\t\t\t\t/* force lookup section at the next one */\n\t\t\t\ts = NULL;\n\t\t\t}\n\t\t}\n\t\tascii_only = false;\n\t}\n\tfree (buf);\n\tif (pj) {\n\t\tpj_end (pj);\n\t\tif (bin) {\n\t\t\tRIO *io = bin->iob.io;\n\t\t\tif (io) {\n\t\t\t\tio->cb_printf (\"%s\", pj_string (pj));\n\t\t\t}\n\t\t}\n\t\tpj_free (pj);\n\t}\n\treturn count;\n}", "func_hash": 198406980128053005896085428351460738080, "file_name": "bfile.c", "file_hash": 44900440862235442449646494909197314048, "cwe": ["CWE-125"], "cve": "CVE-2022-1899", "cve_desc": "Out-of-bounds Read in GitHub repository radareorg/radare2 prior to 5.7.0.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-1899", "file_path": "libr/bin/bfile.c"}
{"idx": 198013, "project": "tensorflow", "commit_id": "3150642acbbe254e3c3c5d2232143fa591855ac9", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/3150642acbbe254e3c3c5d2232143fa591855ac9", "commit_message": "Fix tf.raw_ops.LoadAndRemapMatrix vulnerability with invalid `row_remapping`.\n\nCheck that `row_remapping` has the correct dims().\n\nPiperOrigin-RevId: 445522800", "target": 1, "func": "  void Compute(OpKernelContext* context) override {\n    // Checks what we're remapping and inverts the relevant remapping Tensors to\n    // be maps with key = old ID, value = new ID.\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n\n    // Calculates the min/max old row ID that we need to read, to save us from\n    // reading some unnecessary slices of the old tensor.\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n\n    // Processes the remapping for columns.\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    // Note that we always \"remap rows\", even when the row vocabulary does\n    // not change, because partitioning requires a mapping from partitioned\n    // Variables to the full checkpoints we load.\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n\n    // Processes the checkpoint source and the provided Tensor name.\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    // This op is limited to loading Tensors of rank 2 (matrices).\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n\n    if (!remap_cols) {\n      // TODO(weiho): Consider relaxing this restriction to allow partial column\n      // loading (even when no column remapping is specified) if there turns out\n      // to be a use case for it.\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n\n    // Uses TensorSlice to potentially load the old tensor in chunks in case\n    // memory usage is a concern.\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      // TODO(weiho): Given the list of old row IDs of interest (the keys of\n      // old_row_to_new_row_map), we could also try something smarter to\n      // find some minimal set of covering ranges for the list of old row IDs\n      // such that the size of each range is less than max_rows_in_memory_.\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                // If max_rows_in_memory_ <= 0, we just load the entire chunk.\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n\n    // Allocates the output matrix.\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n\n    // Iterates through tensor slices and copies over values from the old tensor\n    // to the output matrix.\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      // Potentially re-allocates the tensor buffer since the last slice may\n      // have fewer rows than the other slices.\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n\n      // Iterates through the old loaded tensor slice row-by-row.\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n\n        // If the old row ID is not found in old_row_to_new_row_map, continue\n        // to the next row; otherwise, copy it to the output matrix.\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n\n        // Copies over the row element-by-element, in case remapping is needed\n        // along the column axis.\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              // Column remapping is specified, but this column is not found in\n              // old_col_to_new_col_map, so we leave it uninitialized, to be\n              // filled in with initializing_values later.\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n\n    // At this point, there are potentially whole rows/columns uninitialized\n    // (corresponding to the indices where row_id_present/col_id_present are\n    // false). We fill this in cell-by-cell using row_id_present and\n    // col_id_present while dequeuing from the initializing_values vector.\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n\n    // Checks that we used all the given initializing values.\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }", "func_hash": 219722258688637064068607537553301810412, "file_name": "load_and_remap_matrix_op.cc", "file_hash": 213297145030896672862854801534820332645, "cwe": ["CWE-703"], "cve": "CVE-2022-29199", "cve_desc": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.LoadAndRemapMatrix does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code assumes `initializing_values` is a vector but there is no validation for this before accessing its value. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29199", "file_path": "tensorflow/core/kernels/load_and_remap_matrix_op.cc"}
{"idx": 268104, "project": "tensorflow", "commit_id": "3150642acbbe254e3c3c5d2232143fa591855ac9", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/3150642acbbe254e3c3c5d2232143fa591855ac9", "commit_message": "Fix tf.raw_ops.LoadAndRemapMatrix vulnerability with invalid `row_remapping`.\n\nCheck that `row_remapping` has the correct dims().\n\nPiperOrigin-RevId: 445522800", "target": 0, "func": "  void Compute(OpKernelContext* context) override {\n    // Checks what we're remapping and inverts the relevant remapping Tensors to\n    // be maps with key = old ID, value = new ID.\n    std::unordered_map<int64_t, int64_t> old_row_to_new_row_map;\n    std::vector<bool> row_id_present;\n    const Tensor* row_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"row_remapping\", &row_remapping_t));\n    OP_REQUIRES(\n        context, row_remapping_t->dims() == 1,\n        errors::InvalidArgument(\"The `row_remapping` tensor must be 1-D, got \"\n                                \"a tensor of shape \",\n                                row_remapping_t->shape().DebugString()));\n    const auto row_remapping = row_remapping_t->vec<int64_t>();\n    OP_REQUIRES(context, row_remapping.size() == num_rows_,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Size of row_remapping is \", row_remapping.size(),\n                    \" instead of being equal to num_rows=\", num_rows_)));\n    OP_REQUIRES_OK(context, RemapVectorToMap(row_remapping, &row_id_present,\n                                             &old_row_to_new_row_map));\n\n    // Calculates the min/max old row ID that we need to read, to save us from\n    // reading some unnecessary slices of the old tensor.\n    int64_t min_old_row = -1;\n    int64_t max_old_row = -1;\n    for (int i = 0; i < row_remapping.size(); ++i) {\n      if (min_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) < min_old_row)) {\n        min_old_row = row_remapping(i);\n      }\n      if (max_old_row < 0 ||\n          (row_remapping(i) >= 0 && row_remapping(i) > max_old_row)) {\n        max_old_row = row_remapping(i);\n      }\n    }\n\n    // Processes the remapping for columns.\n    std::unordered_map<int64_t, int64_t> old_col_to_new_col_map;\n    std::vector<bool> col_id_present;\n    const Tensor* col_remapping_t;\n    OP_REQUIRES_OK(context, context->input(\"col_remapping\", &col_remapping_t));\n    const auto col_remapping = col_remapping_t->vec<int64_t>();\n    // Note that we always \"remap rows\", even when the row vocabulary does\n    // not change, because partitioning requires a mapping from partitioned\n    // Variables to the full checkpoints we load.\n    const bool remap_cols = col_remapping.size() > 0;\n    if (remap_cols) {\n      OP_REQUIRES(\n          context, col_remapping.size() == num_cols_,\n          errors::InvalidArgument(strings::StrCat(\n              \"Provided col_remapping, but its size is \", col_remapping.size(),\n              \" instead of being equal to num_cols=\", num_cols_)));\n      OP_REQUIRES_OK(context, RemapVectorToMap(col_remapping, &col_id_present,\n                                               &old_col_to_new_col_map));\n    } else {\n      col_id_present.clear();\n      col_id_present.resize(num_cols_, true);\n    }\n\n    // Processes the checkpoint source and the provided Tensor name.\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();\n\n    LOG(INFO) << \"Processing checkpoint : \" << ckpt_path;\n    BundleReader reader(context->env(), ckpt_path);\n    OP_REQUIRES_OK(context, reader.status());\n\n    DataType tensor_type;\n    TensorShape tensor_shape;\n    OP_REQUIRES_OK(context, reader.LookupDtypeAndShape(\n                                old_tensor_name, &tensor_type, &tensor_shape));\n    OP_REQUIRES(context, tensor_type == DT_FLOAT,\n                errors::InvalidArgument(strings::StrCat(\n                    \"Tensor \", old_tensor_name, \" has invalid type \",\n                    DataTypeString(tensor_type), \" instead of expected type \",\n                    DataTypeString(DT_FLOAT))));\n    // This op is limited to loading Tensors of rank 2 (matrices).\n    OP_REQUIRES(\n        context, tensor_shape.dims() == 2,\n        errors::InvalidArgument(strings::StrCat(\n            \"Tensor \", old_tensor_name, \" has shape \",\n            tensor_shape.DebugString(), \" of invalid rank \",\n            tensor_shape.dims(), \" instead of expected shape of rank 2.\")));\n\n    if (!remap_cols) {\n      // TODO(weiho): Consider relaxing this restriction to allow partial column\n      // loading (even when no column remapping is specified) if there turns out\n      // to be a use case for it.\n      OP_REQUIRES(context, num_cols_ == tensor_shape.dim_size(1),\n                  errors::InvalidArgument(strings::StrCat(\n                      \"Tensor \", old_tensor_name, \" has shape \",\n                      tensor_shape.DebugString(),\n                      \", where the size of its 2nd dimension is \",\n                      tensor_shape.dim_size(1),\n                      \" instead of being equal to num_cols=\", num_cols_)));\n    }\n\n    // Uses TensorSlice to potentially load the old tensor in chunks in case\n    // memory usage is a concern.\n    std::vector<TensorSlice> tensor_slices;\n    TensorSlice slice(tensor_shape.dims());\n    if (min_old_row >= 0 && max_old_row >= 0) {\n      int64_t row_start = min_old_row;\n      // TODO(weiho): Given the list of old row IDs of interest (the keys of\n      // old_row_to_new_row_map), we could also try something smarter to\n      // find some minimal set of covering ranges for the list of old row IDs\n      // such that the size of each range is less than max_rows_in_memory_.\n      while (row_start <= max_old_row) {\n        const int64_t slice_length =\n            max_rows_in_memory_ <= 0\n                // If max_rows_in_memory_ <= 0, we just load the entire chunk.\n                ? max_old_row - row_start + 1\n                : std::min(max_rows_in_memory_, max_old_row - row_start + 1);\n        slice.set_start(0, row_start);\n        slice.set_length(0, slice_length);\n        tensor_slices.push_back(slice);\n        row_start += slice_length;\n      }\n    }\n\n    // Allocates the output matrix.\n    Tensor* output_matrix_t = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\"output_matrix\",\n                                            TensorShape({num_rows_, num_cols_}),\n                                            &output_matrix_t));\n    auto output_matrix = output_matrix_t->matrix<float>();\n\n    // Iterates through tensor slices and copies over values from the old tensor\n    // to the output matrix.\n    int64_t row_index = min_old_row;\n    int64_t rows_copied = 0;\n    Tensor loaded_tensor_t;\n    for (const TensorSlice& tensor_slice : tensor_slices) {\n      LOG(INFO) << \"Loading slice \" << tensor_slice.DebugString();\n      TensorShape slice_shape;\n      OP_REQUIRES_OK(context,\n                     tensor_slice.SliceTensorShape(tensor_shape, &slice_shape));\n      // Potentially re-allocates the tensor buffer since the last slice may\n      // have fewer rows than the other slices.\n      if (loaded_tensor_t.shape() != slice_shape) {\n        loaded_tensor_t = Tensor(DT_FLOAT, slice_shape);\n      }\n      OP_REQUIRES_OK(context, reader.LookupSlice(old_tensor_name, tensor_slice,\n                                                 &loaded_tensor_t));\n\n      // Iterates through the old loaded tensor slice row-by-row.\n      for (int row = 0; row < loaded_tensor_t.dim_size(0); ++row, ++row_index) {\n        if (row_index % 500000 == min_old_row) {\n          LOG(INFO) << \"Processing old row \" << row_index;\n        }\n\n        // If the old row ID is not found in old_row_to_new_row_map, continue\n        // to the next row; otherwise, copy it to the output matrix.\n        const int64_t* new_row_ptr =\n            gtl::FindOrNull(old_row_to_new_row_map, row_index);\n        if (new_row_ptr == nullptr) {\n          continue;\n        }\n        ++rows_copied;\n        const int64_t new_row = *new_row_ptr;\n\n        // Copies over the row element-by-element, in case remapping is needed\n        // along the column axis.\n        const auto& loaded_tensor = loaded_tensor_t.matrix<float>();\n        for (int old_col = 0; old_col < loaded_tensor_t.dim_size(1);\n             ++old_col) {\n          int64_t new_col = old_col;\n          if (remap_cols) {\n            const int64_t* new_col_ptr =\n                gtl::FindOrNull(old_col_to_new_col_map, old_col);\n            if (new_col_ptr == nullptr) {\n              // Column remapping is specified, but this column is not found in\n              // old_col_to_new_col_map, so we leave it uninitialized, to be\n              // filled in with initializing_values later.\n              continue;\n            }\n            new_col = *new_col_ptr;\n          }\n\n          OP_REQUIRES(context,\n                      new_row < num_rows_ && new_col < num_cols_ &&\n                          new_row >= 0 && new_col >= 0,\n                      errors::Internal(strings::StrCat(\n                          \"new_row=\", new_row, \" and new_col=\", new_col,\n                          \" should have been less than num_rows_=\", num_rows_,\n                          \" and num_cols_=\", num_cols_,\n                          \" and non-negative. This should never have happened \"\n                          \"if the code were correct. Please file a bug.\")));\n          output_matrix(new_row, new_col) = loaded_tensor(row, old_col);\n        }\n      }\n    }\n    LOG(INFO) << \"Copied \" << rows_copied << \" rows from old matrix (with \"\n              << tensor_shape.dim_size(0) << \" rows) to new matrix (with \"\n              << num_rows_ << \" rows).\";\n\n    // At this point, there are potentially whole rows/columns uninitialized\n    // (corresponding to the indices where row_id_present/col_id_present are\n    // false). We fill this in cell-by-cell using row_id_present and\n    // col_id_present while dequeuing from the initializing_values vector.\n    const Tensor* initializing_values_t;\n    OP_REQUIRES_OK(\n        context, context->input(\"initializing_values\", &initializing_values_t));\n    const auto initializing_values = initializing_values_t->flat<float>();\n    int64_t initializing_values_index = 0;\n    for (int i = 0; i < num_rows_; ++i) {\n      for (int j = 0; j < num_cols_; ++j) {\n        if (row_id_present[i] && col_id_present[j]) continue;\n        OP_REQUIRES(\n            context, initializing_values_index < initializing_values.size(),\n            errors::InvalidArgument(\n                \"initializing_values contained \", initializing_values.size(),\n                \" elements, but more missing values remain.\"));\n        output_matrix(i, j) = initializing_values(initializing_values_index);\n        ++initializing_values_index;\n      }\n    }\n\n    // Checks that we used all the given initializing values.\n    OP_REQUIRES(\n        context, initializing_values_index == initializing_values.size(),\n        errors::InvalidArgument(\n            \"initializing_values contained \", initializing_values.size(),\n            \" elements, but only \", initializing_values_index,\n            \" elements were used to fill in missing values.\"));\n  }", "func_hash": 327012646245116178249898912195700700283, "file_name": "load_and_remap_matrix_op.cc", "file_hash": 176825367634149624360733361050584878598, "cwe": ["CWE-703"], "cve": "CVE-2022-29199", "cve_desc": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.LoadAndRemapMatrix does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. The code assumes `initializing_values` is a vector but there is no validation for this before accessing its value. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29199", "file_path": "tensorflow/core/kernels/load_and_remap_matrix_op.cc"}
{"idx": 198116, "project": "tensorflow", "commit_id": "87158f43f05f2720a374f3e6d22a7aaa3a33f750", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/87158f43f05f2720a374f3e6d22a7aaa3a33f750", "commit_message": "Prevent heap OOB in sparse reduction ops.\n\nPiperOrigin-RevId: 387934524\nChange-Id: I894aa30f1e454f09b471d565b4a325da49322c1a", "target": 1, "func": "  void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n\n    // TODO(zongheng): we will call Reorder() below, which will modify\n    // in-place the underlying indices and values buffers.  To avoid\n    // surprises of this kernel being stateful, we work around the above by\n    // making deep copies here.  Remove this if/when we change Reorder()'s\n    // semantics.\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n\n    // Compute strides, and use it to convert coords to flat index.  The\n    // coordinates returned by .group() have the same ndims as group_by_dims.\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  // Do this iff we don't reduce all.\n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  // Reduce all.\n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n\n    // Each group maps one-on-one onto a value in the reduced tensor.\n    // g.group() provides the coordinates of a particular reduced value.\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }", "func_hash": 226769425429975920381040402527052798393, "file_name": "sparse_reduce_op.cc", "file_hash": 21049285234129890276227338943103143863, "cwe": ["CWE-125"], "cve": "CVE-2021-37635", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of sparse reduction operations in TensorFlow can trigger accesses outside of bounds of heap allocated data. The [implementation](https://github.com/tensorflow/tensorflow/blob/a1bc56203f21a5a4995311825ffaba7a670d7747/tensorflow/core/kernels/sparse_reduce_op.cc#L217-L228) fails to validate that each reduction group does not overflow and that each corresponding index does not point to outside the bounds of the input tensor. We have patched the issue in GitHub commit 87158f43f05f2720a374f3e6d22a7aaa3a33f750. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37635", "file_path": "tensorflow/core/kernels/sparse_reduce_op.cc"}
{"idx": 269323, "project": "tensorflow", "commit_id": "87158f43f05f2720a374f3e6d22a7aaa3a33f750", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/87158f43f05f2720a374f3e6d22a7aaa3a33f750", "commit_message": "Prevent heap OOB in sparse reduction ops.\n\nPiperOrigin-RevId: 387934524\nChange-Id: I894aa30f1e454f09b471d565b4a325da49322c1a", "target": 0, "func": "  void Compute(OpKernelContext *ctx) override {\n    const Tensor *indices_t, *values_t, *shape_t, *reduction_axes_t;\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_indices\", &indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_values\", &values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"input_shape\", &shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"reduction_axes\", &reduction_axes_t));\n\n    OP_REQUIRES_OK(ctx, ValidateInputs(shape_t, reduction_axes_t));\n\n    // TODO(zongheng): we will call Reorder() below, which will modify\n    // in-place the underlying indices and values buffers.  To avoid\n    // surprises of this kernel being stateful, we work around the above by\n    // making deep copies here.  Remove this if/when we change Reorder()'s\n    // semantics.\n    const auto shape_vec = shape_t->vec<int64>();\n    SparseTensor sp;\n    OP_REQUIRES_OK(ctx, SparseTensor::Create(\n        tensor::DeepCopy(*indices_t), tensor::DeepCopy(*values_t),\n                    TensorShape(shape_vec), &sp));\n    ReduceDetails reduction = SparseTensorReduceHelper(\n        sp, reduction_axes_t->flat<int32>(), keep_dims_);\n\n    Tensor *out_values;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, reduction.reduced_shape, &out_values));\n    auto out_flat = out_values->flat<T>();\n    out_flat.setZero();\n\n    Tensor tmp_reduced_val;\n    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                           TensorShape({}), &tmp_reduced_val));\n    auto reduced_val = tmp_reduced_val.scalar<T>();\n\n    // Compute strides, and use it to convert coords to flat index.  The\n    // coordinates returned by .group() have the same ndims as group_by_dims.\n    gtl::InlinedVector<int64, 8> output_strides(reduction.group_by_dims.size());\n    if (!output_strides.empty()) {  // Do this iff we don't reduce all.\n      output_strides.back() = 1;\n      for (int d = output_strides.size() - 2; d >= 0; --d) {\n        output_strides[d] =\n            output_strides[d + 1] * shape_vec(reduction.group_by_dims[d + 1]);\n      }\n    }\n\n    auto CoordinatesToFlatIndex = [](ArraySlice<int64> coords,\n                                     ArraySlice<int64> strides) -> int64 {\n      if (strides.empty()) {  // Reduce all.\n        return 0;\n      }\n      CHECK_EQ(coords.size(), strides.size());\n      int64_t idx = 0;\n      for (int i = 0; i < coords.size(); ++i) {\n        idx += coords[i] * strides[i];\n      }\n      return idx;\n    };\n\n    // Each group maps one-on-one onto a value in the reduced tensor.\n    // g.group() provides the coordinates of a particular reduced value.\n    sp.Reorder<T>(reduction.reorder_dims);\n    for (const auto &g : sp.group(reduction.group_by_dims)) {\n      Op::template Run<T>(ctx, reduced_val, g.template values<T>());\n      OP_REQUIRES(ctx,\n                  output_strides.empty() ||\n                  (g.group().size() == output_strides.size()),\n                  errors::Internal(\n                      \"Expected group size and output_strides size to match\",\n                      \", but got \", g.group().size(), \" and \",\n                      output_strides.size()));\n      const int64_t idx = CoordinatesToFlatIndex(g.group(), output_strides);\n      OP_REQUIRES(ctx,\n                  idx >= 0 && idx < out_flat.size(),\n                  errors::Internal(\n                      \"Obtained a write index of \", idx,\n                      \" which is outside of bounds of [0, \",\n                      out_flat.size(), \")\"));\n      out_flat(idx) = reduced_val();\n      VLOG(2) << \"coords: \" << absl::StrJoin(g.group(), \",\")\n              << \"; idx: \" << idx << \"; group \" << Op::Name() << \": \"\n              << reduced_val();\n    }\n  }", "func_hash": 101373936648548971871118830985437949443, "file_name": "sparse_reduce_op.cc", "file_hash": 180034479721260906291386673174160626569, "cwe": ["CWE-125"], "cve": "CVE-2021-37635", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of sparse reduction operations in TensorFlow can trigger accesses outside of bounds of heap allocated data. The [implementation](https://github.com/tensorflow/tensorflow/blob/a1bc56203f21a5a4995311825ffaba7a670d7747/tensorflow/core/kernels/sparse_reduce_op.cc#L217-L228) fails to validate that each reduction group does not overflow and that each corresponding index does not point to outside the bounds of the input tensor. We have patched the issue in GitHub commit 87158f43f05f2720a374f3e6d22a7aaa3a33f750. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37635", "file_path": "tensorflow/core/kernels/sparse_reduce_op.cc"}
