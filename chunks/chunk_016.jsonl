{"idx": 195056, "project": "tensorflow", "commit_id": "8c6f391a2282684a25cbfec7687bd5d35261a209", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209", "commit_message": "[lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check\n\nPiperOrigin-RevId: 416383645\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb", "target": 1, "func": "inline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,\n                         const float* bias_data, int array_size,\n                         float* array_data) {\n  // Note: see b/132215220: in May 2019 we thought it would be OK to replace\n  // this with the Eigen one-liner:\n  //   return (array.colwise() + bias).cwiseMin(clamp_max).cwiseMin(clamp_max).\n  // This turned out to severely regress performance: +4ms (i.e. 8%) on\n  // MobileNet v2 / 1.0 / 224. So we keep custom NEON code for now.\n  TFLITE_DCHECK_EQ((array_size % bias_size), 0);\n#ifdef USE_NEON\n  float* array_ptr = array_data;\n  float* array_end_ptr = array_ptr + array_size;\n  const auto clamp_min_vec = vdupq_n_f32(clamp_min);\n  const auto clamp_max_vec = vdupq_n_f32(clamp_max);\n  for (; array_ptr != array_end_ptr; array_ptr += bias_size) {\n    int i = 0;\n    for (; i <= bias_size - 16; i += 16) {\n      auto b0 = vld1q_f32(bias_data + i);\n      auto b1 = vld1q_f32(bias_data + i + 4);\n      auto b2 = vld1q_f32(bias_data + i + 8);\n      auto b3 = vld1q_f32(bias_data + i + 12);\n      auto a0 = vld1q_f32(array_ptr + i);\n      auto a1 = vld1q_f32(array_ptr + i + 4);\n      auto a2 = vld1q_f32(array_ptr + i + 8);\n      auto a3 = vld1q_f32(array_ptr + i + 12);\n      auto x0 = vaddq_f32(a0, b0);\n      auto x1 = vaddq_f32(a1, b1);\n      auto x2 = vaddq_f32(a2, b2);\n      auto x3 = vaddq_f32(a3, b3);\n      x0 = vmaxq_f32(clamp_min_vec, x0);\n      x1 = vmaxq_f32(clamp_min_vec, x1);\n      x2 = vmaxq_f32(clamp_min_vec, x2);\n      x3 = vmaxq_f32(clamp_min_vec, x3);\n      x0 = vminq_f32(clamp_max_vec, x0);\n      x1 = vminq_f32(clamp_max_vec, x1);\n      x2 = vminq_f32(clamp_max_vec, x2);\n      x3 = vminq_f32(clamp_max_vec, x3);\n      vst1q_f32(array_ptr + i, x0);\n      vst1q_f32(array_ptr + i + 4, x1);\n      vst1q_f32(array_ptr + i + 8, x2);\n      vst1q_f32(array_ptr + i + 12, x3);\n    }\n    for (; i <= bias_size - 4; i += 4) {\n      auto b = vld1q_f32(bias_data + i);\n      auto a = vld1q_f32(array_ptr + i);\n      auto x = vaddq_f32(a, b);\n      x = vmaxq_f32(clamp_min_vec, x);\n      x = vminq_f32(clamp_max_vec, x);\n      vst1q_f32(array_ptr + i, x);\n    }\n    for (; i < bias_size; i++) {\n      array_ptr[i] = ActivationFunctionWithMinMax(array_ptr[i] + bias_data[i],\n                                                  clamp_min, clamp_max);\n    }\n  }\n#else  // not NEON\n  for (int array_offset = 0; array_offset < array_size;\n       array_offset += bias_size) {\n    for (int i = 0; i < bias_size; i++) {\n      array_data[array_offset + i] = ActivationFunctionWithMinMax(\n          array_data[array_offset + i] + bias_data[i], clamp_min, clamp_max);\n    }\n  }\n#endif\n}", "func_hash": 154263320578941255259441922880599149557, "file_name": "common.h", "file_hash": 11373796702176609664888229687660280569, "cwe": ["CWE-369"], "cve": "CVE-2022-23557", "cve_desc": "Tensorflow is an Open Source Machine Learning Framework. An attacker can craft a TFLite model that would trigger a division by zero in `BiasAndClamp` implementation. There is no check that the `bias_size` is non zero. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-23557", "file_path": "tensorflow/core/data/service/common.h"}
{"idx": 220841, "project": "tensorflow", "commit_id": "8c6f391a2282684a25cbfec7687bd5d35261a209", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209", "commit_message": "[lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check\n\nPiperOrigin-RevId: 416383645\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb", "target": 0, "func": "inline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,\n                         const float* bias_data, int array_size,\n                         float* array_data) {\n  if (bias_size == 0) return;\n  // Note: see b/132215220: in May 2019 we thought it would be OK to replace\n  // this with the Eigen one-liner:\n  //   return (array.colwise() + bias).cwiseMin(clamp_max).cwiseMin(clamp_max).\n  // This turned out to severely regress performance: +4ms (i.e. 8%) on\n  // MobileNet v2 / 1.0 / 224. So we keep custom NEON code for now.\n  TFLITE_DCHECK_EQ((array_size % bias_size), 0);\n#ifdef USE_NEON\n  float* array_ptr = array_data;\n  float* array_end_ptr = array_ptr + array_size;\n  const auto clamp_min_vec = vdupq_n_f32(clamp_min);\n  const auto clamp_max_vec = vdupq_n_f32(clamp_max);\n  for (; array_ptr != array_end_ptr; array_ptr += bias_size) {\n    int i = 0;\n    for (; i <= bias_size - 16; i += 16) {\n      auto b0 = vld1q_f32(bias_data + i);\n      auto b1 = vld1q_f32(bias_data + i + 4);\n      auto b2 = vld1q_f32(bias_data + i + 8);\n      auto b3 = vld1q_f32(bias_data + i + 12);\n      auto a0 = vld1q_f32(array_ptr + i);\n      auto a1 = vld1q_f32(array_ptr + i + 4);\n      auto a2 = vld1q_f32(array_ptr + i + 8);\n      auto a3 = vld1q_f32(array_ptr + i + 12);\n      auto x0 = vaddq_f32(a0, b0);\n      auto x1 = vaddq_f32(a1, b1);\n      auto x2 = vaddq_f32(a2, b2);\n      auto x3 = vaddq_f32(a3, b3);\n      x0 = vmaxq_f32(clamp_min_vec, x0);\n      x1 = vmaxq_f32(clamp_min_vec, x1);\n      x2 = vmaxq_f32(clamp_min_vec, x2);\n      x3 = vmaxq_f32(clamp_min_vec, x3);\n      x0 = vminq_f32(clamp_max_vec, x0);\n      x1 = vminq_f32(clamp_max_vec, x1);\n      x2 = vminq_f32(clamp_max_vec, x2);\n      x3 = vminq_f32(clamp_max_vec, x3);\n      vst1q_f32(array_ptr + i, x0);\n      vst1q_f32(array_ptr + i + 4, x1);\n      vst1q_f32(array_ptr + i + 8, x2);\n      vst1q_f32(array_ptr + i + 12, x3);\n    }\n    for (; i <= bias_size - 4; i += 4) {\n      auto b = vld1q_f32(bias_data + i);\n      auto a = vld1q_f32(array_ptr + i);\n      auto x = vaddq_f32(a, b);\n      x = vmaxq_f32(clamp_min_vec, x);\n      x = vminq_f32(clamp_max_vec, x);\n      vst1q_f32(array_ptr + i, x);\n    }\n    for (; i < bias_size; i++) {\n      array_ptr[i] = ActivationFunctionWithMinMax(array_ptr[i] + bias_data[i],\n                                                  clamp_min, clamp_max);\n    }\n  }\n#else  // not NEON\n  for (int array_offset = 0; array_offset < array_size;\n       array_offset += bias_size) {\n    for (int i = 0; i < bias_size; i++) {\n      array_data[array_offset + i] = ActivationFunctionWithMinMax(\n          array_data[array_offset + i] + bias_data[i], clamp_min, clamp_max);\n    }\n  }\n#endif\n}", "func_hash": 163406073569204971648641083480315438791, "file_name": "common.h", "file_hash": 206010119069068373550820723284960883967, "cwe": ["CWE-369"], "cve": "CVE-2022-23557", "cve_desc": "Tensorflow is an Open Source Machine Learning Framework. An attacker can craft a TFLite model that would trigger a division by zero in `BiasAndClamp` implementation. There is no check that the `bias_size` is non zero. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-23557", "file_path": "tensorflow/core/data/service/common.h"}
