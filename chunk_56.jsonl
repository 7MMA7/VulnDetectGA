{"idx": 197824, "project": "gpac", "commit_id": "c535bad50d5812d27ee5b22b54371bddec411514", "project_url": "https://github.com/gpac/gpac", "commit_url": "https://github.com/gpac/gpac/commit/c535bad50d5812d27ee5b22b54371bddec411514", "commit_message": "fixed #2194", "target": 1, "func": "static GF_Err BM_ParseGlobalQuantizer(GF_BifsDecoder *codec, GF_BitStream *bs, GF_List *com_list)\n{\n\tGF_Node *node;\n\tGF_Command *com;\n\tGF_CommandField *inf;\n\tnode = gf_bifs_dec_node(codec, bs, NDT_SFWorldNode);\n\tif (!node) return GF_NON_COMPLIANT_BITSTREAM;\n\n\t/*reset global QP*/\n\tif (codec->scenegraph->global_qp) {\n\t\tgf_node_unregister(codec->scenegraph->global_qp, NULL);\n\t}\n\tcodec->ActiveQP = NULL;\n\tcodec->scenegraph->global_qp = NULL;\n\n\tif (gf_node_get_tag(node) != TAG_MPEG4_QuantizationParameter) {\n\t\tgf_node_unregister(node, NULL);\n\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t}\n\n\t/*register global QP*/\n\tcodec->ActiveQP = (M_QuantizationParameter *) node;\n\tcodec->ActiveQP->isLocal = 0;\n\tcodec->scenegraph->global_qp = node;\n\n\t/*register TWICE: once for the command, and for the scenegraph globalQP*/\n\tnode->sgprivate->num_instances = 2;\n\n\tcom = gf_sg_command_new(codec->current_graph, GF_SG_GLOBAL_QUANTIZER);\n\tinf = gf_sg_command_field_new(com);\n\tinf->new_node = node;\n\tinf->field_ptr = &inf->new_node;\n\tinf->fieldType = GF_SG_VRML_SFNODE;\n\tgf_list_add(com_list, com);\n\treturn GF_OK;\n}", "func_hash": 227330749119032349040137878791232526501, "file_name": "memory_decoder.c", "file_hash": 144617483436873036933597599292760665537, "cwe": ["CWE-416"], "cve": "CVE-2022-1795", "cve_desc": "Use After Free in GitHub repository gpac/gpac prior to v2.1.0-DEV.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-1795", "file_path": "src/bifs/memory_decoder.c"}
{"idx": 264657, "project": "gpac", "commit_id": "c535bad50d5812d27ee5b22b54371bddec411514", "project_url": "https://github.com/gpac/gpac", "commit_url": "https://github.com/gpac/gpac/commit/c535bad50d5812d27ee5b22b54371bddec411514", "commit_message": "fixed #2194", "target": 0, "func": "static GF_Err BM_ParseGlobalQuantizer(GF_BifsDecoder *codec, GF_BitStream *bs, GF_List *com_list)\n{\n\tGF_Node *node;\n\tGF_Command *com;\n\tGF_CommandField *inf;\n\tnode = gf_bifs_dec_node(codec, bs, NDT_SFWorldNode);\n\tif (!node) return GF_NON_COMPLIANT_BITSTREAM;\n\n\t/*reset global QP*/\n\tif (codec->scenegraph->global_qp) {\n\t\tgf_node_unregister(codec->scenegraph->global_qp, NULL);\n\t}\n\tcodec->ActiveQP = NULL;\n\tcodec->scenegraph->global_qp = NULL;\n\n\tif (gf_node_get_tag(node) != TAG_MPEG4_QuantizationParameter) {\n\t\t//if node was just created (num_instances == 0), unregister\n\t\t//otherwise (USE node) don't do anything\n\t\tif (!node->sgprivate->num_instances) {\n\t\t\tnode->sgprivate->num_instances = 1;\n\t\t\tgf_node_unregister(node, NULL);\n\t\t}\n\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t}\n\n\t/*register global QP*/\n\tcodec->ActiveQP = (M_QuantizationParameter *) node;\n\tcodec->ActiveQP->isLocal = 0;\n\tcodec->scenegraph->global_qp = node;\n\n\t/*register TWICE: once for the command, and for the scenegraph globalQP*/\n\tgf_node_unregister(node, NULL);\n\tgf_node_unregister(node, NULL);\n\n\tcom = gf_sg_command_new(codec->current_graph, GF_SG_GLOBAL_QUANTIZER);\n\tinf = gf_sg_command_field_new(com);\n\tinf->new_node = node;\n\tinf->field_ptr = &inf->new_node;\n\tinf->fieldType = GF_SG_VRML_SFNODE;\n\tgf_list_add(com_list, com);\n\treturn GF_OK;\n}", "func_hash": 121103708905571280232350867601982124843, "file_name": "memory_decoder.c", "file_hash": 38461926007506254402005076671567829745, "cwe": ["CWE-416"], "cve": "CVE-2022-1795", "cve_desc": "Use After Free in GitHub repository gpac/gpac prior to v2.1.0-DEV.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-1795", "file_path": "src/bifs/memory_decoder.c"}
{"idx": 197826, "project": "tensorflow", "commit_id": "7731e8dfbe4a56773be5dc94d631611211156659", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659", "commit_message": "Don't constant-fold DT_RESOURCE constants.\n\nPiperOrigin-RevId: 391803952\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "target": 1, "func": "bool IsConstantFoldable(\n    const Node* n,\n    const std::unordered_map<string, std::vector<PartialTensorShape>>*\n        shape_map,\n    const std::function<bool(const Node*)>& consider,\n    int64_t max_constant_size_in_bytes,\n    std::unordered_map<const Node*, std::vector<Tensor>>*\n        shape_replacement_map) {\n  if (n->IsConstant()) {\n    return true;\n  }\n  if (MaybeReplaceShapeOp(n, shape_map, shape_replacement_map)) {\n    return true;\n  }\n  if (n->op_def().is_stateful()) {\n    return false;\n  }\n  if (consider && !consider(n)) {\n    return false;\n  }\n  if (shape_map != nullptr) {\n    // We can skip the node if an output is known to be oversized.\n    auto shape_it = shape_map->find(n->name());\n    if (shape_it != shape_map->end()) {\n      for (int64_t i = 0; i < shape_it->second.size(); ++i) {\n        const auto& out_shape = shape_it->second[i];\n        if (out_shape.IsFullyDefined() &&\n            out_shape.num_elements() * DataTypeSize(n->output_type(i)) >\n                max_constant_size_in_bytes) {\n          return false;\n        }\n      }\n    }\n  }\n  if (n->IsControlFlow() || n->IsSend() || n->IsRecv()) {\n    return false;\n  }\n  // TODO(yuanbyu): For now disable these session handle operations.\n  if (n->IsGetSessionHandle() || n->IsGetSessionTensor() ||\n      n->IsDeleteSessionTensor()) {\n    return false;\n  }\n  if (n->IsSource()) {\n    return false;\n  }\n  if (n->IsSink()) {\n    return false;\n  }\n  if (n->IsFakeParam()) {\n    return false;\n  }\n  // Since constant-folding runs on the CPU, do not attempt to constant-fold\n  // operators that have no CPU kernel. Also implies that we will not\n  // constant-fold functions.\n  // TODO(phawkins): allow constant-folding for functions; functions may\n  // be arbitrarily expensive to execute.\n  if (!KernelDefAvailable(DeviceType(DEVICE_CPU), n->def())) {\n    return false;\n  }\n  // Do not constant fold nodes which will be allocated by ScopedAllocator.\n  // This is because the constant-folding graph will not contain the\n  // `_ScopedAllocator` node, and that is necessary to be able to run a node\n  // that will use this allocator.\n  if (n->attrs().Find(kScopedAllocatorAttrName) != nullptr) {\n    VLOG(2) << \"Skip node [\" << n->DebugString()\n            << \"] for constant folding due to scoped allocator\";\n    return false;\n  }\n  return true;\n}", "func_hash": 116289485656616917830363077368123441202, "file_name": "constant_folding.cc", "file_hash": 46768745532828534791253050765124097339, "cwe": ["CWE-824"], "cve": "CVE-2021-41204", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions during TensorFlow's Grappler optimizer phase, constant folding might attempt to deep copy a resource tensor. This results in a segfault, as these tensors are supposed to not change. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41204", "file_path": "tensorflow/core/common_runtime/constant_folding.cc"}
{"idx": 264715, "project": "tensorflow", "commit_id": "7731e8dfbe4a56773be5dc94d631611211156659", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/7731e8dfbe4a56773be5dc94d631611211156659", "commit_message": "Don't constant-fold DT_RESOURCE constants.\n\nPiperOrigin-RevId: 391803952\nChange-Id: I0ea3ec31d3e7dfda0f03b4027a237f08d00a3091", "target": 0, "func": "bool IsConstantFoldable(\n    const Node* n,\n    const std::unordered_map<string, std::vector<PartialTensorShape>>*\n        shape_map,\n    const std::function<bool(const Node*)>& consider,\n    int64_t max_constant_size_in_bytes,\n    std::unordered_map<const Node*, std::vector<Tensor>>*\n        shape_replacement_map) {\n  if (n->IsConstant()) {\n    // Skip constant folding resources as they cannot be deep copied.\n    return n->output_type(0) != DT_RESOURCE;\n  }\n  if (MaybeReplaceShapeOp(n, shape_map, shape_replacement_map)) {\n    return true;\n  }\n  if (n->op_def().is_stateful()) {\n    return false;\n  }\n  if (consider && !consider(n)) {\n    return false;\n  }\n  if (shape_map != nullptr) {\n    // We can skip the node if an output is known to be oversized.\n    auto shape_it = shape_map->find(n->name());\n    if (shape_it != shape_map->end()) {\n      for (int64_t i = 0; i < shape_it->second.size(); ++i) {\n        const auto& out_shape = shape_it->second[i];\n        if (out_shape.IsFullyDefined() &&\n            out_shape.num_elements() * DataTypeSize(n->output_type(i)) >\n                max_constant_size_in_bytes) {\n          return false;\n        }\n      }\n    }\n  }\n  if (n->IsControlFlow() || n->IsSend() || n->IsRecv()) {\n    return false;\n  }\n  // TODO(yuanbyu): For now disable these session handle operations.\n  if (n->IsGetSessionHandle() || n->IsGetSessionTensor() ||\n      n->IsDeleteSessionTensor()) {\n    return false;\n  }\n  if (n->IsSource()) {\n    return false;\n  }\n  if (n->IsSink()) {\n    return false;\n  }\n  if (n->IsFakeParam()) {\n    return false;\n  }\n  // Since constant-folding runs on the CPU, do not attempt to constant-fold\n  // operators that have no CPU kernel. Also implies that we will not\n  // constant-fold functions.\n  // TODO(phawkins): allow constant-folding for functions; functions may\n  // be arbitrarily expensive to execute.\n  if (!KernelDefAvailable(DeviceType(DEVICE_CPU), n->def())) {\n    return false;\n  }\n  // Do not constant fold nodes which will be allocated by ScopedAllocator.\n  // This is because the constant-folding graph will not contain the\n  // `_ScopedAllocator` node, and that is necessary to be able to run a node\n  // that will use this allocator.\n  if (n->attrs().Find(kScopedAllocatorAttrName) != nullptr) {\n    VLOG(2) << \"Skip node [\" << n->DebugString()\n            << \"] for constant folding due to scoped allocator\";\n    return false;\n  }\n  return true;\n}", "func_hash": 319400414252695019518928506839820495961, "file_name": "constant_folding.cc", "file_hash": 129656680363802690204267718015734305272, "cwe": ["CWE-824"], "cve": "CVE-2021-41204", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions during TensorFlow's Grappler optimizer phase, constant folding might attempt to deep copy a resource tensor. This results in a segfault, as these tensors are supposed to not change. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41204", "file_path": "tensorflow/core/common_runtime/constant_folding.cc"}
