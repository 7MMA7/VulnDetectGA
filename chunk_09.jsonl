{"idx": 197111, "project": "tinyexr", "commit_id": "a685e3332f61cd4e59324bf3f669d36973d64270", "project_url": "https://github.com/syoyo/tinyexr", "commit_url": "https://github.com/syoyo/tinyexr/commit/a685e3332f61cd4e59324bf3f669d36973d64270", "commit_message": "Make line_no with too large value(2**20) invalid. Fixes #124", "target": 1, "func": "static int DecodeChunk(EXRImage *exr_image, const EXRHeader *exr_header,\n                       const std::vector<tinyexr::tinyexr_uint64> &offsets,\n                       const unsigned char *head, const size_t size,\n                       std::string *err) {\n  int num_channels = exr_header->num_channels;\n\n  int num_scanline_blocks = 1;\n  if (exr_header->compression_type == TINYEXR_COMPRESSIONTYPE_ZIP) {\n    num_scanline_blocks = 16;\n  } else if (exr_header->compression_type == TINYEXR_COMPRESSIONTYPE_PIZ) {\n    num_scanline_blocks = 32;\n  } else if (exr_header->compression_type == TINYEXR_COMPRESSIONTYPE_ZFP) {\n    num_scanline_blocks = 16;\n  }\n\n  int data_width = exr_header->data_window[2] - exr_header->data_window[0] + 1;\n  int data_height = exr_header->data_window[3] - exr_header->data_window[1] + 1;\n\n  if ((data_width < 0) || (data_height < 0)) {\n    if (err) {\n      std::stringstream ss;\n      ss << \"Invalid data width or data height: \" << data_width << \", \"\n         << data_height << std::endl;\n      (*err) += ss.str();\n    }\n    return TINYEXR_ERROR_INVALID_DATA;\n  }\n\n  // Do not allow too large data_width and data_height. header invalid?\n  {\n    const int threshold = 1024 * 8192;  // heuristics\n    if ((data_width > threshold) || (data_height > threshold)) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"data_with or data_height too large. data_width: \" << data_width\n           << \", \"\n           << \"data_height = \" << data_height << std::endl;\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_DATA;\n    }\n  }\n\n  size_t num_blocks = offsets.size();\n\n  std::vector<size_t> channel_offset_list;\n  int pixel_data_size = 0;\n  size_t channel_offset = 0;\n  if (!tinyexr::ComputeChannelLayout(&channel_offset_list, &pixel_data_size,\n                                     &channel_offset, num_channels,\n                                     exr_header->channels)) {\n    if (err) {\n      (*err) += \"Failed to compute channel layout.\\n\";\n    }\n    return TINYEXR_ERROR_INVALID_DATA;\n  }\n\n  bool invalid_data = false;  // TODO(LTE): Use atomic lock for MT safety.\n\n  if (exr_header->tiled) {\n    // value check\n    if (exr_header->tile_size_x < 0) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"Invalid tile size x : \" << exr_header->tile_size_x << \"\\n\";\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_HEADER;\n    }\n\n    if (exr_header->tile_size_y < 0) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"Invalid tile size y : \" << exr_header->tile_size_y << \"\\n\";\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_HEADER;\n    }\n\n    size_t num_tiles = offsets.size();  // = # of blocks\n\n    exr_image->tiles = static_cast<EXRTile *>(\n        calloc(sizeof(EXRTile), static_cast<size_t>(num_tiles)));\n\n    for (size_t tile_idx = 0; tile_idx < num_tiles; tile_idx++) {\n      // Allocate memory for each tile.\n      exr_image->tiles[tile_idx].images = tinyexr::AllocateImage(\n          num_channels, exr_header->channels, exr_header->requested_pixel_types,\n          exr_header->tile_size_x, exr_header->tile_size_y);\n\n      // 16 byte: tile coordinates\n      // 4 byte : data size\n      // ~      : data(uncompressed or compressed)\n      if (offsets[tile_idx] + sizeof(int) * 5 > size) {\n        if (err) {\n          (*err) += \"Insufficient data size.\\n\";\n        }\n        return TINYEXR_ERROR_INVALID_DATA;\n      }\n\n      size_t data_size = size_t(size - (offsets[tile_idx] + sizeof(int) * 5));\n      const unsigned char *data_ptr =\n          reinterpret_cast<const unsigned char *>(head + offsets[tile_idx]);\n\n      int tile_coordinates[4];\n      memcpy(tile_coordinates, data_ptr, sizeof(int) * 4);\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[0]));\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[1]));\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[2]));\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[3]));\n\n      // @todo{ LoD }\n      if (tile_coordinates[2] != 0) {\n        return TINYEXR_ERROR_UNSUPPORTED_FEATURE;\n      }\n      if (tile_coordinates[3] != 0) {\n        return TINYEXR_ERROR_UNSUPPORTED_FEATURE;\n      }\n\n      int data_len;\n      memcpy(&data_len, data_ptr + 16,\n             sizeof(int));  // 16 = sizeof(tile_coordinates)\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&data_len));\n\n      if (data_len < 4 || size_t(data_len) > data_size) {\n        if (err) {\n          (*err) += \"Insufficient data length.\\n\";\n        }\n        return TINYEXR_ERROR_INVALID_DATA;\n      }\n\n      // Move to data addr: 20 = 16 + 4;\n      data_ptr += 20;\n\n      tinyexr::DecodeTiledPixelData(\n          exr_image->tiles[tile_idx].images,\n          &(exr_image->tiles[tile_idx].width),\n          &(exr_image->tiles[tile_idx].height),\n          exr_header->requested_pixel_types, data_ptr,\n          static_cast<size_t>(data_len), exr_header->compression_type,\n          exr_header->line_order, data_width, data_height, tile_coordinates[0],\n          tile_coordinates[1], exr_header->tile_size_x, exr_header->tile_size_y,\n          static_cast<size_t>(pixel_data_size),\n          static_cast<size_t>(exr_header->num_custom_attributes),\n          exr_header->custom_attributes,\n          static_cast<size_t>(exr_header->num_channels), exr_header->channels,\n          channel_offset_list);\n\n      exr_image->tiles[tile_idx].offset_x = tile_coordinates[0];\n      exr_image->tiles[tile_idx].offset_y = tile_coordinates[1];\n      exr_image->tiles[tile_idx].level_x = tile_coordinates[2];\n      exr_image->tiles[tile_idx].level_y = tile_coordinates[3];\n\n      exr_image->num_tiles = static_cast<int>(num_tiles);\n    }\n  } else {  // scanline format\n\n    // Don't allow too large image(256GB * pixel_data_size or more). Workaround\n    // for #104.\n    size_t total_data_len =\n        size_t(data_width) * size_t(data_height) * size_t(num_channels);\n    const bool total_data_len_overflown = sizeof(void*) == 8 ? (total_data_len >= 0x4000000000) : false;\n    if ((total_data_len == 0) || total_data_len_overflown ) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"Image data size is zero or too large: width = \" << data_width\n           << \", height = \" << data_height << \", channels = \" << num_channels\n           << std::endl;\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_DATA;\n    }\n\n    exr_image->images = tinyexr::AllocateImage(\n        num_channels, exr_header->channels, exr_header->requested_pixel_types,\n        data_width, data_height);\n\n#ifdef _OPENMP\n#pragma omp parallel for\n#endif\n    for (int y = 0; y < static_cast<int>(num_blocks); y++) {\n      size_t y_idx = static_cast<size_t>(y);\n\n      if (offsets[y_idx] + sizeof(int) * 2 > size) {\n        invalid_data = true;\n      } else {\n        // 4 byte: scan line\n        // 4 byte: data size\n        // ~     : pixel data(uncompressed or compressed)\n        size_t data_size = size_t(size - (offsets[y_idx] + sizeof(int) * 2));\n        const unsigned char *data_ptr =\n            reinterpret_cast<const unsigned char *>(head + offsets[y_idx]);\n\n        int line_no;\n        memcpy(&line_no, data_ptr, sizeof(int));\n        int data_len;\n        memcpy(&data_len, data_ptr + 4, sizeof(int));\n        tinyexr::swap4(reinterpret_cast<unsigned int *>(&line_no));\n        tinyexr::swap4(reinterpret_cast<unsigned int *>(&data_len));\n\n        if (size_t(data_len) > data_size) {\n          invalid_data = true;\n        } else if (data_len == 0) {\n          // TODO(syoyo): May be ok to raise the threshold for example `data_len\n          // < 4`\n          invalid_data = true;\n        } else {\n          // line_no may be negative.\n          int end_line_no = (std::min)(line_no + num_scanline_blocks,\n                                       (exr_header->data_window[3] + 1));\n\n          int num_lines = end_line_no - line_no;\n\n          if (num_lines <= 0) {\n            invalid_data = true;\n          } else {\n            // Move to data addr: 8 = 4 + 4;\n            data_ptr += 8;\n\n            // Adjust line_no with data_window.bmin.y\n\n            // overflow check\n            tinyexr_int64 lno = static_cast<tinyexr_int64>(line_no) - static_cast<tinyexr_int64>(exr_header->data_window[1]);\n            if (lno > std::numeric_limits<int>::max()) {\n              line_no = -1; // invalid\n            } else if (lno < -std::numeric_limits<int>::max()) {\n              line_no = -1; // invalid\n            } else {\n              line_no -= exr_header->data_window[1];\n            }\n\n            if (line_no < 0) {\n              invalid_data = true;\n            } else {\n              if (!tinyexr::DecodePixelData(\n                      exr_image->images, exr_header->requested_pixel_types,\n                      data_ptr, static_cast<size_t>(data_len),\n                      exr_header->compression_type, exr_header->line_order,\n                      data_width, data_height, data_width, y, line_no,\n                      num_lines, static_cast<size_t>(pixel_data_size),\n                      static_cast<size_t>(exr_header->num_custom_attributes),\n                      exr_header->custom_attributes,\n                      static_cast<size_t>(exr_header->num_channels),\n                      exr_header->channels, channel_offset_list)) {\n                invalid_data = true;\n              }\n            }\n          }\n        }\n      }\n    }  // omp parallel\n  }\n\n  if (invalid_data) {\n    if (err) {\n      std::stringstream ss;\n      (*err) += \"Invalid data found when decoding pixels.\\n\";\n    }\n    return TINYEXR_ERROR_INVALID_DATA;\n  }\n\n  // Overwrite `pixel_type` with `requested_pixel_type`.\n  {\n    for (int c = 0; c < exr_header->num_channels; c++) {\n      exr_header->pixel_types[c] = exr_header->requested_pixel_types[c];\n    }\n  }\n\n  {\n    exr_image->num_channels = num_channels;\n\n    exr_image->width = data_width;\n    exr_image->height = data_height;\n  }\n\n  return TINYEXR_SUCCESS;\n}", "func_hash": 204576660378471471312302041175468111939, "file_name": "tinyexr.h", "file_hash": 28581937103314011160798682220091501322, "cwe": ["CWE-20"], "cve": "CVE-2020-19490", "cve_desc": "tinyexr 0.9.5 has a integer overflow over-write in tinyexr::DecodePixelData in tinyexr.h, related to OpenEXR code.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2020-19490", "file_path": "tinyexr.h"}
{"idx": 252437, "project": "tinyexr", "commit_id": "a685e3332f61cd4e59324bf3f669d36973d64270", "project_url": "https://github.com/syoyo/tinyexr", "commit_url": "https://github.com/syoyo/tinyexr/commit/a685e3332f61cd4e59324bf3f669d36973d64270", "commit_message": "Make line_no with too large value(2**20) invalid. Fixes #124", "target": 0, "func": "static int DecodeChunk(EXRImage *exr_image, const EXRHeader *exr_header,\n                       const std::vector<tinyexr::tinyexr_uint64> &offsets,\n                       const unsigned char *head, const size_t size,\n                       std::string *err) {\n  int num_channels = exr_header->num_channels;\n\n  int num_scanline_blocks = 1;\n  if (exr_header->compression_type == TINYEXR_COMPRESSIONTYPE_ZIP) {\n    num_scanline_blocks = 16;\n  } else if (exr_header->compression_type == TINYEXR_COMPRESSIONTYPE_PIZ) {\n    num_scanline_blocks = 32;\n  } else if (exr_header->compression_type == TINYEXR_COMPRESSIONTYPE_ZFP) {\n    num_scanline_blocks = 16;\n  }\n\n  int data_width = exr_header->data_window[2] - exr_header->data_window[0] + 1;\n  int data_height = exr_header->data_window[3] - exr_header->data_window[1] + 1;\n\n  if ((data_width < 0) || (data_height < 0)) {\n    if (err) {\n      std::stringstream ss;\n      ss << \"Invalid data width or data height: \" << data_width << \", \"\n         << data_height << std::endl;\n      (*err) += ss.str();\n    }\n    return TINYEXR_ERROR_INVALID_DATA;\n  }\n\n  // Do not allow too large data_width and data_height. header invalid?\n  {\n    const int threshold = 1024 * 8192;  // heuristics\n    if ((data_width > threshold) || (data_height > threshold)) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"data_with or data_height too large. data_width: \" << data_width\n           << \", \"\n           << \"data_height = \" << data_height << std::endl;\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_DATA;\n    }\n  }\n\n  size_t num_blocks = offsets.size();\n\n  std::vector<size_t> channel_offset_list;\n  int pixel_data_size = 0;\n  size_t channel_offset = 0;\n  if (!tinyexr::ComputeChannelLayout(&channel_offset_list, &pixel_data_size,\n                                     &channel_offset, num_channels,\n                                     exr_header->channels)) {\n    if (err) {\n      (*err) += \"Failed to compute channel layout.\\n\";\n    }\n    return TINYEXR_ERROR_INVALID_DATA;\n  }\n\n  bool invalid_data = false;  // TODO(LTE): Use atomic lock for MT safety.\n\n  if (exr_header->tiled) {\n    // value check\n    if (exr_header->tile_size_x < 0) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"Invalid tile size x : \" << exr_header->tile_size_x << \"\\n\";\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_HEADER;\n    }\n\n    if (exr_header->tile_size_y < 0) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"Invalid tile size y : \" << exr_header->tile_size_y << \"\\n\";\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_HEADER;\n    }\n\n    size_t num_tiles = offsets.size();  // = # of blocks\n\n    exr_image->tiles = static_cast<EXRTile *>(\n        calloc(sizeof(EXRTile), static_cast<size_t>(num_tiles)));\n\n    for (size_t tile_idx = 0; tile_idx < num_tiles; tile_idx++) {\n      // Allocate memory for each tile.\n      exr_image->tiles[tile_idx].images = tinyexr::AllocateImage(\n          num_channels, exr_header->channels, exr_header->requested_pixel_types,\n          exr_header->tile_size_x, exr_header->tile_size_y);\n\n      // 16 byte: tile coordinates\n      // 4 byte : data size\n      // ~      : data(uncompressed or compressed)\n      if (offsets[tile_idx] + sizeof(int) * 5 > size) {\n        if (err) {\n          (*err) += \"Insufficient data size.\\n\";\n        }\n        return TINYEXR_ERROR_INVALID_DATA;\n      }\n\n      size_t data_size = size_t(size - (offsets[tile_idx] + sizeof(int) * 5));\n      const unsigned char *data_ptr =\n          reinterpret_cast<const unsigned char *>(head + offsets[tile_idx]);\n\n      int tile_coordinates[4];\n      memcpy(tile_coordinates, data_ptr, sizeof(int) * 4);\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[0]));\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[1]));\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[2]));\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&tile_coordinates[3]));\n\n      // @todo{ LoD }\n      if (tile_coordinates[2] != 0) {\n        return TINYEXR_ERROR_UNSUPPORTED_FEATURE;\n      }\n      if (tile_coordinates[3] != 0) {\n        return TINYEXR_ERROR_UNSUPPORTED_FEATURE;\n      }\n\n      int data_len;\n      memcpy(&data_len, data_ptr + 16,\n             sizeof(int));  // 16 = sizeof(tile_coordinates)\n      tinyexr::swap4(reinterpret_cast<unsigned int *>(&data_len));\n\n      if (data_len < 4 || size_t(data_len) > data_size) {\n        if (err) {\n          (*err) += \"Insufficient data length.\\n\";\n        }\n        return TINYEXR_ERROR_INVALID_DATA;\n      }\n\n      // Move to data addr: 20 = 16 + 4;\n      data_ptr += 20;\n\n      tinyexr::DecodeTiledPixelData(\n          exr_image->tiles[tile_idx].images,\n          &(exr_image->tiles[tile_idx].width),\n          &(exr_image->tiles[tile_idx].height),\n          exr_header->requested_pixel_types, data_ptr,\n          static_cast<size_t>(data_len), exr_header->compression_type,\n          exr_header->line_order, data_width, data_height, tile_coordinates[0],\n          tile_coordinates[1], exr_header->tile_size_x, exr_header->tile_size_y,\n          static_cast<size_t>(pixel_data_size),\n          static_cast<size_t>(exr_header->num_custom_attributes),\n          exr_header->custom_attributes,\n          static_cast<size_t>(exr_header->num_channels), exr_header->channels,\n          channel_offset_list);\n\n      exr_image->tiles[tile_idx].offset_x = tile_coordinates[0];\n      exr_image->tiles[tile_idx].offset_y = tile_coordinates[1];\n      exr_image->tiles[tile_idx].level_x = tile_coordinates[2];\n      exr_image->tiles[tile_idx].level_y = tile_coordinates[3];\n\n      exr_image->num_tiles = static_cast<int>(num_tiles);\n    }\n  } else {  // scanline format\n\n    // Don't allow too large image(256GB * pixel_data_size or more). Workaround\n    // for #104.\n    size_t total_data_len =\n        size_t(data_width) * size_t(data_height) * size_t(num_channels);\n    const bool total_data_len_overflown = sizeof(void*) == 8 ? (total_data_len >= 0x4000000000) : false;\n    if ((total_data_len == 0) || total_data_len_overflown ) {\n      if (err) {\n        std::stringstream ss;\n        ss << \"Image data size is zero or too large: width = \" << data_width\n           << \", height = \" << data_height << \", channels = \" << num_channels\n           << std::endl;\n        (*err) += ss.str();\n      }\n      return TINYEXR_ERROR_INVALID_DATA;\n    }\n\n    exr_image->images = tinyexr::AllocateImage(\n        num_channels, exr_header->channels, exr_header->requested_pixel_types,\n        data_width, data_height);\n\n#ifdef _OPENMP\n#pragma omp parallel for\n#endif\n    for (int y = 0; y < static_cast<int>(num_blocks); y++) {\n      size_t y_idx = static_cast<size_t>(y);\n\n      if (offsets[y_idx] + sizeof(int) * 2 > size) {\n        invalid_data = true;\n      } else {\n        // 4 byte: scan line\n        // 4 byte: data size\n        // ~     : pixel data(uncompressed or compressed)\n        size_t data_size = size_t(size - (offsets[y_idx] + sizeof(int) * 2));\n        const unsigned char *data_ptr =\n            reinterpret_cast<const unsigned char *>(head + offsets[y_idx]);\n\n        int line_no;\n        memcpy(&line_no, data_ptr, sizeof(int));\n        int data_len;\n        memcpy(&data_len, data_ptr + 4, sizeof(int));\n        tinyexr::swap4(reinterpret_cast<unsigned int *>(&line_no));\n        tinyexr::swap4(reinterpret_cast<unsigned int *>(&data_len));\n\n        if (size_t(data_len) > data_size) {\n          invalid_data = true;\n\n        } else if ((line_no > (2 << 20)) || (line_no < -(2 << 20))) {\n          // Too large value. Assume this is invalid\n          // 2**20 = 1048576 = heuristic value.\n          invalid_data = true;\n        } else if (data_len == 0) {\n          // TODO(syoyo): May be ok to raise the threshold for example `data_len\n          // < 4`\n          invalid_data = true;\n        } else {\n          // line_no may be negative.\n          int end_line_no = (std::min)(line_no + num_scanline_blocks,\n                                       (exr_header->data_window[3] + 1));\n\n          int num_lines = end_line_no - line_no;\n\n          if (num_lines <= 0) {\n            invalid_data = true;\n          } else {\n            // Move to data addr: 8 = 4 + 4;\n            data_ptr += 8;\n\n            // Adjust line_no with data_window.bmin.y\n\n            // overflow check\n            tinyexr_int64 lno = static_cast<tinyexr_int64>(line_no) - static_cast<tinyexr_int64>(exr_header->data_window[1]);\n            if (lno > std::numeric_limits<int>::max()) {\n              line_no = -1; // invalid\n            } else if (lno < -std::numeric_limits<int>::max()) {\n              line_no = -1; // invalid\n            } else {\n              line_no -= exr_header->data_window[1];\n            }\n\n            if (line_no < 0) {\n              invalid_data = true;\n            } else {\n              if (!tinyexr::DecodePixelData(\n                      exr_image->images, exr_header->requested_pixel_types,\n                      data_ptr, static_cast<size_t>(data_len),\n                      exr_header->compression_type, exr_header->line_order,\n                      data_width, data_height, data_width, y, line_no,\n                      num_lines, static_cast<size_t>(pixel_data_size),\n                      static_cast<size_t>(exr_header->num_custom_attributes),\n                      exr_header->custom_attributes,\n                      static_cast<size_t>(exr_header->num_channels),\n                      exr_header->channels, channel_offset_list)) {\n                invalid_data = true;\n              }\n            }\n          }\n        }\n      }\n    }  // omp parallel\n  }\n\n  if (invalid_data) {\n    if (err) {\n      std::stringstream ss;\n      (*err) += \"Invalid data found when decoding pixels.\\n\";\n    }\n    return TINYEXR_ERROR_INVALID_DATA;\n  }\n\n  // Overwrite `pixel_type` with `requested_pixel_type`.\n  {\n    for (int c = 0; c < exr_header->num_channels; c++) {\n      exr_header->pixel_types[c] = exr_header->requested_pixel_types[c];\n    }\n  }\n\n  {\n    exr_image->num_channels = num_channels;\n\n    exr_image->width = data_width;\n    exr_image->height = data_height;\n  }\n\n  return TINYEXR_SUCCESS;\n}", "func_hash": 151063885227112731774220545683650954280, "file_name": "tinyexr.h", "file_hash": 117099710183052888477970990927407043374, "cwe": ["CWE-20"], "cve": "CVE-2020-19490", "cve_desc": "tinyexr 0.9.5 has a integer overflow over-write in tinyexr::DecodePixelData in tinyexr.h, related to OpenEXR code.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2020-19490", "file_path": "tinyexr.h"}
{"idx": 197135, "project": "linux", "commit_id": "505d9dcb0f7ddf9d075e729523a33d38642ae680", "project_url": "https://github.com/torvalds/linux", "commit_url": "https://github.com/torvalds/linux/commit/505d9dcb0f7ddf9d075e729523a33d38642ae680", "commit_message": "crypto: ccp - fix resource leaks in ccp_run_aes_gcm_cmd()\n\nThere are three bugs in this code:\n\n1) If we ccp_init_data() fails for &src then we need to free aad.\n   Use goto e_aad instead of goto e_ctx.\n2) The label to free the &final_wa was named incorrectly as \"e_tag\" but\n   it should have been \"e_final_wa\".  One error path leaked &final_wa.\n3) The &tag was leaked on one error path.  In that case, I added a free\n   before the goto because the resource was local to that block.\n\nFixes: 36cf515b9bbe (\"crypto: ccp - Enable support for AES GCM on v5 CCPs\")\nReported-by: \"minihanshen(\u6c88\u660e\u822a)\" <minihanshen@tencent.com>\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nReviewed-by: John Allen <john.allen@amd.com>\nTested-by: John Allen <john.allen@amd.com>\nSigned-off-by: Herbert Xu <herbert@gondor.apana.org.au>", "target": 1, "func": "ccp_run_aes_gcm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx, final_wa, tag;\n\tstruct ccp_data src, dst;\n\tstruct ccp_data aad;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int authsize;\n\tunsigned int jobid;\n\tunsigned int ilen;\n\tbool in_place = true; /* Default value */\n\t__be64 *final;\n\tint ret;\n\n\tstruct scatterlist *p_inp, sg_inp[2];\n\tstruct scatterlist *p_tag, sg_tag[2];\n\tstruct scatterlist *p_outp, sg_outp[2];\n\tstruct scatterlist *p_aad;\n\n\tif (!aes->iv)\n\t\treturn -EINVAL;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t\t(aes->key_len == AES_KEYSIZE_192) ||\n\t\t(aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key) /* Gotta have a key SGL */\n\t\treturn -EINVAL;\n\n\t/* Zero defaults to 16 bytes, the maximum size */\n\tauthsize = aes->authsize ? aes->authsize : AES_BLOCK_SIZE;\n\tswitch (authsize) {\n\tcase 16:\n\tcase 15:\n\tcase 14:\n\tcase 13:\n\tcase 12:\n\tcase 8:\n\tcase 4:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* First, decompose the source buffer into AAD & PT,\n\t * and the destination buffer into AAD, CT & tag, or\n\t * the input into CT & tag.\n\t * It is expected that the input and output SGs will\n\t * be valid, even if the AAD and input lengths are 0.\n\t */\n\tp_aad = aes->src;\n\tp_inp = scatterwalk_ffwd(sg_inp, aes->src, aes->aad_len);\n\tp_outp = scatterwalk_ffwd(sg_outp, aes->dst, aes->aad_len);\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\tilen = aes->src_len;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_outp, ilen);\n\t} else {\n\t\t/* Input length for decryption includes tag */\n\t\tilen = aes->src_len - authsize;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_inp, ilen);\n\t}\n\n\tjobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\n\t/* Copy the key to the LSB */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* Copy the context (IV) to the LSB.\n\t * There is an assumption here that the IV is 96 bits in length, plus\n\t * a nonce of 32 bits. If no IV is present, use a zeroed buffer.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES - aes->iv_len;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\top.init = 1;\n\tif (aes->aad_len > 0) {\n\t\t/* Step 1: Run a GHASH over the Additional Authenticated Data */\n\t\tret = ccp_init_data(&aad, cmd_q, p_aad, aes->aad_len,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\t\top.u.aes.action = CCP_AES_GHASHAAD;\n\n\t\twhile (aad.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&aad, NULL, &op, AES_BLOCK_SIZE, true);\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_aad;\n\t\t\t}\n\n\t\t\tccp_process_data(&aad, NULL, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\top.u.aes.mode = CCP_AES_MODE_GCTR;\n\top.u.aes.action = aes->action;\n\n\tif (ilen > 0) {\n\t\t/* Step 2: Run a GCTR over the plaintext */\n\t\tin_place = (sg_virt(p_inp) == sg_virt(p_outp)) ? true : false;\n\n\t\tret = ccp_init_data(&src, cmd_q, p_inp, ilen,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n\t\t\t\t\t     : DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\tif (in_place) {\n\t\t\tdst = src;\n\t\t} else {\n\t\t\tret = ccp_init_data(&dst, cmd_q, p_outp, ilen,\n\t\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t}\n\n\t\top.soc = 0;\n\t\top.eom = 0;\n\t\top.init = 1;\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\t\tif (!src.sg_wa.bytes_left) {\n\t\t\t\tunsigned int nbytes = ilen % AES_BLOCK_SIZE;\n\n\t\t\t\tif (nbytes) {\n\t\t\t\t\top.eom = 1;\n\t\t\t\t\top.u.aes.size = (nbytes * 8) - 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_dst;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, &dst, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\t/* Step 3: Update the IV portion of the context with the original IV */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* Step 4: Concatenate the lengths of the AAD and source, and\n\t * hash that 16 byte buffer.\n\t */\n\tret = ccp_init_dm_workarea(&final_wa, cmd_q, AES_BLOCK_SIZE,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_dst;\n\tfinal = (__be64 *)final_wa.address;\n\tfinal[0] = cpu_to_be64(aes->aad_len * 8);\n\tfinal[1] = cpu_to_be64(ilen * 8);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\top.u.aes.action = CCP_AES_GHASHFINAL;\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = final_wa.dma.address;\n\top.src.u.dma.length = AES_BLOCK_SIZE;\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = final_wa.dma.address;\n\top.dst.u.dma.length = AES_BLOCK_SIZE;\n\top.eom = 1;\n\top.u.aes.size = 0;\n\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\t/* Put the ciphered tag after the ciphertext. */\n\t\tccp_get_dm_area(&final_wa, 0, p_tag, 0, authsize);\n\t} else {\n\t\t/* Does this ciphered tag match the input? */\n\t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_tag;\n\t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n\t\tif (ret)\n\t\t\tgoto e_tag;\n\n\t\tret = crypto_memneq(tag.address, final_wa.address,\n\t\t\t\t    authsize) ? -EBADMSG : 0;\n\t\tccp_dm_free(&tag);\n\t}\n\ne_tag:\n\tccp_dm_free(&final_wa);\n\ne_dst:\n\tif (ilen > 0 && !in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tif (ilen > 0)\n\t\tccp_free_data(&src, cmd_q);\n\ne_aad:\n\tif (aes->aad_len)\n\t\tccp_free_data(&aad, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}", "func_hash": 164338035268758054525512134234222500237, "file_name": "ccp-ops.c", "file_hash": 220238669007539960746784666211129167889, "cwe": ["CWE-703"], "cve": "CVE-2021-3744", "cve_desc": "A memory leak flaw was found in the Linux kernel in the ccp_run_aes_gcm_cmd() function in drivers/crypto/ccp/ccp-ops.c, which allows attackers to cause a denial of service (memory consumption). This vulnerability is similar with the older CVE-2019-18808.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-3744", "file_path": "drivers/crypto/ccp/ccp-ops.c"}
{"idx": 253699, "project": "linux", "commit_id": "505d9dcb0f7ddf9d075e729523a33d38642ae680", "project_url": "https://github.com/torvalds/linux", "commit_url": "https://github.com/torvalds/linux/commit/505d9dcb0f7ddf9d075e729523a33d38642ae680", "commit_message": "crypto: ccp - fix resource leaks in ccp_run_aes_gcm_cmd()\n\nThere are three bugs in this code:\n\n1) If we ccp_init_data() fails for &src then we need to free aad.\n   Use goto e_aad instead of goto e_ctx.\n2) The label to free the &final_wa was named incorrectly as \"e_tag\" but\n   it should have been \"e_final_wa\".  One error path leaked &final_wa.\n3) The &tag was leaked on one error path.  In that case, I added a free\n   before the goto because the resource was local to that block.\n\nFixes: 36cf515b9bbe (\"crypto: ccp - Enable support for AES GCM on v5 CCPs\")\nReported-by: \"minihanshen(\u6c88\u660e\u822a)\" <minihanshen@tencent.com>\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nReviewed-by: John Allen <john.allen@amd.com>\nTested-by: John Allen <john.allen@amd.com>\nSigned-off-by: Herbert Xu <herbert@gondor.apana.org.au>", "target": 0, "func": "ccp_run_aes_gcm_cmd(struct ccp_cmd_queue *cmd_q, struct ccp_cmd *cmd)\n{\n\tstruct ccp_aes_engine *aes = &cmd->u.aes;\n\tstruct ccp_dm_workarea key, ctx, final_wa, tag;\n\tstruct ccp_data src, dst;\n\tstruct ccp_data aad;\n\tstruct ccp_op op;\n\tunsigned int dm_offset;\n\tunsigned int authsize;\n\tunsigned int jobid;\n\tunsigned int ilen;\n\tbool in_place = true; /* Default value */\n\t__be64 *final;\n\tint ret;\n\n\tstruct scatterlist *p_inp, sg_inp[2];\n\tstruct scatterlist *p_tag, sg_tag[2];\n\tstruct scatterlist *p_outp, sg_outp[2];\n\tstruct scatterlist *p_aad;\n\n\tif (!aes->iv)\n\t\treturn -EINVAL;\n\n\tif (!((aes->key_len == AES_KEYSIZE_128) ||\n\t\t(aes->key_len == AES_KEYSIZE_192) ||\n\t\t(aes->key_len == AES_KEYSIZE_256)))\n\t\treturn -EINVAL;\n\n\tif (!aes->key) /* Gotta have a key SGL */\n\t\treturn -EINVAL;\n\n\t/* Zero defaults to 16 bytes, the maximum size */\n\tauthsize = aes->authsize ? aes->authsize : AES_BLOCK_SIZE;\n\tswitch (authsize) {\n\tcase 16:\n\tcase 15:\n\tcase 14:\n\tcase 13:\n\tcase 12:\n\tcase 8:\n\tcase 4:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t/* First, decompose the source buffer into AAD & PT,\n\t * and the destination buffer into AAD, CT & tag, or\n\t * the input into CT & tag.\n\t * It is expected that the input and output SGs will\n\t * be valid, even if the AAD and input lengths are 0.\n\t */\n\tp_aad = aes->src;\n\tp_inp = scatterwalk_ffwd(sg_inp, aes->src, aes->aad_len);\n\tp_outp = scatterwalk_ffwd(sg_outp, aes->dst, aes->aad_len);\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\tilen = aes->src_len;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_outp, ilen);\n\t} else {\n\t\t/* Input length for decryption includes tag */\n\t\tilen = aes->src_len - authsize;\n\t\tp_tag = scatterwalk_ffwd(sg_tag, p_inp, ilen);\n\t}\n\n\tjobid = CCP_NEW_JOBID(cmd_q->ccp);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\n\t/* Copy the key to the LSB */\n\tret = ccp_init_dm_workarea(&key, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret)\n\t\treturn ret;\n\n\tdm_offset = CCP_SB_BYTES - aes->key_len;\n\tret = ccp_set_dm_area(&key, dm_offset, aes->key, 0, aes->key_len);\n\tif (ret)\n\t\tgoto e_key;\n\tret = ccp_copy_to_sb(cmd_q, &key, op.jobid, op.sb_key,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_key;\n\t}\n\n\t/* Copy the context (IV) to the LSB.\n\t * There is an assumption here that the IV is 96 bits in length, plus\n\t * a nonce of 32 bits. If no IV is present, use a zeroed buffer.\n\t */\n\tret = ccp_init_dm_workarea(&ctx, cmd_q,\n\t\t\t\t   CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_key;\n\n\tdm_offset = CCP_AES_CTX_SB_COUNT * CCP_SB_BYTES - aes->iv_len;\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_ctx;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_ctx;\n\t}\n\n\top.init = 1;\n\tif (aes->aad_len > 0) {\n\t\t/* Step 1: Run a GHASH over the Additional Authenticated Data */\n\t\tret = ccp_init_data(&aad, cmd_q, p_aad, aes->aad_len,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_ctx;\n\n\t\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\t\top.u.aes.action = CCP_AES_GHASHAAD;\n\n\t\twhile (aad.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&aad, NULL, &op, AES_BLOCK_SIZE, true);\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_aad;\n\t\t\t}\n\n\t\t\tccp_process_data(&aad, NULL, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\top.u.aes.mode = CCP_AES_MODE_GCTR;\n\top.u.aes.action = aes->action;\n\n\tif (ilen > 0) {\n\t\t/* Step 2: Run a GCTR over the plaintext */\n\t\tin_place = (sg_virt(p_inp) == sg_virt(p_outp)) ? true : false;\n\n\t\tret = ccp_init_data(&src, cmd_q, p_inp, ilen,\n\t\t\t\t    AES_BLOCK_SIZE,\n\t\t\t\t    in_place ? DMA_BIDIRECTIONAL\n\t\t\t\t\t     : DMA_TO_DEVICE);\n\t\tif (ret)\n\t\t\tgoto e_aad;\n\n\t\tif (in_place) {\n\t\t\tdst = src;\n\t\t} else {\n\t\t\tret = ccp_init_data(&dst, cmd_q, p_outp, ilen,\n\t\t\t\t\t    AES_BLOCK_SIZE, DMA_FROM_DEVICE);\n\t\t\tif (ret)\n\t\t\t\tgoto e_src;\n\t\t}\n\n\t\top.soc = 0;\n\t\top.eom = 0;\n\t\top.init = 1;\n\t\twhile (src.sg_wa.bytes_left) {\n\t\t\tccp_prepare_data(&src, &dst, &op, AES_BLOCK_SIZE, true);\n\t\t\tif (!src.sg_wa.bytes_left) {\n\t\t\t\tunsigned int nbytes = ilen % AES_BLOCK_SIZE;\n\n\t\t\t\tif (nbytes) {\n\t\t\t\t\top.eom = 1;\n\t\t\t\t\top.u.aes.size = (nbytes * 8) - 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\t\t\tif (ret) {\n\t\t\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\t\t\tgoto e_dst;\n\t\t\t}\n\n\t\t\tccp_process_data(&src, &dst, &op);\n\t\t\top.init = 0;\n\t\t}\n\t}\n\n\t/* Step 3: Update the IV portion of the context with the original IV */\n\tret = ccp_copy_from_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t       CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\tret = ccp_set_dm_area(&ctx, dm_offset, aes->iv, 0, aes->iv_len);\n\tif (ret)\n\t\tgoto e_dst;\n\n\tret = ccp_copy_to_sb(cmd_q, &ctx, op.jobid, op.sb_ctx,\n\t\t\t     CCP_PASSTHRU_BYTESWAP_256BIT);\n\tif (ret) {\n\t\tcmd->engine_error = cmd_q->cmd_error;\n\t\tgoto e_dst;\n\t}\n\n\t/* Step 4: Concatenate the lengths of the AAD and source, and\n\t * hash that 16 byte buffer.\n\t */\n\tret = ccp_init_dm_workarea(&final_wa, cmd_q, AES_BLOCK_SIZE,\n\t\t\t\t   DMA_BIDIRECTIONAL);\n\tif (ret)\n\t\tgoto e_dst;\n\tfinal = (__be64 *)final_wa.address;\n\tfinal[0] = cpu_to_be64(aes->aad_len * 8);\n\tfinal[1] = cpu_to_be64(ilen * 8);\n\n\tmemset(&op, 0, sizeof(op));\n\top.cmd_q = cmd_q;\n\top.jobid = jobid;\n\top.sb_key = cmd_q->sb_key; /* Pre-allocated */\n\top.sb_ctx = cmd_q->sb_ctx; /* Pre-allocated */\n\top.init = 1;\n\top.u.aes.type = aes->type;\n\top.u.aes.mode = CCP_AES_MODE_GHASH;\n\top.u.aes.action = CCP_AES_GHASHFINAL;\n\top.src.type = CCP_MEMTYPE_SYSTEM;\n\top.src.u.dma.address = final_wa.dma.address;\n\top.src.u.dma.length = AES_BLOCK_SIZE;\n\top.dst.type = CCP_MEMTYPE_SYSTEM;\n\top.dst.u.dma.address = final_wa.dma.address;\n\top.dst.u.dma.length = AES_BLOCK_SIZE;\n\top.eom = 1;\n\top.u.aes.size = 0;\n\tret = cmd_q->ccp->vdata->perform->aes(&op);\n\tif (ret)\n\t\tgoto e_final_wa;\n\n\tif (aes->action == CCP_AES_ACTION_ENCRYPT) {\n\t\t/* Put the ciphered tag after the ciphertext. */\n\t\tccp_get_dm_area(&final_wa, 0, p_tag, 0, authsize);\n\t} else {\n\t\t/* Does this ciphered tag match the input? */\n\t\tret = ccp_init_dm_workarea(&tag, cmd_q, authsize,\n\t\t\t\t\t   DMA_BIDIRECTIONAL);\n\t\tif (ret)\n\t\t\tgoto e_final_wa;\n\t\tret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);\n\t\tif (ret) {\n\t\t\tccp_dm_free(&tag);\n\t\t\tgoto e_final_wa;\n\t\t}\n\n\t\tret = crypto_memneq(tag.address, final_wa.address,\n\t\t\t\t    authsize) ? -EBADMSG : 0;\n\t\tccp_dm_free(&tag);\n\t}\n\ne_final_wa:\n\tccp_dm_free(&final_wa);\n\ne_dst:\n\tif (ilen > 0 && !in_place)\n\t\tccp_free_data(&dst, cmd_q);\n\ne_src:\n\tif (ilen > 0)\n\t\tccp_free_data(&src, cmd_q);\n\ne_aad:\n\tif (aes->aad_len)\n\t\tccp_free_data(&aad, cmd_q);\n\ne_ctx:\n\tccp_dm_free(&ctx);\n\ne_key:\n\tccp_dm_free(&key);\n\n\treturn ret;\n}", "func_hash": 192342382845429506037482636536985801583, "file_name": "ccp-ops.c", "file_hash": 212296156007927081336268591305304160676, "cwe": ["CWE-703"], "cve": "CVE-2021-3744", "cve_desc": "A memory leak flaw was found in the Linux kernel in the ccp_run_aes_gcm_cmd() function in drivers/crypto/ccp/ccp-ops.c, which allows attackers to cause a denial of service (memory consumption). This vulnerability is similar with the older CVE-2019-18808.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-3744", "file_path": "drivers/crypto/ccp/ccp-ops.c"}
{"idx": 197142, "project": "tensorflow", "commit_id": "6da6620efad397c85493b8f8667b821403516708", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/6da6620efad397c85493b8f8667b821403516708", "commit_message": "Secure tf.raw_ops.QuantizeV2\n\nValidate size and shape of min_range and max_range\nEnsure axis is within input dims limits\n\nPiperOrigin-RevId: 387232799\nChange-Id: I36975281f7b5758e9e31a8dcc73fe610ef456318", "target": 1, "func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_range = ctx->input(1);\n    const Tensor& input_max_range = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n\n    const TensorShape& minmax_shape = ctx->input(1).shape();\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n\n    Tensor* output_min_tensor = nullptr;\n    Tensor* output_max_tensor = nullptr;\n\n    if (num_slices == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(1, {}, &output_min_tensor));\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(2, {}, &output_max_tensor));\n      const float min_range = input_min_range.template flat<float>()(0);\n      const float max_range = input_max_range.template flat<float>()(0);\n      QuantizeTensor(ctx, input, min_range, max_range, output,\n                     output_min_tensor, output_max_tensor);\n      return;\n    }\n\n    OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                      \"Quantize with axis != -1.\"));\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(1, minmax_shape, &output_min_tensor));\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(2, minmax_shape, &output_max_tensor));\n\n    auto input_tensor =\n        input.template flat_inner_outer_dims<float, 3>(axis_ - 1);\n    int64_t pre_dim = 1, post_dim = 1;\n    for (int i = 0; i < axis_; ++i) {\n      pre_dim *= output->dim_size(i);\n    }\n    for (int i = axis_ + 1; i < output->dims(); ++i) {\n      post_dim *= output->dim_size(i);\n    }\n    auto output_tensor = output->template bit_casted_shaped<T, 3>(\n        {pre_dim, num_slices, post_dim});\n    auto min_ranges = input_min_range.template vec<float>();\n    auto max_ranges = input_max_range.template vec<float>();\n    for (int i = 0; i < num_slices; ++i) {\n      QuantizeSlice(ctx->eigen_device<Device>(), ctx,\n                    input_tensor.template chip<1>(i), min_ranges(i),\n                    max_ranges(i), output_tensor.template chip<1>(i),\n                    &output_min_tensor->flat<float>()(i),\n                    &output_max_tensor->flat<float>()(i));\n    }\n  }", "func_hash": 175054869100587978050053791864843631748, "file_name": "quantize_op.cc", "file_hash": 45910676002815731564213734124644481802, "cwe": ["CWE-476"], "cve": "CVE-2021-37663", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions due to incomplete validation in `tf.raw_ops.QuantizeV2`, an attacker can trigger undefined behavior via binding a reference to a null pointer or can access data outside the bounds of heap allocated arrays. The [implementation](https://github.com/tensorflow/tensorflow/blob/84d053187cb80d975ef2b9684d4b61981bca0c41/tensorflow/core/kernels/quantize_op.cc#L59) has some validation but does not check that `min_range` and `max_range` both have the same non-zero number of elements. If `axis` is provided (i.e., not `-1`), then validation should check that it is a value in range for the rank of `input` tensor and then the lengths of `min_range` and `max_range` inputs match the `axis` dimension of the `input` tensor. We have patched the issue in GitHub commit 6da6620efad397c85493b8f8667b821403516708. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37663", "file_path": "tensorflow/core/kernels/quantize_op.cc"}
{"idx": 253970, "project": "tensorflow", "commit_id": "6da6620efad397c85493b8f8667b821403516708", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/6da6620efad397c85493b8f8667b821403516708", "commit_message": "Secure tf.raw_ops.QuantizeV2\n\nValidate size and shape of min_range and max_range\nEnsure axis is within input dims limits\n\nPiperOrigin-RevId: 387232799\nChange-Id: I36975281f7b5758e9e31a8dcc73fe610ef456318", "target": 0, "func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const Tensor& input_min_range = ctx->input(1);\n    const Tensor& input_max_range = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      OP_REQUIRES(\n          ctx, input.dims() > axis_,\n          errors::InvalidArgument(\n              \"Axis is on a zero-based index, so its value must always be less \"\n              \"than number of input's dims, but given axis value was \",\n              axis_, \" and input's dims was \", input.dims()));\n      num_slices = input.dim_size(axis_);\n      OP_REQUIRES(ctx, input_min_range.dims() == 1,\n                  errors::InvalidArgument(\n                      \"If axis is specified, min_range must be a 1-D tensor \"\n                      \"whose size matches the axis dimension of the input and \"\n                      \"output tensors, but min_range dims are \",\n                      input_min_range.dims()));\n      OP_REQUIRES(ctx, input_min_range.dim_size(0) == num_slices,\n                  errors::InvalidArgument(\n                      \"If axis is specified, min_range must be a 1-D tensor \"\n                      \"whose size matches the axis dimension of the input and \"\n                      \"output tensors, but min_range is a 1-D tensor of size \",\n                      input_min_range.dim_size(0),\n                      \" and input's axis dimension is of size \", num_slices));\n      OP_REQUIRES(ctx, input_max_range.dims() == 1,\n                  errors::InvalidArgument(\n                      \"If axis is specified, max_range must be a 1-D tensor \"\n                      \"whose size matches the axis dimension of the input and \"\n                      \"output tensors, but max_range dims are \",\n                      input_max_range.dims()));\n      OP_REQUIRES(ctx, input_max_range.dim_size(0) == num_slices,\n                  errors::InvalidArgument(\n                      \"If axis is specified, max_range must be a 1-D tensor \"\n                      \"whose size matches the axis dimension of the input and \"\n                      \"output tensors, but max_range is a 1-D tensor of size \",\n                      input_max_range.dim_size(0),\n                      \" and input's axis dimension is of size \", num_slices));\n    } else {\n      OP_REQUIRES(ctx, input_min_range.NumElements() == 1,\n                  errors::InvalidArgument(\n                      \"If axis is not specified, min_range must contain a \"\n                      \"single float element, but it contains \",\n                      input_min_range.NumElements(), \" elements\"));\n      OP_REQUIRES(ctx, input_max_range.NumElements() == 1,\n                  errors::InvalidArgument(\n                      \"If axis is not specified, max_range must contain a \"\n                      \"single float element, but it contains \",\n                      input_max_range.NumElements(), \" elements\"));\n    }\n\n    const TensorShape& minmax_shape = ctx->input(1).shape();\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n\n    Tensor* output_min_tensor = nullptr;\n    Tensor* output_max_tensor = nullptr;\n\n    if (num_slices == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(1, {}, &output_min_tensor));\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(2, {}, &output_max_tensor));\n      const float min_range = input_min_range.template flat<float>()(0);\n      const float max_range = input_max_range.template flat<float>()(0);\n      QuantizeTensor(ctx, input, min_range, max_range, output,\n                     output_min_tensor, output_max_tensor);\n      return;\n    }\n\n    OP_REQUIRES(ctx, mode_ != QUANTIZE_MODE_MIN_FIRST,\n                errors::Unimplemented(\"MIN_FIRST mode is not implemented for \"\n                                      \"Quantize with axis != -1.\"));\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(1, minmax_shape, &output_min_tensor));\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(2, minmax_shape, &output_max_tensor));\n\n    auto input_tensor =\n        input.template flat_inner_outer_dims<float, 3>(axis_ - 1);\n    int64_t pre_dim = 1, post_dim = 1;\n    for (int i = 0; i < axis_; ++i) {\n      pre_dim *= output->dim_size(i);\n    }\n    for (int i = axis_ + 1; i < output->dims(); ++i) {\n      post_dim *= output->dim_size(i);\n    }\n    auto output_tensor = output->template bit_casted_shaped<T, 3>(\n        {pre_dim, num_slices, post_dim});\n    auto min_ranges = input_min_range.template vec<float>();\n    auto max_ranges = input_max_range.template vec<float>();\n    for (int i = 0; i < num_slices; ++i) {\n      QuantizeSlice(ctx->eigen_device<Device>(), ctx,\n                    input_tensor.template chip<1>(i), min_ranges(i),\n                    max_ranges(i), output_tensor.template chip<1>(i),\n                    &output_min_tensor->flat<float>()(i),\n                    &output_max_tensor->flat<float>()(i));\n    }\n  }", "func_hash": 245467204964919391793075021199596127255, "file_name": "quantize_op.cc", "file_hash": 256130062942264695553473842652666995281, "cwe": ["CWE-476"], "cve": "CVE-2021-37663", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions due to incomplete validation in `tf.raw_ops.QuantizeV2`, an attacker can trigger undefined behavior via binding a reference to a null pointer or can access data outside the bounds of heap allocated arrays. The [implementation](https://github.com/tensorflow/tensorflow/blob/84d053187cb80d975ef2b9684d4b61981bca0c41/tensorflow/core/kernels/quantize_op.cc#L59) has some validation but does not check that `min_range` and `max_range` both have the same non-zero number of elements. If `axis` is provided (i.e., not `-1`), then validation should check that it is a value in range for the rank of `input` tensor and then the lengths of `min_range` and `max_range` inputs match the `axis` dimension of the `input` tensor. We have patched the issue in GitHub commit 6da6620efad397c85493b8f8667b821403516708. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37663", "file_path": "tensorflow/core/kernels/quantize_op.cc"}
{"idx": 197223, "project": "njs", "commit_id": "ab1702c7af9959366a5ddc4a75b4357d4e9ebdc1", "project_url": "https://github.com/nginx/njs", "commit_url": "https://github.com/nginx/njs/commit/ab1702c7af9959366a5ddc4a75b4357d4e9ebdc1", "commit_message": "Fixed typo while calculating module path length.\n\nThe issue was introduced in 77c398f26d7e (not released yet).", "target": 1, "func": "njs_module_path(njs_vm_t *vm, const njs_str_t *dir, njs_module_info_t *info)\n{\n    char        *p;\n    size_t      length;\n    njs_bool_t  trail;\n    char        src[NJS_MAX_PATH + 1];\n\n    trail = 0;\n    length = info->name.length;\n\n    if (dir != NULL) {\n        length = dir->length;\n\n        if (length == 0) {\n            return NJS_DECLINED;\n        }\n\n        trail = (dir->start[dir->length - 1] != '/');\n\n        if (trail) {\n            length++;\n        }\n    }\n\n    if (njs_slow_path(length > NJS_MAX_PATH)) {\n        return NJS_ERROR;\n    }\n\n    p = &src[0];\n\n    if (dir != NULL) {\n        p = (char *) njs_cpymem(p, dir->start, dir->length);\n\n        if (trail) {\n            *p++ = '/';\n        }\n    }\n\n    p = (char *) njs_cpymem(p, info->name.start, info->name.length);\n    *p = '\\0';\n\n    p = realpath(&src[0], &info->path[0]);\n    if (p == NULL) {\n        return NJS_DECLINED;\n    }\n\n    info->fd = open(&info->path[0], O_RDONLY);\n    if (info->fd < 0) {\n        return NJS_DECLINED;\n    }\n\n\n    info->file.start = (u_char *) &info->path[0];\n    info->file.length = njs_strlen(info->file.start);\n\n    return NJS_OK;\n}", "func_hash": 236847431367932249474325490160608146650, "file_name": "njs_module.c", "file_hash": 230069477074111336405443824718201036517, "cwe": ["CWE-787"], "cve": "CVE-2022-29379", "cve_desc": "Nginx NJS v0.7.3 was discovered to contain a stack overflow in the function njs_default_module_loader at /src/njs/src/njs_module.c. NOTE: multiple third parties dispute this report, e.g., the behavior is only found in unreleased development code that was not part of the 0.7.2, 0.7.3, or 0.7.4 release", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29379", "file_path": "src/njs_module.c"}
{"idx": 255574, "project": "njs", "commit_id": "ab1702c7af9959366a5ddc4a75b4357d4e9ebdc1", "project_url": "https://github.com/nginx/njs", "commit_url": "https://github.com/nginx/njs/commit/ab1702c7af9959366a5ddc4a75b4357d4e9ebdc1", "commit_message": "Fixed typo while calculating module path length.\n\nThe issue was introduced in 77c398f26d7e (not released yet).", "target": 0, "func": "njs_module_path(njs_vm_t *vm, const njs_str_t *dir, njs_module_info_t *info)\n{\n    char        *p;\n    size_t      length;\n    njs_bool_t  trail;\n    char        src[NJS_MAX_PATH + 1];\n\n    trail = 0;\n    length = info->name.length;\n\n    if (dir != NULL) {\n        length += dir->length;\n\n        if (length == 0) {\n            return NJS_DECLINED;\n        }\n\n        trail = (dir->start[dir->length - 1] != '/');\n\n        if (trail) {\n            length++;\n        }\n    }\n\n    if (njs_slow_path(length > NJS_MAX_PATH)) {\n        return NJS_ERROR;\n    }\n\n    p = &src[0];\n\n    if (dir != NULL) {\n        p = (char *) njs_cpymem(p, dir->start, dir->length);\n\n        if (trail) {\n            *p++ = '/';\n        }\n    }\n\n    p = (char *) njs_cpymem(p, info->name.start, info->name.length);\n    *p = '\\0';\n\n    p = realpath(&src[0], &info->path[0]);\n    if (p == NULL) {\n        return NJS_DECLINED;\n    }\n\n    info->fd = open(&info->path[0], O_RDONLY);\n    if (info->fd < 0) {\n        return NJS_DECLINED;\n    }\n\n\n    info->file.start = (u_char *) &info->path[0];\n    info->file.length = njs_strlen(info->file.start);\n\n    return NJS_OK;\n}", "func_hash": 78973632734119439539784178182699583703, "file_name": "njs_module.c", "file_hash": 205736021540779021398705962921872486789, "cwe": ["CWE-787"], "cve": "CVE-2022-29379", "cve_desc": "Nginx NJS v0.7.3 was discovered to contain a stack overflow in the function njs_default_module_loader at /src/njs/src/njs_module.c. NOTE: multiple third parties dispute this report, e.g., the behavior is only found in unreleased development code that was not part of the 0.7.2, 0.7.3, or 0.7.4 release", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29379", "file_path": "src/njs_module.c"}
{"idx": 197239, "project": "tensorflow", "commit_id": "203214568f5bc237603dbab6e1fd389f1572f5c9", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/203214568f5bc237603dbab6e1fd389f1572f5c9", "commit_message": "Reorganize and add more validation to MKL requantization\n\nPiperOrigin-RevId: 387901341\nChange-Id: I2515b9034c64e113db0bcec8337d30643ab0a0f1", "target": 1, "func": "  void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n\n      size_t depth = input_min_vec.NumElements();\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n      OP_REQUIRES(\n          ctx, input_min_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_min has incorrect size, expected \",\n                                  depth, \" was \", input_min_vec.dim_size(0)));\n      OP_REQUIRES(\n          ctx, input_max_vec.dim_size(0) == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.dim_size(0)));\n\n      if (out_type_ == DT_QINT8) DCHECK(input_requested_min_float < 0.0f);\n\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }", "func_hash": 324241400481110766657474950076141174434, "file_name": "mkl_requantize_per_channel_op.cc", "file_hash": 251676487738280920031977890166955496760, "cwe": ["CWE-20"], "cve": "CVE-2021-37665", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions due to incomplete validation in MKL implementation of requantization, an attacker can trigger undefined behavior via binding a reference to a null pointer or can access data outside the bounds of heap allocated arrays. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/mkl/mkl_requantization_range_per_channel_op.cc) does not validate the dimensions of the `input` tensor. A similar issue occurs in `MklRequantizePerChannelOp`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/mkl/mkl_requantize_per_channel_op.cc) does not perform full validation for all the input arguments. We have patched the issue in GitHub commit 9e62869465573cb2d9b5053f1fa02a81fce21d69 and in the Github commit 203214568f5bc237603dbab6e1fd389f1572f5c9. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37665", "file_path": "tensorflow/core/kernels/mkl/mkl_requantize_per_channel_op.cc"}
{"idx": 255787, "project": "tensorflow", "commit_id": "203214568f5bc237603dbab6e1fd389f1572f5c9", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/203214568f5bc237603dbab6e1fd389f1572f5c9", "commit_message": "Reorganize and add more validation to MKL requantization\n\nPiperOrigin-RevId: 387901341\nChange-Id: I2515b9034c64e113db0bcec8337d30643ab0a0f1", "target": 0, "func": "  void Compute(OpKernelContext* ctx) override {\n    try {\n      const Tensor& input = ctx->input(kInputTensorIndex);\n      OP_REQUIRES(\n          ctx, input.dims() == 4,\n          errors::InvalidArgument(\"Current RequantizePerChannel operator\"\n                                  \"supports 4D tensors only.\"));\n\n      const Tensor& input_min_vec = ctx->input(kInputMinVecIndex);\n      size_t depth = input_min_vec.NumElements();\n      float* input_min_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_min_vec.flat<float>().data()));\n\n      const Tensor& input_max_vec = ctx->input(kInputMaxVecIndex);\n      OP_REQUIRES(\n          ctx, input_max_vec.NumElements() == depth,\n          errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                  depth, \" was \", input_max_vec.NumElements()));\n      float* input_max_vec_data = (float*)const_cast<void*>(\n          static_cast<const void*>(input_max_vec.flat<float>().data()));\n\n      const Tensor& input_requested_min = ctx->input(this->kRequestMinIndex);\n      OP_REQUIRES(\n          ctx, input_requested_min.NumElements() == 1,\n          errors::InvalidArgument(\"requested_output_min must be a scalar\"));\n      const float input_requested_min_float =\n          input_requested_min.flat<float>()(0);\n\n      const Tensor& input_requested_max = ctx->input(this->kRequestMaxIndex);\n      OP_REQUIRES(\n          ctx, input_requested_min.NumElements() == 1,\n          errors::InvalidArgument(\"requested_output_max must be a scalar\"));\n      const float input_requested_max_float =\n          input_requested_max.flat<float>()(0);\n\n      if (out_type_ == DT_QINT8) {\n        OP_REQUIRES(ctx, input_requested_min_float < 0.0f,\n                    errors::InvalidArgument(\n                        \"If out_type is QINT8, requested_output_max must be \"\n                        \"non negative, got \",\n                        input_requested_min_float));\n      }\n\n      const float factor = (out_type_ == DT_QINT8) ? 127.0f : 255.0f;\n      const float requested_min_max =\n          std::max(std::abs(input_requested_min_float),\n                   std::abs(input_requested_max_float));\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(kOutputTensorIndex,\n                                               input.shape(), &output));\n\n      std::vector<float> scales(depth);\n      for (int i = 0; i < depth; ++i) {\n        float min_max_from_vec = std::max(std::abs(input_min_vec_data[i]),\n                                          std::abs(input_max_vec_data[i]));\n        scales[i] = factor * (min_max_from_vec / requested_min_max /\n                              static_cast<float>(1L << 31));\n      }\n\n      mkldnn::primitive_attr reorder_attr;\n      reorder_attr.set_output_scales(2, scales);\n\n      memory::dims dims_mkl_order =\n          TFShapeToMklDnnDimsInNCHW(input.shape(), FORMAT_NHWC);\n      memory::desc input_md = memory::desc(dims_mkl_order, MklDnnType<qint32>(),\n                                           memory::format_tag::nhwc);\n      memory::desc output_md =\n          (out_type_ == DT_QINT8)\n              ? memory::desc(dims_mkl_order, MklDnnType<qint8>(),\n                             memory::format_tag::nhwc)\n              : memory::desc(dims_mkl_order, MklDnnType<quint8>(),\n                             memory::format_tag::nhwc);\n\n      void* input_buf =\n          static_cast<void*>(const_cast<qint32*>(input.flat<qint32>().data()));\n      void* output_buf;\n      if (out_type_ == DT_QINT8) {\n        output_buf = static_cast<void*>(\n            const_cast<qint8*>(output->flat<qint8>().data()));\n      } else {\n        output_buf = static_cast<void*>(\n            const_cast<quint8*>(output->flat<quint8>().data()));\n      }\n\n      std::unique_ptr<memory> input_mem_prim(\n          new memory(input_md, cpu_engine_, input_buf));\n      std::unique_ptr<memory> output_mem_prim(\n          new memory(output_md, cpu_engine_, output_buf));\n\n      mkldnn::reorder::primitive_desc reorder_pd =\n          ReorderPd(cpu_engine_, input_mem_prim->get_desc(), cpu_engine_,\n                    output_mem_prim->get_desc(), reorder_attr);\n      std::shared_ptr<stream> reorder_stream;\n      MklDnnThreadPool eigen_tp(ctx);\n      reorder_stream.reset(CreateStream(&eigen_tp, cpu_engine_));\n      std::unordered_map<int, mkldnn::memory> reorder_args = {\n          {MKLDNN_ARG_FROM, *input_mem_prim},\n          {MKLDNN_ARG_TO, *output_mem_prim}};\n      std::unique_ptr<mkldnn::primitive> reorder_prim(\n          new mkldnn::reorder(reorder_pd));\n      reorder_prim->execute(*reorder_stream, reorder_args);\n\n      Tensor* output_min = nullptr;\n      Tensor* output_max = nullptr;\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMinIndex, {}, &output_min));\n      OP_REQUIRES_OK(ctx,\n                     ctx->allocate_output(kOutputMaxIndex, {}, &output_max));\n\n      output_min->flat<float>()(0) = input_requested_min_float;\n      output_max->flat<float>()(0) = input_requested_max_float;\n    } catch (mkldnn::error& e) {\n      string error_msg = \"Status: \" + std::to_string(e.status) +\n                         \", message: \" + std::string(e.message) + \", in file \" +\n                         std::string(__FILE__) + \":\" + std::to_string(__LINE__);\n      OP_REQUIRES_OK(\n          ctx, errors::Aborted(\"Operation received an exception:\", error_msg));\n    }\n  }", "func_hash": 201878661619754288848170161353082339443, "file_name": "mkl_requantize_per_channel_op.cc", "file_hash": 305646743359625407590451757209165637710, "cwe": ["CWE-20"], "cve": "CVE-2021-37665", "cve_desc": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions due to incomplete validation in MKL implementation of requantization, an attacker can trigger undefined behavior via binding a reference to a null pointer or can access data outside the bounds of heap allocated arrays. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/mkl/mkl_requantization_range_per_channel_op.cc) does not validate the dimensions of the `input` tensor. A similar issue occurs in `MklRequantizePerChannelOp`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/mkl/mkl_requantize_per_channel_op.cc) does not perform full validation for all the input arguments. We have patched the issue in GitHub commit 9e62869465573cb2d9b5053f1fa02a81fce21d69 and in the Github commit 203214568f5bc237603dbab6e1fd389f1572f5c9. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-37665", "file_path": "tensorflow/core/kernels/mkl/mkl_requantize_per_channel_op.cc"}
{"idx": 197262, "project": "tensorflow", "commit_id": "e6cf28c72ba2eb949ca950d834dd6d66bb01cfae", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae", "commit_message": "Validate that matrix dimension sizes in SparseMatMul are positive.\n\nPiperOrigin-RevId: 401149683\nChange-Id: Ib33eafc561a39c8741ece80b2edce6d4aae9a57d", "target": 1, "func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& a = ctx->input(0);\n    const Tensor& b = ctx->input(1);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"a is not a matrix\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"b is not a matrix\"));\n\n    const int m = transpose_a_ ? a.dim_size(1) : a.dim_size(0);\n    const int k = transpose_a_ ? a.dim_size(0) : a.dim_size(1);\n    const int n = transpose_b_ ? b.dim_size(0) : b.dim_size(1);\n    const int k2 = transpose_b_ ? b.dim_size(1) : b.dim_size(0);\n\n    OP_REQUIRES(ctx, k == k2,\n                errors::InvalidArgument(\n                    \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                    \", b: \", b.shape().DebugString()));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n\n    if (k == 0) {\n      // If the inner dimension k in the matrix multiplication is zero, we fill\n      // the output with zeros.\n      functor::SetZeroFunctor<CPUDevice, float> f;\n      f(ctx->eigen_device<CPUDevice>(), output->flat<float>());\n      return;\n    }\n\n    auto out = output->matrix<float>();\n\n    std::unique_ptr<Tensor> a_float;\n    std::unique_ptr<Tensor> b_float;\n    if (!a_is_sparse_ && !b_is_sparse_) {\n      auto left = &a;\n      auto right = &b;\n      // TODO(agarwal): multi-thread the conversions from bfloat16 to float.\n      if (std::is_same<TL, bfloat16>::value) {\n        a_float.reset(new Tensor(DT_FLOAT, a.shape()));\n        BFloat16ToFloat(a.flat<bfloat16>().data(),\n                        a_float->flat<float>().data(), a.NumElements());\n        left = a_float.get();\n      }\n      if (std::is_same<TR, bfloat16>::value) {\n        b_float.reset(new Tensor(DT_FLOAT, b.shape()));\n        BFloat16ToFloat(b.flat<bfloat16>().data(),\n                        b_float->flat<float>().data(), b.NumElements());\n        right = b_float.get();\n      }\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0].first = transpose_a_ ? 0 : 1;\n      dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n      out.device(ctx->template eigen_device<CPUDevice>()) =\n          left->matrix<float>().contract(right->matrix<float>(), dim_pair);\n      return;\n    }\n\n    auto left = &a;\n    auto right = &b;\n    bool transpose_output = false;\n    bool transpose_a = transpose_a_;\n    bool transpose_b = transpose_b_;\n    if (!a_is_sparse_) {\n      // Swap the order of multiplications using the identity:\n      // A * B = (B' *  A')'.\n      std::swap(left, right);\n      std::swap(transpose_a, transpose_b);\n      transpose_a = !transpose_a;\n      transpose_b = !transpose_b;\n      transpose_output = !transpose_output;\n    }\n\n    std::unique_ptr<Tensor> right_tr;\n    if (transpose_b) {\n      // TODO(agarwal): avoid transposing the matrix here and directly handle\n      // transpose in CreateDenseSlices.\n      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      right_tr.reset(\n          new Tensor(right->dtype(),\n                     TensorShape({right->dim_size(1), right->dim_size(0)})));\n\n      const auto perm = dsizes_10();\n      if (transpose_output) {\n        right_tr->matrix<TL>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TL>().shuffle(perm);\n      } else {\n        right_tr->matrix<TR>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TR>().shuffle(perm);\n      }\n      right = right_tr.get();\n    }\n\n    if (transpose_output) {\n      DoMatMul<TR, TL>::Compute(&this->cache_tr_, left->matrix<TR>(),\n                                right->matrix<TL>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    } else {\n      DoMatMul<TL, TR>::Compute(&this->cache_nt_, left->matrix<TL>(),\n                                right->matrix<TR>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    }\n  }", "func_hash": 281398484697111962957982643274603954439, "file_name": "sparse_matmul_op.cc", "file_hash": 108103579052154515117474451671828260520, "cwe": ["CWE-125"], "cve": "CVE-2021-41219", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions the code for sparse matrix multiplication is vulnerable to undefined behavior via binding a reference to `nullptr`. This occurs whenever the dimensions of `a` or `b` are 0 or less. In the case on one of these is 0, an empty output tensor should be allocated (to conserve the invariant that output tensors are always allocated when the operation is successful) but nothing should be written to it (that is, we should return early from the kernel implementation). Otherwise, attempts to write to this empty tensor would result in heap OOB access. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41219", "file_path": "tensorflow/core/kernels/sparse_matmul_op.cc"}
{"idx": 256138, "project": "tensorflow", "commit_id": "e6cf28c72ba2eb949ca950d834dd6d66bb01cfae", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/e6cf28c72ba2eb949ca950d834dd6d66bb01cfae", "commit_message": "Validate that matrix dimension sizes in SparseMatMul are positive.\n\nPiperOrigin-RevId: 401149683\nChange-Id: Ib33eafc561a39c8741ece80b2edce6d4aae9a57d", "target": 0, "func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& a = ctx->input(0);\n    const Tensor& b = ctx->input(1);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(a.shape()),\n                errors::InvalidArgument(\"a is not a matrix\"));\n    OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(b.shape()),\n                errors::InvalidArgument(\"b is not a matrix\"));\n\n    const int m = transpose_a_ ? a.dim_size(1) : a.dim_size(0);\n    const int k = transpose_a_ ? a.dim_size(0) : a.dim_size(1);\n    const int n = transpose_b_ ? b.dim_size(0) : b.dim_size(1);\n    const int k2 = transpose_b_ ? b.dim_size(1) : b.dim_size(0);\n\n    OP_REQUIRES(ctx, k == k2,\n                errors::InvalidArgument(\n                    \"Matrix size incompatible: a: \", a.shape().DebugString(),\n                    \", b: \", b.shape().DebugString()));\n    OP_REQUIRES(ctx, m >= 0 && n >= 0 && k >= 0,\n                errors::InvalidArgument(\n                    \"Matrix dimensions cannot be negative: a: \",\n                    a.shape().DebugString(), \", b: \", b.shape().DebugString()));\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({m, n}), &output));\n\n    // Return early if at least one of the output dimension size is 0.\n    if (m == 0 || n == 0) {\n      return;\n    }\n\n    if (k == 0) {\n      // If the inner dimension k in the matrix multiplication is zero, we fill\n      // the output with zeros.\n      functor::SetZeroFunctor<CPUDevice, float> f;\n      f(ctx->eigen_device<CPUDevice>(), output->flat<float>());\n      return;\n    }\n\n    auto out = output->matrix<float>();\n\n    std::unique_ptr<Tensor> a_float;\n    std::unique_ptr<Tensor> b_float;\n    if (!a_is_sparse_ && !b_is_sparse_) {\n      auto left = &a;\n      auto right = &b;\n      // TODO(agarwal): multi-thread the conversions from bfloat16 to float.\n      if (std::is_same<TL, bfloat16>::value) {\n        a_float.reset(new Tensor(DT_FLOAT, a.shape()));\n        BFloat16ToFloat(a.flat<bfloat16>().data(),\n                        a_float->flat<float>().data(), a.NumElements());\n        left = a_float.get();\n      }\n      if (std::is_same<TR, bfloat16>::value) {\n        b_float.reset(new Tensor(DT_FLOAT, b.shape()));\n        BFloat16ToFloat(b.flat<bfloat16>().data(),\n                        b_float->flat<float>().data(), b.NumElements());\n        right = b_float.get();\n      }\n      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> dim_pair;\n      dim_pair[0].first = transpose_a_ ? 0 : 1;\n      dim_pair[0].second = transpose_b_ ? 1 : 0;\n\n      out.device(ctx->template eigen_device<CPUDevice>()) =\n          left->matrix<float>().contract(right->matrix<float>(), dim_pair);\n      return;\n    }\n\n    auto left = &a;\n    auto right = &b;\n    bool transpose_output = false;\n    bool transpose_a = transpose_a_;\n    bool transpose_b = transpose_b_;\n    if (!a_is_sparse_) {\n      // Swap the order of multiplications using the identity:\n      // A * B = (B' *  A')'.\n      std::swap(left, right);\n      std::swap(transpose_a, transpose_b);\n      transpose_a = !transpose_a;\n      transpose_b = !transpose_b;\n      transpose_output = !transpose_output;\n    }\n\n    std::unique_ptr<Tensor> right_tr;\n    if (transpose_b) {\n      // TODO(agarwal): avoid transposing the matrix here and directly handle\n      // transpose in CreateDenseSlices.\n      OP_REQUIRES(ctx, right->dim_size(0) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      OP_REQUIRES(ctx, right->dim_size(1) != 0,\n                  errors::InvalidArgument(\"b has an entry 0 in it's shape.\"));\n      right_tr.reset(\n          new Tensor(right->dtype(),\n                     TensorShape({right->dim_size(1), right->dim_size(0)})));\n\n      const auto perm = dsizes_10();\n      if (transpose_output) {\n        right_tr->matrix<TL>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TL>().shuffle(perm);\n      } else {\n        right_tr->matrix<TR>().device(ctx->template eigen_device<CPUDevice>()) =\n            right->matrix<TR>().shuffle(perm);\n      }\n      right = right_tr.get();\n    }\n\n    if (transpose_output) {\n      DoMatMul<TR, TL>::Compute(&this->cache_tr_, left->matrix<TR>(),\n                                right->matrix<TL>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    } else {\n      DoMatMul<TL, TR>::Compute(&this->cache_nt_, left->matrix<TL>(),\n                                right->matrix<TR>(), transpose_a,\n                                ctx->device()->tensorflow_cpu_worker_threads(),\n                                transpose_output, &out);\n    }\n  }", "func_hash": 324184408352079066865576775824843839346, "file_name": "sparse_matmul_op.cc", "file_hash": 60640554660394807532445387638343536122, "cwe": ["CWE-125"], "cve": "CVE-2021-41219", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions the code for sparse matrix multiplication is vulnerable to undefined behavior via binding a reference to `nullptr`. This occurs whenever the dimensions of `a` or `b` are 0 or less. In the case on one of these is 0, an empty output tensor should be allocated (to conserve the invariant that output tensors are always allocated when the operation is successful) but nothing should be written to it (that is, we should return early from the kernel implementation). Otherwise, attempts to write to this empty tensor would result in heap OOB access. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41219", "file_path": "tensorflow/core/kernels/sparse_matmul_op.cc"}
{"idx": 197305, "project": "pjproject", "commit_id": "11559e49e65bdf00922ad5ae28913ec6a198d508", "project_url": "https://github.com/pjsip/pjproject", "commit_url": "https://github.com/pjsip/pjproject/commit/11559e49e65bdf00922ad5ae28913ec6a198d508", "commit_message": "Merge pull request from GHSA-vhxv-phmx-g52q\n\n* Prevent OOB read/write when parsing RTCP FB RPSI\n\n* Add log information\n\n* Modification based on comments.", "target": 1, "func": "PJ_DEF(pj_status_t) pjmedia_rtcp_fb_parse_rpsi(\n\t\t\t\t\tconst void *buf,\n\t\t\t\t\tpj_size_t length,\n\t\t\t\t\tpjmedia_rtcp_fb_rpsi *rpsi)\n{\n    pjmedia_rtcp_common *hdr = (pjmedia_rtcp_common*) buf;\n    pj_uint8_t *p;\n    pj_uint8_t padlen;\n    pj_size_t rpsi_len;\n\n    PJ_ASSERT_RETURN(buf && rpsi, PJ_EINVAL);\n    PJ_ASSERT_RETURN(length >= sizeof(pjmedia_rtcp_common), PJ_ETOOSMALL);\n\n    /* RPSI uses pt==RTCP_PSFB and FMT==3 */\n    if (hdr->pt != RTCP_PSFB || hdr->count != 3)\n\treturn PJ_ENOTFOUND;\n\n    rpsi_len = (pj_ntohs((pj_uint16_t)hdr->length)-2) * 4;\n    if (length < rpsi_len + 12)\n\treturn PJ_ETOOSMALL;\n\n    p = (pj_uint8_t*)hdr + sizeof(*hdr);\n    padlen = *p++;\n    rpsi->pt = (*p++ & 0x7F);\n    rpsi->rpsi_bit_len = rpsi_len*8 - 16 - padlen;\n    pj_strset(&rpsi->rpsi, (char*)p, (rpsi->rpsi_bit_len + 7)/8);\n\n    return PJ_SUCCESS;\n}", "func_hash": 36256848360171648205913357387602986017, "file_name": "rtcp_fb.c", "file_hash": 8904386284990700433551187598162654514, "cwe": ["CWE-200"], "cve": "CVE-2022-24786", "cve_desc": "PJSIP is a free and open source multimedia communication library written in C. PJSIP versions 2.12 and prior do not parse incoming RTCP feedback RPSI (Reference Picture Selection Indication) packet, but any app that directly uses pjmedia_rtcp_fb_parse_rpsi() will be affected. A patch is available in the `master` branch of the `pjsip/pjproject` GitHub repository. There are currently no known workarounds.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-24786", "file_path": "pjmedia/src/pjmedia/rtcp_fb.c"}
{"idx": 256441, "project": "pjproject", "commit_id": "11559e49e65bdf00922ad5ae28913ec6a198d508", "project_url": "https://github.com/pjsip/pjproject", "commit_url": "https://github.com/pjsip/pjproject/commit/11559e49e65bdf00922ad5ae28913ec6a198d508", "commit_message": "Merge pull request from GHSA-vhxv-phmx-g52q\n\n* Prevent OOB read/write when parsing RTCP FB RPSI\n\n* Add log information\n\n* Modification based on comments.", "target": 0, "func": "PJ_DEF(pj_status_t) pjmedia_rtcp_fb_parse_rpsi(\n\t\t\t\t\tconst void *buf,\n\t\t\t\t\tpj_size_t length,\n\t\t\t\t\tpjmedia_rtcp_fb_rpsi *rpsi)\n{\n    pjmedia_rtcp_fb_common *hdr = (pjmedia_rtcp_fb_common*) buf;\n    pj_uint8_t *p;\n    pj_uint8_t padlen;\n    pj_size_t rpsi_len;\n\n    PJ_ASSERT_RETURN(buf && rpsi, PJ_EINVAL);\n    PJ_ASSERT_RETURN(length >= sizeof(pjmedia_rtcp_fb_common), PJ_ETOOSMALL);\n\n    /* RPSI uses pt==RTCP_PSFB and FMT==3 */\n    if (hdr->rtcp_common.pt != RTCP_PSFB || hdr->rtcp_common.count != 3)\n\treturn PJ_ENOTFOUND;\n\n    if (hdr->rtcp_common.length < 3) {    \n        PJ_PERROR(3, (THIS_FILE, PJ_ETOOSMALL,\n                      \"Failed parsing FB RPSI, invalid header length\"));\n\treturn PJ_ETOOSMALL;\n    }\n\n    rpsi_len = (pj_ntohs((pj_uint16_t)hdr->rtcp_common.length)-2) * 4;\n    if (length < rpsi_len + 12)\n\treturn PJ_ETOOSMALL;\n\n    p = (pj_uint8_t*)hdr + sizeof(*hdr);\n    padlen = *p++;\n\n    if (padlen >= 32) {\n        PJ_PERROR(3, (THIS_FILE, PJ_ETOOBIG,\n                      \"Failed parsing FB RPSI, invalid RPSI padding len\"));\n\treturn PJ_ETOOBIG;\n    }\n\n    if ((rpsi_len * 8) < (unsigned)(16 + padlen)) {\n        PJ_PERROR(3, (THIS_FILE, PJ_ETOOSMALL,\n                      \"Failed parsing FB RPSI, invalid RPSI bit len\"));\n\treturn PJ_ETOOSMALL;\n    }\n\n    rpsi->pt = (*p++ & 0x7F);\n    rpsi->rpsi_bit_len = rpsi_len*8 - 16 - padlen;\n    pj_strset(&rpsi->rpsi, (char*)p, (rpsi->rpsi_bit_len + 7)/8);\n\n    return PJ_SUCCESS;\n}", "func_hash": 124069603564950829711267554809235900949, "file_name": "rtcp_fb.c", "file_hash": 331052800319582176026781515649325160782, "cwe": ["CWE-200"], "cve": "CVE-2022-24786", "cve_desc": "PJSIP is a free and open source multimedia communication library written in C. PJSIP versions 2.12 and prior do not parse incoming RTCP feedback RPSI (Reference Picture Selection Indication) packet, but any app that directly uses pjmedia_rtcp_fb_parse_rpsi() will be affected. A patch is available in the `master` branch of the `pjsip/pjproject` GitHub repository. There are currently no known workarounds.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-24786", "file_path": "pjmedia/src/pjmedia/rtcp_fb.c"}
{"idx": 197318, "project": "tensorflow", "commit_id": "cff267650c6a1b266e4b4500f69fbc49cdd773c5", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/cff267650c6a1b266e4b4500f69fbc49cdd773c5", "commit_message": "Fix tf.raw_ops.DeleteSessionTensor vulnerability with invalid `handle`.\n\nCheck that `handle` input is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445228994", "target": 1, "func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    const string& name = handle.scalar<tstring>()();\n    auto session_state = ctx->session_state();\n    OP_REQUIRES(ctx, session_state != nullptr,\n                errors::FailedPrecondition(\n                    \"DeleteSessionTensor called on null session state\"));\n    OP_REQUIRES_OK(ctx, session_state->DeleteTensor(name));\n  }", "func_hash": 169160525191336828594160871802042029438, "file_name": "session_ops.cc", "file_hash": 314434656782851829699760158294919826764, "cwe": ["CWE-703"], "cve": "CVE-2022-29194", "cve_desc": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.DeleteSessionTensor` does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29194", "file_path": "tensorflow/core/kernels/session_ops.cc"}
{"idx": 256587, "project": "tensorflow", "commit_id": "cff267650c6a1b266e4b4500f69fbc49cdd773c5", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/cff267650c6a1b266e4b4500f69fbc49cdd773c5", "commit_message": "Fix tf.raw_ops.DeleteSessionTensor vulnerability with invalid `handle`.\n\nCheck that `handle` input is actually a scalar before treating it as such.\n\nPiperOrigin-RevId: 445228994", "target": 0, "func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& handle = ctx->input(0);\n    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(handle.shape()),\n                errors::InvalidArgument(\"`handle` must be scalar\"));\n    const string& name = handle.scalar<tstring>()();\n    auto session_state = ctx->session_state();\n    OP_REQUIRES(ctx, session_state != nullptr,\n                errors::FailedPrecondition(\n                    \"DeleteSessionTensor called on null session state\"));\n    OP_REQUIRES_OK(ctx, session_state->DeleteTensor(name));\n  }", "func_hash": 206711037393129995970821103192159531680, "file_name": "session_ops.cc", "file_hash": 175671208479371306711458012027187883621, "cwe": ["CWE-703"], "cve": "CVE-2022-29194", "cve_desc": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.DeleteSessionTensor` does not fully validate the input arguments. This results in a `CHECK`-failure which can be used to trigger a denial of service attack. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2022-29194", "file_path": "tensorflow/core/kernels/session_ops.cc"}
{"idx": 197326, "project": "tensorflow", "commit_id": "f09caa532b6e1ac8d2aa61b7832c78c5b79300c6", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/f09caa532b6e1ac8d2aa61b7832c78c5b79300c6", "commit_message": "Fix EinsumHelper::ParseEquation to avoid uninitialized accesses.\n\nEinsumHelper::ParseEquation is supposed to return true or false in\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\nellipsis in the inputs and output. Previously, when there is no ellipsis in the\ninputs or output, the routine doesn't assign false to the variables. This\nchange initializes the two variables with false to fix the problem.\nPiperOrigin-RevId: 391772004\nChange-Id: I17b6c88aadef4131470378e48cced054bf252e86", "target": 1, "func": "  static Status ParseEquation(const string& equation,\n                              OperandLabels* input_labels,\n                              Labels* output_labels,\n                              std::vector<DimensionType>* label_types,\n                              OperandLabelCounts* input_label_counts,\n                              LabelCounts* output_label_counts,\n                              gtl::InlinedVector<bool, 2>* input_has_ellipsis,\n                              bool* output_has_ellipsis) {\n    gtl::InlinedVector<string, 2> input_str;\n    string output_str;\n    TF_RETURN_IF_ERROR(ParseEinsumEquation(equation, &input_str, &output_str));\n\n    // Temporary map from single character labels to (consecutive) integer\n    // labels.\n    absl::flat_hash_map<char, int> label_mapping;\n    int num_inputs = input_str.size();\n    input_labels->resize(num_inputs);\n\n    // Map from single characters to integer labels.\n    for (int i = 0; i < num_inputs; ++i) {\n      MapToLabels(input_str[i], &input_labels->at(i), &label_mapping);\n    }\n    MapToLabels(output_str, output_labels, &label_mapping);\n\n    // Compute counts for input and output labels.\n    int num_labels = label_mapping.size();\n    input_label_counts->resize(num_inputs);\n    input_has_ellipsis->resize(num_inputs);\n    for (int i = 0; i < num_inputs; ++i) {\n      input_label_counts->at(i).resize(num_labels);\n      for (const int label : input_labels->at(i)) {\n        if (label != kEllipsisLabel)\n          input_label_counts->at(i)[label] += 1;\n        else\n          input_has_ellipsis->at(i) = true;\n      }\n    }\n    output_label_counts->resize(num_labels);\n    for (const int label : *output_labels) {\n      if (label != kEllipsisLabel)\n        output_label_counts->at(label) += 1;\n      else\n        *output_has_ellipsis = true;\n    }\n\n    // Map each label to a unique DimensionType.\n    label_types->resize(num_labels);\n    for (int label = 0; label < num_labels; ++label) {\n      if (label == kEllipsisLabel) continue;\n      bool removed = (*output_label_counts)[label] == 0;\n      bool unique = num_inputs == 1 || (*input_label_counts)[0][label] == 0 ||\n                    (*input_label_counts)[1][label] == 0;\n      (*label_types)[label] = GetDimensionType(removed, unique);\n    }\n    return Status::OK();\n  }", "func_hash": 245883315476396879421677768643467596549, "file_name": "einsum_op_impl.h", "file_hash": 325930431028150928108252149027684363493, "cwe": ["CWE-703"], "cve": "CVE-2021-41201", "cve_desc": "TensorFlow is an open source platform for machine learning. In affeced versions during execution, `EinsumHelper::ParseEquation()` is supposed to set the flags in `input_has_ellipsis` vector and `*output_has_ellipsis` boolean to indicate whether there is ellipsis in the corresponding inputs and output. However, the code only changes these flags to `true` and never assigns `false`. This results in unitialized variable access if callers assume that `EinsumHelper::ParseEquation()` always sets these flags. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41201", "file_path": "tensorflow/core/kernels/linalg/einsum_op_impl.h"}
{"idx": 256947, "project": "tensorflow", "commit_id": "f09caa532b6e1ac8d2aa61b7832c78c5b79300c6", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/f09caa532b6e1ac8d2aa61b7832c78c5b79300c6", "commit_message": "Fix EinsumHelper::ParseEquation to avoid uninitialized accesses.\n\nEinsumHelper::ParseEquation is supposed to return true or false in\ninput_has_ellipsis and output_has_ellipsis to indicate whether there is\nellipsis in the inputs and output. Previously, when there is no ellipsis in the\ninputs or output, the routine doesn't assign false to the variables. This\nchange initializes the two variables with false to fix the problem.\nPiperOrigin-RevId: 391772004\nChange-Id: I17b6c88aadef4131470378e48cced054bf252e86", "target": 0, "func": "  static Status ParseEquation(const string& equation,\n                              OperandLabels* input_labels,\n                              Labels* output_labels,\n                              std::vector<DimensionType>* label_types,\n                              OperandLabelCounts* input_label_counts,\n                              LabelCounts* output_label_counts,\n                              gtl::InlinedVector<bool, 2>* input_has_ellipsis,\n                              bool* output_has_ellipsis) {\n    gtl::InlinedVector<string, 2> input_str;\n    string output_str;\n    TF_RETURN_IF_ERROR(ParseEinsumEquation(equation, &input_str, &output_str));\n\n    // Temporary map from single character labels to (consecutive) integer\n    // labels.\n    absl::flat_hash_map<char, int> label_mapping;\n    int num_inputs = input_str.size();\n    input_labels->resize(num_inputs);\n\n    // Map from single characters to integer labels.\n    for (int i = 0; i < num_inputs; ++i) {\n      MapToLabels(input_str[i], &input_labels->at(i), &label_mapping);\n    }\n    MapToLabels(output_str, output_labels, &label_mapping);\n\n    // Compute counts for input and output labels.\n    int num_labels = label_mapping.size();\n    input_label_counts->resize(num_inputs);\n    input_has_ellipsis->resize(num_inputs);\n    for (int i = 0; i < num_inputs; ++i) {\n      input_label_counts->at(i).resize(num_labels);\n      input_has_ellipsis->at(i) = false;\n      for (const int label : input_labels->at(i)) {\n        if (label != kEllipsisLabel)\n          input_label_counts->at(i)[label] += 1;\n        else\n          input_has_ellipsis->at(i) = true;\n      }\n    }\n    output_label_counts->resize(num_labels);\n    *output_has_ellipsis = false;\n    for (const int label : *output_labels) {\n      if (label != kEllipsisLabel)\n        output_label_counts->at(label) += 1;\n      else\n        *output_has_ellipsis = true;\n    }\n\n    // Map each label to a unique DimensionType.\n    label_types->resize(num_labels);\n    for (int label = 0; label < num_labels; ++label) {\n      if (label == kEllipsisLabel) continue;\n      bool removed = (*output_label_counts)[label] == 0;\n      bool unique = num_inputs == 1 || (*input_label_counts)[0][label] == 0 ||\n                    (*input_label_counts)[1][label] == 0;\n      (*label_types)[label] = GetDimensionType(removed, unique);\n    }\n    return Status::OK();\n  }", "func_hash": 228212953693539342712398927899777742190, "file_name": "einsum_op_impl.h", "file_hash": 62458496214900513409618845321239996805, "cwe": ["CWE-703"], "cve": "CVE-2021-41201", "cve_desc": "TensorFlow is an open source platform for machine learning. In affeced versions during execution, `EinsumHelper::ParseEquation()` is supposed to set the flags in `input_has_ellipsis` vector and `*output_has_ellipsis` boolean to indicate whether there is ellipsis in the corresponding inputs and output. However, the code only changes these flags to `true` and never assigns `false`. This results in unitialized variable access if callers assume that `EinsumHelper::ParseEquation()` always sets these flags. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41201", "file_path": "tensorflow/core/kernels/linalg/einsum_op_impl.h"}
{"idx": 197359, "project": "tensorflow", "commit_id": "68867bf01239d9e1048f98cbad185bf4761bedd3", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/68867bf01239d9e1048f98cbad185bf4761bedd3", "commit_message": "Prevent unitialized variable use in grappler.\n\nPiperOrigin-RevId: 399702928\nChange-Id: Id7e75451fbff297692dfb687f60ea04b25c96b24", "target": 1, "func": "Status AutoParallel::Initialize(const GrapplerItem& item) {\n  num_gpus_ = GetNumAvailableGPUs();\n  LOG(INFO) << \"Number of GPUs: \" << num_gpus_;\n  item_ = &item;\n  graph_ = item.graph;\n  LOG(INFO) << \"Original graph size: \" << graph_.node_size();\n  if (item.fetch.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\n  }\n\n  if (item.MainVariables().empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\n  }\n\n  for (const auto& init : item.init_ops) {\n    VLOG(1) << \"Init node: \" << init;\n  }\n\n  for (const auto& fetch : item.fetch) {\n    VLOG(1) << \"Fetch node: \" << fetch;\n  }\n\n  for (const auto& var : item.MainVariables()) {\n    VLOG(2) << \"Variable: \" << var->name();\n  }\n\n  const std::set<string> apply_gradients_ops = {\"ApplyGradientDescent\",\n                                                \"ApplyProximalGradientDescent\",\n                                                \"ApplyAdadelta\",\n                                                \"ApplyAdagrad\",\n                                                \"ApplyProximalAdagrad\",\n                                                \"ApplyAdagradDA\",\n                                                \"ApplyFtrl\",\n                                                \"ApplyMomentum\",\n                                                \"ApplyAdam\",\n                                                \"ApplyRMSProp\",\n                                                \"ApplyCenteredRMSProp\"};\n  for (int i = 0; i < graph_.node_size(); i++) {\n    all_nodes_.insert(\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\n    if (apply_gradients_ops.find(graph_.node(i).op()) !=\n        apply_gradients_ops.end()) {\n      apply_gradients_nodes_.insert(graph_.node(i).name());\n      VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();\n    }\n  }\n\n  auto div_const_node = AddNodeDivConst();\n  all_nodes_.insert(std::make_pair(div_const_node->name(), div_const_node));\n  std::map<string, int> gradient_pos = {{\"ApplyGradientDescent\", 2},\n                                        {\"ApplyProximalGradientDescent\", 4},\n                                        {\"ApplyAdadelta\", 6},\n                                        {\"ApplyAdagrad\", 3},\n                                        {\"ApplyProximalAdagrad\", 5},\n                                        {\"ApplyAdagradDA\", 3},\n                                        {\"ApplyFtrl\", 3},\n                                        {\"ApplyMomentum\", 3},\n                                        {\"ApplyAdam\", 9},\n                                        {\"ApplyRMSProp\", 7},\n                                        {\"ApplyCenteredRMSProp\", 8}};\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\n    auto apply_gradients_op = all_nodes_[apply_gradient_node_name]->op();\n    auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\n\n    auto div_node = AddNodeDiv(\n        apply_gradient_node_name,\n        apply_gradients_node->input(gradient_pos[apply_gradients_op]),\n        div_const_node->name());\n    all_nodes_.insert(std::make_pair(div_node->name(), div_node));\n    *apply_gradients_node->mutable_input(gradient_pos[apply_gradients_op]) =\n        div_node->name();\n  }\n  LOG(INFO) << \"Graph size after adding div nodes: \" << all_nodes_.size();\n\n  std::vector<const NodeDef*> train_nodes;\n  TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, item.fetch, &train_nodes));\n  LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\n\n  const NodeDef* dequeue_node;\n  for (const auto& train_node : train_nodes) {\n    if (IsDequeueOp(*train_node)) {\n      dequeue_node = train_node;\n      break;\n    }\n  }\n\n  std::vector<const NodeDef*> input_nodes;\n  if (dequeue_node) {\n    LOG(INFO) << \"Dequeue node: \" << dequeue_node->name();\n    TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, {dequeue_node->name()},\n                                              {}, &input_nodes));\n  }\n  LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();\n\n  std::set<string> dont_replicate_nodes;\n  for (const auto& variable : item.MainVariables()) {\n    dont_replicate_nodes.insert(variable->name());\n  }\n\n  for (const auto& init : item.init_ops) {\n    dont_replicate_nodes.insert(NodeName(init));\n  }\n\n  // Don't replicate all input nodes, except the dequeue node.\n  for (const auto& input_node : input_nodes) {\n    if (input_node->name() != dequeue_node->name()) {\n      dont_replicate_nodes.insert(input_node->name());\n    }\n  }\n\n  for (const auto& node : train_nodes) {\n    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {\n      replica_nodes_.insert(node->name());\n    }\n  }\n  LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();\n\n  for (const auto& node : all_nodes_) {\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\n      shared_nodes_.insert(node.first);\n    }\n  }\n  LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();\n  return Status::OK();\n}", "func_hash": 251567762309539032923316029186493191559, "file_name": "auto_parallel.cc", "file_hash": 291637818016549468794962575194747709507, "cwe": ["CWE-703"], "cve": "CVE-2021-41225", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions TensorFlow's Grappler optimizer has a use of unitialized variable. If the `train_nodes` vector (obtained from the saved model that gets optimized) does not contain a `Dequeue` node, then `dequeue_node` is left unitialized. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41225", "file_path": "tensorflow/core/grappler/optimizers/auto_parallel.cc"}
{"idx": 257461, "project": "tensorflow", "commit_id": "68867bf01239d9e1048f98cbad185bf4761bedd3", "project_url": "https://github.com/tensorflow/tensorflow", "commit_url": "https://github.com/tensorflow/tensorflow/commit/68867bf01239d9e1048f98cbad185bf4761bedd3", "commit_message": "Prevent unitialized variable use in grappler.\n\nPiperOrigin-RevId: 399702928\nChange-Id: Id7e75451fbff297692dfb687f60ea04b25c96b24", "target": 0, "func": "Status AutoParallel::Initialize(const GrapplerItem& item) {\n  num_gpus_ = GetNumAvailableGPUs();\n  LOG(INFO) << \"Number of GPUs: \" << num_gpus_;\n  item_ = &item;\n  graph_ = item.graph;\n  LOG(INFO) << \"Original graph size: \" << graph_.node_size();\n  if (item.fetch.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\n  }\n\n  if (item.MainVariables().empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\n  }\n\n  for (const auto& init : item.init_ops) {\n    VLOG(1) << \"Init node: \" << init;\n  }\n\n  for (const auto& fetch : item.fetch) {\n    VLOG(1) << \"Fetch node: \" << fetch;\n  }\n\n  for (const auto& var : item.MainVariables()) {\n    VLOG(2) << \"Variable: \" << var->name();\n  }\n\n  const std::set<string> apply_gradients_ops = {\"ApplyGradientDescent\",\n                                                \"ApplyProximalGradientDescent\",\n                                                \"ApplyAdadelta\",\n                                                \"ApplyAdagrad\",\n                                                \"ApplyProximalAdagrad\",\n                                                \"ApplyAdagradDA\",\n                                                \"ApplyFtrl\",\n                                                \"ApplyMomentum\",\n                                                \"ApplyAdam\",\n                                                \"ApplyRMSProp\",\n                                                \"ApplyCenteredRMSProp\"};\n  for (int i = 0; i < graph_.node_size(); i++) {\n    all_nodes_.insert(\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\n    if (apply_gradients_ops.find(graph_.node(i).op()) !=\n        apply_gradients_ops.end()) {\n      apply_gradients_nodes_.insert(graph_.node(i).name());\n      VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();\n    }\n  }\n\n  auto div_const_node = AddNodeDivConst();\n  all_nodes_.insert(std::make_pair(div_const_node->name(), div_const_node));\n  std::map<string, int> gradient_pos = {{\"ApplyGradientDescent\", 2},\n                                        {\"ApplyProximalGradientDescent\", 4},\n                                        {\"ApplyAdadelta\", 6},\n                                        {\"ApplyAdagrad\", 3},\n                                        {\"ApplyProximalAdagrad\", 5},\n                                        {\"ApplyAdagradDA\", 3},\n                                        {\"ApplyFtrl\", 3},\n                                        {\"ApplyMomentum\", 3},\n                                        {\"ApplyAdam\", 9},\n                                        {\"ApplyRMSProp\", 7},\n                                        {\"ApplyCenteredRMSProp\", 8}};\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\n    auto apply_gradients_op = all_nodes_[apply_gradient_node_name]->op();\n    auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\n\n    auto div_node = AddNodeDiv(\n        apply_gradient_node_name,\n        apply_gradients_node->input(gradient_pos[apply_gradients_op]),\n        div_const_node->name());\n    all_nodes_.insert(std::make_pair(div_node->name(), div_node));\n    *apply_gradients_node->mutable_input(gradient_pos[apply_gradients_op]) =\n        div_node->name();\n  }\n  LOG(INFO) << \"Graph size after adding div nodes: \" << all_nodes_.size();\n\n  std::vector<const NodeDef*> train_nodes;\n  TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, item.fetch, &train_nodes));\n  LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\n\n  const NodeDef* dequeue_node = nullptr;\n  for (const auto& train_node : train_nodes) {\n    if (IsDequeueOp(*train_node)) {\n      dequeue_node = train_node;\n      break;\n    }\n  }\n\n  std::vector<const NodeDef*> input_nodes;\n  if (dequeue_node) {\n    LOG(INFO) << \"Dequeue node: \" << dequeue_node->name();\n    TF_RETURN_IF_ERROR(ComputeTransitiveFanin(graph_, {dequeue_node->name()},\n                                              {}, &input_nodes));\n  }\n  LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();\n\n  std::set<string> dont_replicate_nodes;\n  for (const auto& variable : item.MainVariables()) {\n    dont_replicate_nodes.insert(variable->name());\n  }\n\n  for (const auto& init : item.init_ops) {\n    dont_replicate_nodes.insert(NodeName(init));\n  }\n\n  // Don't replicate all input nodes, except the dequeue node.\n  for (const auto& input_node : input_nodes) {\n    if (input_node->name() != dequeue_node->name()) {\n      dont_replicate_nodes.insert(input_node->name());\n    }\n  }\n\n  for (const auto& node : train_nodes) {\n    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {\n      replica_nodes_.insert(node->name());\n    }\n  }\n  LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();\n\n  for (const auto& node : all_nodes_) {\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\n      shared_nodes_.insert(node.first);\n    }\n  }\n  LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();\n  return Status::OK();\n}", "func_hash": 239400794091887748487635505232463109682, "file_name": "auto_parallel.cc", "file_hash": 250375078296494886105234781412804054984, "cwe": ["CWE-703"], "cve": "CVE-2021-41225", "cve_desc": "TensorFlow is an open source platform for machine learning. In affected versions TensorFlow's Grappler optimizer has a use of unitialized variable. If the `train_nodes` vector (obtained from the saved model that gets optimized) does not contain a `Dequeue` node, then `dequeue_node` is left unitialized. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "nvd_url": "https://nvd.nist.gov/vuln/detail/CVE-2021-41225", "file_path": "tensorflow/core/grappler/optimizers/auto_parallel.cc"}
